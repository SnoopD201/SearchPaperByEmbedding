{
  "model": "all-MiniLM-L6-v2",
  "total": 200,
  "results": [
    {
      "paper": {
        "id": "VhGUS8kyaC",
        "number": 7504,
        "title": "$\\texttt{STRCMP}$: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization",
        "authors": [
          "Xijun Li",
          "Jiexiang Yang",
          "Jinghao Wang",
          "Bo Peng",
          "Jianguo Yao",
          "Haibing Guan"
        ],
        "abstract": "Combinatorial optimization (CO) problems, central to operation research and theoretical computer science, present significant computational challenges due to their $\\mathcal{NP}$-hard nature. While large language models (LLMs) have emerged as promising tools for CO—either by directly generating solutions or synthesizing solver-specific codes—existing approaches often $\\textit{neglect critical structural priors inherent to CO problems}$, leading to suboptimality and iterative inefficiency. Inspired by human experts’ success in leveraging CO structures for algorithm design, we propose $\\texttt{STRCMP}$, a novel structure-aware LLM-based algorithm discovery framework that systematically integrates structure priors to enhance solution quality and solving efficiency. Our framework combines a graph neural network (GNN) for extracting structural embeddings from CO instances with an LLM conditioned on these embeddings to identify high-performed algorithms in the form of solver-specific codes. This composite architecture ensures syntactic correctness, preserves problem topology, and aligns with natural language objectives, while an evolutionary refinement process iteratively optimizes generated algorithm. Extensive evaluations across Mixed Integer Linear Programming and Boolean Satisfiability problems, using nine benchmark datasets, demonstrate that our proposed $\\texttt{STRCMP}$ outperforms five strong neural and LLM-based methods by a large margin, in terms of both solution optimality and computational efficiency. The code is publicly available in the repository: https://github.com/Y-Palver/L2O-STRCMP.",
        "keywords": [
          "Learning to Optimize",
          "LLM Code Generation",
          "Combinatorial Optimization"
        ],
        "primary_area": "machine_learning_for_sciences",
        "forum_url": "https://openreview.net/forum?id=VhGUS8kyaC"
      },
      "similarity": 0.550644040107727
    },
    {
      "paper": {
        "id": "QPJjiNCRq1",
        "number": 21867,
        "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
        "authors": [
          "Yuanyao Chen",
          "Rongsheng Chen",
          "Fu Luo",
          "Zhenkun Wang"
        ],
        "abstract": "Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions. The source code can be found in https://github.com/CIAM-Group/TTPL.",
        "keywords": [
          "Neural Combinatorial Optimiztion",
          "Large Language Model",
          "Vehicle Routing Problem"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=QPJjiNCRq1"
      },
      "similarity": 0.5477230548858643
    },
    {
      "paper": {
        "id": "VcJTTVoysQ",
        "number": 24701,
        "title": "Alignment of Large Language Models with Constrained Learning",
        "authors": [
          "Botong Zhang",
          "Shuo Li",
          "Ignacio Hounie",
          "Osbert Bastani",
          "Dongsheng Ding",
          "Alejandro Ribeiro"
        ],
        "abstract": "We study the problem of computing an optimal large language model (LLM) policy for the constrained alignment problem, where the goal is to maximize a primary reward objective while satisfying constraints on secondary utilities. Despite the popularity of Lagrangian-based LLM policy search in constrained alignment, iterative primal-dual methods often fail to converge, and non-iterative dual-based methods do not achieve optimality in the LLM parameter space. To address these challenges, we employ Lagrangian duality to develop an iterative dual-based alignment method that alternates between updating the LLM policy via Lagrangian maximization and updating the dual variable via dual descent. In theory, we characterize the primal-dual gap between the primal value in the distribution space and the dual value in the LLM parameter space. We further quantify the optimality gap of the learned LLM policies at near-optimal dual variables with respect to both the objective and the constraint functions. These results prove that dual-based alignment methods can find an optimal constrained LLM policy, up to an LLM parametrization gap. We demonstrate the effectiveness and merits of our approach through extensive experiments conducted on the PKU-SafeRLHF and Anthropic HH-RLHF datasets.",
        "keywords": [
          "Large Language Models",
          "Alignment",
          "RLHF",
          "Safety",
          "Duality",
          "Optimality gap"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=VcJTTVoysQ"
      },
      "similarity": 0.538705587387085
    },
    {
      "paper": {
        "id": "4EkEL77k6O",
        "number": 2327,
        "title": "Compress Large Language Models via  Collaboration Between Learning and Matrix Approximation",
        "authors": [
          "Yuesen Liao",
          "Zhiwei Li",
          "Binrui Wu",
          "Zihao Cheng",
          "Su Zhao",
          "Shuai Chen",
          "WEIZHONG ZHANG"
        ],
        "abstract": "Sparse and low-rank matrix composite approximation has emerged as a promising paradigm for compressing large language models (LLMs), offering a more flexible pruning structure than conventional methods based solely on sparse matrices. The significant variation in weight redundancy across layers, along with the differing rank and sparsity structures of weight matrices, makes identifying the globally optimal pruning structure extremely challenging. Existing methods often depend on uniform or manually designed heuristic rules to allocate weight sparsity across layers, subsequently compressing each matrix using matrix approximation techniques. Given the above theoretical difficulty in global compression of LLMs and the limited computational and data resources available compared to the training phase, we argue that a collaboration between learning and matrix approximation is essential for effective compression. In this paper, we propose a novel LLM compression framework based on generalized bilevel optimization that naturally formulates an effective collaborative mechanism. Specifically, the outer loop frames the weight allocation task as a probabilistic optimization problem, enabling the automatic learning of both layer-wise sparsities and matrix-wise retained ranks, while the inner loop solves the corresponding sparsity and rank-constrained model compression problem via matrix approximation. Our main technical contributions include two key innovations for efficiently solving this bilevel optimization problem. First,  we  introduce a truncated Gaussian prior-based probabilistic parameterization integrated with a policy gradient estimator, which avoids expensive backpropagation and stabilizes the optimization process. Second, we design an adapted QR-based matrix approximation algorithm that significantly accelerates inner loop computations. Extensive experiments on Phi-3 and the LLama-2/3 family demonstrate the effectiveness of our method. Notably, it maintains over 95\\% zero-shot accuracy under 50\\% sparsity and achieves up to 2× inference speedup.",
        "keywords": [
          "Model Compression",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=4EkEL77k6O"
      },
      "similarity": 0.5335067510604858
    },
    {
      "paper": {
        "id": "prGyR9id7X",
        "number": 8744,
        "title": "A Partition Cover Approach to Tokenization",
        "authors": [
          "Jia Peng Lim",
          "Shawn Tan",
          "Davin Choo",
          "Hady W. Lauw"
        ],
        "abstract": "Tokenization is the process of encoding strings into tokens of a fixed vocabulary size, and is widely utilized in Natural Language Processing applications. The leading tokenization algorithm today is Byte Pair Encoding (BPE), which formulates the tokenization problem as a compression problem and tackles it by performing sequences of merges. In this work, we formulate tokenization as an optimization objective, show that it is NP-hard via a simple reduction from vertex cover, and propose a polynomial-time greedy algorithm GreedTok. Our formulation naturally relaxes to the well-studied weighted maximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm GreedWMC. Through empirical evaluations on real-world corpora, we show that GreedTok outperforms BPE and Unigram on compression and achieves a covering score comparable to GreedWMC. Finally, our extensive pre-training for two transformer-based language models with 1 billion parameters, comparing the choices of BPE and GreedTok as the tokenizer, shows that GreedTok achieves a lower bit per byte even when we control for either the total dataset proportion or total training tokens.",
        "keywords": [
          "tokenization",
          "large language models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=prGyR9id7X"
      },
      "similarity": 0.5270604491233826
    },
    {
      "paper": {
        "id": "BuYtcTUMyA",
        "number": 13646,
        "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks",
        "authors": [
          "Fali Wang",
          "Hui Liu",
          "Zhenwei DAI",
          "Jingying Zeng",
          "Zhiwei Zhang",
          "Zongyu Wu",
          "Chen Luo",
          "Zhen Li",
          "Xianfeng Tang",
          "Qi He",
          "Suhang Wang"
        ],
        "abstract": "Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.",
        "keywords": [
          "language models",
          "scaling",
          "test-time compute",
          "compute-optimal inference"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=BuYtcTUMyA"
      },
      "similarity": 0.5266221761703491
    },
    {
      "paper": {
        "id": "qr5uMEs6iR",
        "number": 15371,
        "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers",
        "authors": [
          "Xia Jiang",
          "Yaoxin Wu",
          "Minshuo Li",
          "Zhiguang Cao",
          "Yingqian Zhang"
        ],
        "abstract": "Combinatorial optimization (CO) problems, central to decision-making scenarios like logistics and manufacturing, are traditionally solved using problem-specific algorithms requiring significant domain expertise. While large language models (LLMs) have shown promise in automating CO problem solving, existing approaches rely on intermediate steps such as code generation or solver invocation, limiting their generality and accessibility. This paper introduces a novel framework that empowers LLMs to serve as end-to-end CO solvers by directly mapping natural language problem descriptions to solutions. We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts LLMs with solution construction patterns from domain-specific solvers, while a feasibility-and-optimality-aware reinforcement learning (FOARL) process explicitly mitigates constraint violations and refines solution quality. Evaluation across seven NP-hard CO problems shows that our method achieves a high feasibility rate and reduces the average optimality gap to 1.03–8.20% by tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o), reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our method establishes a unified language-based pipeline for CO without extensive code execution or manual architectural adjustments for different problems, offering a general and language-driven alternative to traditional solver design while maintaining relative feasibility guarantees.",
        "keywords": [
          "Large Language Model; Combinatorial Optimization; Reinforcement Learning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=qr5uMEs6iR"
      },
      "similarity": 0.5132385492324829
    },
    {
      "paper": {
        "id": "Ej20yjWMCj",
        "number": 25426,
        "title": "OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling",
        "authors": [
          "Haoyang Liu",
          "Jie Wang",
          "Yuyang Cai",
          "Xiongwei Han",
          "Yufei Kuang",
          "Jianye HAO"
        ],
        "abstract": "Optimization modeling is one of the most crucial but technical parts of operations research (OR).\n  To automate the modeling process, existing works have leveraged large language models (LLMs), prompting them to break down tasks into steps for generating variables, constraints, and objectives.\n  However, due to the highly complex mathematical structures inherent in OR problems, standard fixed-step decomposition often fails to achieve high performance.\n  To address this challenge, we introduce OptiTree, a novel tree search approach designed to enhance modeling capabilities for complex problems through adaptive problem decomposition into simpler subproblems.\n  Specifically, we develop a modeling tree that organizes a wide range of OR problems based on their hierarchical problem taxonomy and complexity, with each node representing a problem category and containing relevant high-level modeling thoughts.\n  Given a problem to model, we recurrently search the tree to identify a series of simpler subproblems and synthesize the global modeling thoughts by adaptively integrating the hierarchical thoughts.\n  Experiments show that OptiTree significantly improves the modeling accuracy compared to the state-of-the-art, achieving over 10% improvements on the challenging benchmarks.",
        "keywords": [
          "Optimization Modeling",
          "Operations research",
          "Large language models"
        ],
        "primary_area": "other",
        "forum_url": "https://openreview.net/forum?id=Ej20yjWMCj"
      },
      "similarity": 0.5081716179847717
    },
    {
      "paper": {
        "id": "b2IU6QOOfo",
        "number": 21319,
        "title": "PaZO: Preconditioned Accelerated Zeroth-Order Optimization for Fine-Tuning LLMs",
        "authors": [
          "Hanzhen Zhao",
          "Shihong Ding",
          "Cong Fang",
          "Zhouchen Lin"
        ],
        "abstract": "This paper introduces PaZO, a preconditioned accelerated zeroth-order optimization algorithm for fine-tuning large language models (LLMs).      First, we theoretically demonstrate the necessity of preconditioning in zeroth-order optimization, proving that zeroth-order stochastic gradient descent (ZO-SGD) alone fails to achieve the ideal convergence rate.     Building on this, we propose a Preconditioned Simultaneous Perturbation Stochastic Approximation (PSPSA) and theoretical version of PaZO, and demonstrate that setting the order of preconditioner as $-1/2$ in PSPSA yields the improved convergence rate for PaZO.     Moreover, we design a practical version of PaZO that stabilizes training via diagonal Hessian estimate and moving average technique.  Extensive experiments on diverse downstream tasks with models like RoBERTa-large and OPT show PaZO’s effectiveness.  Compared to other zeroth-order baselines, PaZO achieves better performance across models and tasks.",
        "keywords": [
          "zeroth-order optimization",
          "fine-tuning LLMs",
          "preconditioner"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=b2IU6QOOfo"
      },
      "similarity": 0.5012246370315552
    },
    {
      "paper": {
        "id": "VrCdsZBbIg",
        "number": 8319,
        "title": "Language Modeling by Language Models",
        "authors": [
          "Junyan Cheng",
          "Peter Clark",
          "Kyle Richardson"
        ],
        "abstract": "*Can we leverage LLMs to model the process of discovering novel language model (LM) architectures?* Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system *Genesys* employs a *Ladder of Scales* approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified) and find the best designs to be competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks).  We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.",
        "keywords": [
          "Autonomous Scientific Discovery",
          "Large Language Model Agents",
          "Genetic Programming"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=VrCdsZBbIg"
      },
      "similarity": 0.4970642328262329
    },
    {
      "paper": {
        "id": "mHHrnCWwrD",
        "number": 3768,
        "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for  Large Language Models Pretraining",
        "authors": [
          "Ping Guo",
          "Yubing Ren",
          "BINBINLIU",
          "Fengze Liu",
          "Haobin Lin",
          "Yifan Zhang",
          "Bingni Zhang",
          "Taifeng Wang",
          "Yin Zheng"
        ],
        "abstract": "Large language models (LLMs) have become integral to a wide range of applications worldwide, driving an unprecedented global demand for effective multilingual capabilities. Central to achieving robust multilingual performance is the strategic allocation of language proportions within training corpora. However, determining optimal language ratios is highly challenging due to intricate cross-lingual interactions and sensitivity to dataset scale. This paper introduces CLIMB (Cross-Lingual Interaction-aware Multilingual Balancing), a novel framework designed to systematically optimize multilingual data allocation. At its core, CLIMB introduces a cross-lingual interaction-aware language ratio, explicitly quantifying each language’s effective allocation by capturing inter-language dependencies. Leveraging this ratio, CLIMB proposes a principled two-step optimization procedure—first equalizing marginal benefits across languages, then maximizing the magnitude of the resulting language allocation vectors—significantly simplifying the inherently complex multilingual optimization problem. Extensive experiments confirm that CLIMB can accurately measure cross-lingual interactions across various multilingual settings. LLMs trained with CLIMB-derived proportions consistently achieve state-of-the-art multilingual performance, even achieve competitive performance with open-sourced LLMs trained with more tokens.",
        "keywords": [
          "Large Language Model",
          "Pre-training",
          "Scaling Law",
          "Language Mix Ratio"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=mHHrnCWwrD"
      },
      "similarity": 0.4970519244670868
    },
    {
      "paper": {
        "id": "0mOBdNsI3L",
        "number": 13735,
        "title": "Approximately Aligned Decoding",
        "authors": [
          "Daniel Melcer",
          "Sujan Kumar Gonugondla",
          "Pramuditha Perera",
          "Haifeng Qian",
          "Wen-Hao Chiang",
          "Yanjun Wang",
          "Nihal Jain",
          "Pranav Garg",
          "Xiaofei Ma",
          "Anoop Deoras"
        ],
        "abstract": "It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation to re-sample after a rejection, or distort the distribution of outputs by constraining the output to highly improbable tokens.\nWe present a method, Approximately Aligned Decoding (AprAD), to balance the distortion of the output distribution with computational efficiency, inspired by algorithms from the speculative decoding literature.\nAprAD allows for the generation of long sequences of text with difficult-to-satisfy constraints, while amplifying low probability outputs much less compared to existing methods.\nWe show through a series of experiments that the task-specific performance of AprAD is comparable to methods that do not distort the output distribution, while being much more computationally efficient.",
        "keywords": [
          "Constrained Decoding",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=0mOBdNsI3L"
      },
      "similarity": 0.4938691258430481
    },
    {
      "paper": {
        "id": "CA1xVSvn72",
        "number": 2963,
        "title": "Lua-LLM: Learning Unstructured-Sparsity Allocation for Large Language Models",
        "authors": [
          "Mingge Lu",
          "Jingwei Sun",
          "Junqing Lin",
          "Zechun Zhou",
          "Guangzhong Sun"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their extensive parameter scales pose significant challenges for practical deployment. Unstructured pruning has emerged as an effective model compression strategy with minimal performance loss, which introduces fine-grained sparsity for weight parameters. While existing methods employ a layer-wise pruning strategy to avoid the complexity of global pruning for billion-scale LLMs, they require appropriate sparsity allocation for the layer-wise pruning objectives and often lead to suboptimal solutions for the overall model. In this paper, we propose Lua-LLM ($\\textbf{L}$earning $\\textbf{u}$nstructured-sparsity $\\textbf{a}$llocation in LLMs), a learning-based global pruning framework that explores the optimal unstructured sparsity allocation. Unlike existing pruning methods, which primarily focus on allocating per-layer sparsity, Lua-LLM achieves flexible allocation for both layer-wise and intra-layer sparsity. Furthermore, Lua-LLM leverages a soft Top-K operator to approximate the importance-based mask selection mechanism, enabling efficient binary mask learning. Experimental results on LLaMA and OPT families demonstrate significant performance improvements over existing methods.",
        "keywords": [
          "Sparsity",
          "Pruning",
          "Efficient Inference",
          "Large Language Models"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=CA1xVSvn72"
      },
      "similarity": 0.4921638071537018
    },
    {
      "paper": {
        "id": "VGB2TV0QUE",
        "number": 16951,
        "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems",
        "authors": [
          "Yi Zhang",
          "Yushen Long",
          "Yun Ni",
          "Liping Huang",
          "Xiaohong Wang",
          "Jun Liu"
        ],
        "abstract": "Online ride-hailing platforms aim to deliver efficient mobility-on-demand services, often facing challenges in balancing dynamic and spatially heterogeneous supply and demand. Existing methods typically fall into two categories: reinforcement learning (RL) approaches, which suffer from data inefficiency, oversimplified modeling of real-world dynamics, and difficulty enforcing operational constraints; or decomposed online optimization methods, which rely on manually designed high-level objectives that lack awareness of low-level routing dynamics. To address this issue, we propose a novel hybrid framework that integrates large language model (LLM) with mathematical optimization in a dynamic hierarchical system: (1) it is training-free, removing the need for large-scale interaction data as in RL, and (2) it leverages LLM to bridge cognitive limitations caused by problem decomposition by adaptively generating high-level objectives. Within this framework, LLM serves as a meta-optimizer, producing semantic heuristics that guide a low-level optimizer responsible for constraint enforcement and real-time decision execution. These heuristics are refined through a closed-loop evolutionary process, driven by harmony search, which iteratively adapts the LLM prompts based on feasibility and performance feedback from the optimization layer. Extensive experiments based on scenarios derived from both the New York and Chicago taxi datasets demonstrate the effectiveness of our approach, achieving an average improvement of 16% compared to state-of-the-art baselines.",
        "keywords": [
          "Dynamic Hierarchical Optimization",
          "LLM-Optimizer Interaction",
          "Mobility-on-Demand Systems",
          "Meta-Objective Design",
          "Feasibility-Aware Adaptation"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=VGB2TV0QUE"
      },
      "similarity": 0.4921463131904602
    },
    {
      "paper": {
        "id": "EdKl4PulMX",
        "number": 18873,
        "title": "Complexity Scaling Laws for Neural Models using Combinatorial Optimization",
        "authors": [
          "Lowell Weissman",
          "Michael Krumdick",
          "A. Lynn Abbott"
        ],
        "abstract": "Recent work on neural scaling laws demonstrates that model performance scales predictably with compute budget, model size, and dataset size. In this work, we develop scaling laws based on problem complexity. We analyze two fundamental complexity measures: solution space size and representation space size. Using the Traveling Salesman Problem (TSP) as a case study, we show that combinatorial optimization promotes smooth cost trends, and therefore meaningful scaling laws can be obtained even in the absence of an interpretable loss. We then show that suboptimality grows predictably for fixed-size models when scaling the number of TSP nodes or spatial dimensions, independent of whether the model was trained with reinforcement learning or supervised fine-tuning on a static dataset. We conclude with an analogy to problem complexity scaling in local search, showing that a much simpler gradient descent of the cost landscape produces similar trends.",
        "keywords": [
          "scaling laws",
          "problem complexity",
          "traveling salesman problem"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=EdKl4PulMX"
      },
      "similarity": 0.490906685590744
    },
    {
      "paper": {
        "id": "R24ZqNwoDz",
        "number": 22375,
        "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
        "authors": [
          "Xueliang Zhao",
          "Wei Wu",
          "Jian Guan",
          "Qintong Li",
          "Lingpeng Kong"
        ],
        "abstract": "In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \\textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. \nExtensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at \\url{https://github.com/zhaoxlpku/DynaAct}.",
        "keywords": [
          "Sequential Decision-Making",
          "Action Space Optimization",
          "Submodular Function"
        ],
        "primary_area": "reinforcement_learning",
        "forum_url": "https://openreview.net/forum?id=R24ZqNwoDz"
      },
      "similarity": 0.4889411926269531
    },
    {
      "paper": {
        "id": "xSHqNf5Pdc",
        "number": 16368,
        "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling",
        "authors": [
          "Xinglin Wang",
          "Yiwei Li",
          "Shaoxiong Feng",
          "Peiwen Yuan",
          "Yueqi Zhang",
          "Jiayi Shi",
          "Chuyi Tan",
          "Boyuan Pan",
          "Yao Hu",
          "Kan Li"
        ],
        "abstract": "Test-Time Scaling (TTS) improves the performance of Large Language Models (LLMs) by using additional inference-time computation to explore multiple reasoning paths through search. Yet how to allocate a fixed rollout budget most effectively during search remains underexplored, often resulting in inefficient use of compute at test time. To bridge this gap, we formulate test-time search as a resource allocation problem and derive the optimal allocation strategy that maximizes the probability of obtaining a correct solution under a fixed rollout budget. Within this formulation, we reveal a core limitation of existing search methods: solution-level allocation tends to favor reasoning directions with more candidates, leading to theoretically suboptimal and inefficient use of compute. To address this, we propose Direction-Oriented Resource Allocation (DORA), a provably optimal method that mitigates this bias by decoupling direction quality from candidate count and allocating resources at the direction level. To demonstrate DORA’s effectiveness, we conduct extensive experiments on challenging mathematical reasoning benchmarks including MATH500, AIME2024, and AIME2025. The empirical results show that DORA consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art accuracy.  We hope our findings contribute to a broader understanding of optimal TTS for LLMs.",
        "keywords": [
          "Test-Time Scaling",
          "Resource Allocation",
          "Large Language Model"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=xSHqNf5Pdc"
      },
      "similarity": 0.4886408746242523
    },
    {
      "paper": {
        "id": "3naHyE5klE",
        "number": 1201,
        "title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models",
        "authors": [
          "Haidong Kang",
          "Lihong Lin",
          "Hanling Wang"
        ],
        "abstract": "The success of computer vision tasks is mainly attributed to the architectural design of neural networks. This highlights the need to automatically design high-performance architectures via Neural Architecture Search (NAS). To accelerate the search process, training-free NAS is proposed, which aims to search high-performance architectures at initialization via zero-cost proxies (ZCPs). However, existing zero-cost proxies heavily rely on manual design, which is often labor-intensive and requires extensive expert knowledge. In addition, these crafted proxies often suffer from poor correlation with final model performance and high computational complexity, severely limiting NAS efficiency in real-world applications. To address those issues, this paper proposes a novel Large Language Models (LLMs)-driven $\\underline{A}$utomatic $\\underline{P}$roxy $\\underline{D}$iscovery ($\\textbf{APD}$) framework, which revolutionizes the design paradigm of ZCPs by leveraging LLMs to automatically discover optimal ZCPs for Training-Free NAS. Moreover, we utilize actor-critic based reinforcement learning to optimize prompts, enabling to generate better ZCPs in the next generation. We conduct extensive experiments on mainstream NAS benchmarks, demonstrating APD excels in both performance and efficiency. Besides, we firmly believe that our APD will dramatically benefit the deep learning community through providing novel paradigm of design algorithms via LLMs.",
        "keywords": [
          "Large Language Models",
          "Training-Free NAS",
          "Automatic Proxy Discovery"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=3naHyE5klE"
      },
      "similarity": 0.48734578490257263
    },
    {
      "paper": {
        "id": "Iytf59QZzl",
        "number": 8636,
        "title": "Preference Optimization by Estimating the Ratio of the Data Distribution",
        "authors": [
          "Yeongmin Kim",
          "HeeSun Bae",
          "Byeonghu Na",
          "Il-chul Moon"
        ],
        "abstract": "Direct preference optimization (DPO) is widely used as a simple and stable method for aligning large language models (LLMs) with human preferences. This paper investigates a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective.  The ratio of the target policy provides a unique identification of the policy distribution without relying on reward models or partition functions. This allows the generalized loss to retain both simplicity and theoretical guarantees, which prior work such as $f$-PO fails to achieve simultaneously. We propose \\textit{Bregman preference optimization} (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing implementation with a few lines of code. We further develop scaled Basu's power divergence (SBA), a gradient scaling method that can be used for BPO instances. The BPO framework complements other DPO variants and is applicable to target policies defined by these variants. In experiments, unlike other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibits a trade-off between generation fidelity and diversity, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-8B-Instruct, BPO achieves state-of-the-art performance among Llama-3-8B backbones, with a 55.9\\% length-controlled win rate on AlpacaEval2. Project page: https://github.com/aailab-kaist/BPO.",
        "keywords": [
          "Direct Preference Optimization",
          "Language Models",
          "AI Alignment"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Iytf59QZzl"
      },
      "similarity": 0.48605337738990784
    },
    {
      "paper": {
        "id": "2jzGEudVdS",
        "number": 11721,
        "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning",
        "authors": [
          "Hua Ye",
          "Siyuan Chen",
          "Haoliang Zhang",
          "Weihao Luo",
          "Yanbin Li",
          "Xuan Zhang"
        ],
        "abstract": "Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.",
        "keywords": [
          "Fine-Tuning",
          "Multi-Domain"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=2jzGEudVdS"
      },
      "similarity": 0.4850311577320099
    },
    {
      "paper": {
        "id": "7VkFMKBVVp",
        "number": 23625,
        "title": "The Rise of Parameter Specialization for Knowledge Storage in Large Language Models",
        "authors": [
          "Yihuai Hong",
          "Yiran Zhao",
          "Wei Tang",
          "Yang Deng",
          "Yu Rong",
          "Wenxuan Zhang"
        ],
        "abstract": "Over time, a growing wave of large language models from various series has been introduced to the community. Researchers are striving to maximize the performance of language models with constrained parameter sizes. However, from a microscopic perspective, there has been limited research on how to better store knowledge in model parameters, particularly within MLPs, to enable more effective utilization of this knowledge by the model. In this work, we analyze twenty publicly available open-source large language models to investigate the relationship between their strong performance and the way knowledge is stored in their corresponding MLP parameters. Our findings reveal that as language models become more advanced and demonstrate stronger knowledge capabilities, their parameters exhibit increased specialization. Specifically, parameters in the MLPs tend to be more focused on encoding similar types of knowledge. We experimentally validate that this specialized distribution of knowledge contributes to improving the efficiency of knowledge utilization in these models. Furthermore, by conducting causal training experiments, we confirm that this specialized knowledge distribution plays a critical role in improving the model's efficiency in leveraging stored knowledge.",
        "keywords": [
          "Large Language Models",
          "Interpretability",
          "Knowledge Storation in LLM"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=7VkFMKBVVp"
      },
      "similarity": 0.48374462127685547
    },
    {
      "paper": {
        "id": "fGBCRZQVse",
        "number": 14371,
        "title": "Theoretical Benefit and Limitation of Diffusion Language Model",
        "authors": [
          "Guhao Feng",
          "Yihan Geng",
          "Jian Guan",
          "Wei Wu",
          "Liwei Wang",
          "Di He"
        ],
        "abstract": "Diffusion language models have emerged as a new approach for text generation. By enabling the parallel sampling of multiple tokens in each diffusion step, they appear to offer a more efficient alternative to auto-regressive models. However, our observations show that current open-sourced diffusion language models require more sampling steps to achieve comparable accuracy on representative tasks--resulting in even higher inference costs than their auto-regressive counterparts. To investigate whether this is an inherent limitation, we conduct a rigorous theoretical analysis of a widely adopted variant: the Masked Diffusion Model (MDM). Surprisingly, our analysis reveals that the conclusion is highly sensitive to the choice of evaluation metric. Under mild conditions, we prove that when the target is near-optimal perplexity, MDMs can achieve this goal in a constant number of sampling steps, independent of sequence length. This result demonstrates that efficiency can, in principle, be attained without compromising generation quality. However, when targeting low sequence error rate--which is important for assessing the ``correctness\" of a generated sequence, such as a reasoning chain--we show that in the worst case, the required sampling steps must scale linearly with sequence length, thereby eliminating the efficiency advantage. Our analysis establishes the first theoretical foundation for understanding the comparative strengths and limitations of MDMs, offering practical guidance on when to favor MDMs over the auto-regressive models and vice versa.",
        "keywords": [
          "Discrete Diffusion Model",
          "Efficiency",
          "Masked Diffusion Model",
          "Language Modeling",
          "Metrics for NLP"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=fGBCRZQVse"
      },
      "similarity": 0.48327311873435974
    },
    {
      "paper": {
        "id": "HW55AwGEC8",
        "number": 12271,
        "title": "Model Merging in Pre-training of Large Language Models",
        "authors": [
          "Yunshui Li",
          "Yiyuan Ma",
          "Shen Yan",
          "Chaoyi Zhang",
          "Jing Liu",
          "Jianqiao Lu",
          "Ziwen Xu",
          "Mengzhao Chen",
          "Minrui Wang",
          "Shiyi Zhan",
          "Jin Ma",
          "Xunhao Lai",
          "Yao Luo",
          "Xingyan Bin",
          "Hongbin Ren",
          "Mingji Han",
          "Wenhao Hao",
          "Bairen Yi",
          "LingJun Liu",
          "Bole Ma",
          "Xiaoying Jia",
          "zhou Xun",
          "liang xiang",
          "Yonghui Wu"
        ],
        "abstract": "Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.",
        "keywords": [
          "Large language model; Pretrain; Model merge; Stable training;"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=HW55AwGEC8"
      },
      "similarity": 0.4830264449119568
    },
    {
      "paper": {
        "id": "10s01YrlKp",
        "number": 6332,
        "title": "metaTextGrad: Automatically optimizing language model optimizers",
        "authors": [
          "Guowei Xu",
          "Mert Yuksekgonul",
          "Carlos Guestrin",
          "James Zou"
        ],
        "abstract": "Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that using LLM-based optimizers to automatically optimize model prompts, demonstrations, predictions themselves, or other components can significantly enhance the performance of AI systems, as demonstrated by frameworks such as DSPy and TextGrad. However, optimizers built on language models themselves are usually designed by humans with manual design choices; optimizers themselves are not optimized. Moreover, these optimizers are general purpose by design, to be useful to a broad audience, and are not tailored for specific tasks. To address these challenges, we propose metaTextGrad, which focuses on designing a meta-optimizer to further enhance existing optimizers and align them to be good optimizers for a given task. Our approach consists of two key components: a meta prompt optimizer and a meta structure optimizer. The combination of these two significantly improves performance across multiple benchmarks, achieving an average absolute performance improvement of up to 6% compared to the best baseline.",
        "keywords": [
          "programming models",
          "prompting techniques",
          "meta-learning",
          "LLM optimizer"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=10s01YrlKp"
      },
      "similarity": 0.48258769512176514
    },
    {
      "paper": {
        "id": "OEawM2coNT",
        "number": 7925,
        "title": "Partition to Evolve: Niching-enhanced Evolution with LLMs for Automated Algorithm Discovery",
        "authors": [
          "Qinglong Hu",
          "Qingfu Zhang"
        ],
        "abstract": "Large language model-assisted Evolutionary Search (LES) has emerged as a promising approach for Automated Algorithm Discovery (AAD). While many evolutionary search strategies have been developed for classic optimization problems, LES operates in abstract language spaces, presenting unique challenges for applying these strategies effectively. To address this, we propose a general LES framework that incorporates feature-assisted niche construction within abstract search spaces, enabling the seamless integration of niche-based search strategies from evolutionary computation. Building on this framework, we introduce PartEvo, an LES method that combines niche collaborative search and advanced prompting strategies to improve algorithm discovery efficiency. Experiments on both synthetic and real-world optimization problems show that PartEvo outperforms human-designed baselines and surpasses prior LES methods, such as Eoh and Funsearch. In particular, on resource scheduling tasks, PartEvo generates meta-heuristics with low design costs, achieving up to 90.1\\% performance improvement over widely-used baseline algorithms, highlighting its potential for real-world applications.",
        "keywords": [
          "Automated algorithm discovery",
          "Evolutionary computation",
          "Large language model"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=OEawM2coNT"
      },
      "similarity": 0.48236191272735596
    },
    {
      "paper": {
        "id": "yISJGSdzdd",
        "number": 15661,
        "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling",
        "authors": [
          "Yuxi Liu",
          "Renjia Deng",
          "Yutong He",
          "Xue Wang",
          "Tao Yao",
          "Kun Yuan"
        ],
        "abstract": "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization.  To overcome these limitations, we propose **M**odule-wise **I**mportance **SA**mpling (**MISA**), a novel method that divides each layer into smaller modules and assigns importance scores to each module. \nMISA uses a weighted random sampling mechanism to activate modules, provably reducing\ngradient variance compared to layer-wise sampling. \nAdditionally, we establish an $\\mathcal{O}(1/\\sqrt{K})$ convergence rate under non-convex and stochastic conditions, where $K$ is the total number of training steps, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods.  Experiments on diverse learning tasks validate the effectiveness of MISA.",
        "keywords": [
          "large language models",
          "memory-efficient fine-tuning",
          "block coordinate descent",
          "importance sampling"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=yISJGSdzdd"
      },
      "similarity": 0.480174720287323
    },
    {
      "paper": {
        "id": "eszmES7j1F",
        "number": 11007,
        "title": "Flatten Graphs as Sequences: Transformers are Scalable Graph Generators",
        "authors": [
          "Dexiong Chen",
          "Markus Krimmel",
          "Karsten Borgwardt"
        ],
        "abstract": "We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.",
        "keywords": [
          "graph generation",
          "transformers",
          "autoregressive modeling",
          "language models",
          "LLMs"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=eszmES7j1F"
      },
      "similarity": 0.47877657413482666
    },
    {
      "paper": {
        "id": "3G0IWDIoRG",
        "number": 13443,
        "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation",
        "authors": [
          "Erfan Baghaei Potraghloo",
          "Seyedarmin Azizi",
          "Souvik Kundu",
          "Massoud Pedram"
        ],
        "abstract": "Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-*p* (nucleus) sampling, and min-*p* sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-*p* sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution.  To effectively incorporate the model confidence, this paper presents **_top-H_ decoding**. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization (ECMM)** problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem.  Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-*p* sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at [https://github.com/ErfanBaghaei/Top-H-Decoding](https://github.com/ErfanBaghaei/Top-H-Decoding).",
        "keywords": [
          "LLMs",
          "Truncated sampling",
          "Decoding Strategies"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=3G0IWDIoRG"
      },
      "similarity": 0.4775330126285553
    },
    {
      "paper": {
        "id": "ZtXT584LrT",
        "number": 16531,
        "title": "Large Language Bayes",
        "authors": [
          "Justin Domke"
        ],
        "abstract": "Many domain experts do not have the time or expertise to write formal Bayesian models. This paper takes an informal problem description as input, and combines a large language model and a probabilistic programming language to define a joint distribution over formal models, latent variables, and data. A posterior over latent variables follows by conditioning on observed data and integrating over formal models. This presents a challenging inference problem. We suggest an inference recipe that amounts to generating many formal models from the large language model, performing approximate inference on each, and then doing a weighted aver- age. This is justified and analyzed as a combination of self-normalized importance sampling, MCMC, and importance-weighted variational inference. Experimentally, this produces sensible predictions from only data and an informal problem description, without the need to specify a formal model.",
        "keywords": [
          "bayesian inference",
          "variational inference"
        ],
        "primary_area": "probabilistic_methods",
        "forum_url": "https://openreview.net/forum?id=ZtXT584LrT"
      },
      "similarity": 0.47659578919410706
    },
    {
      "paper": {
        "id": "HvklLrtyxK",
        "number": 17762,
        "title": "Doubly Robust Alignment for Large Language Models",
        "authors": [
          "Erhan Xu",
          "Kai Ye",
          "Hongyi Zhou",
          "Luhan Zhu",
          "Francesco Quinzan",
          "Chengchun Shi"
        ],
        "abstract": "This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM",
        "keywords": [
          "Large language models",
          "Doubly robust methods",
          "Reinforcement learning from human feedback",
          "Semiparametric efficiency"
        ],
        "primary_area": "general_machine_learning",
        "forum_url": "https://openreview.net/forum?id=HvklLrtyxK"
      },
      "similarity": 0.47632095217704773
    },
    {
      "paper": {
        "id": "7Spt8cAJq0",
        "number": 21868,
        "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
        "authors": [
          "Wei Zhu",
          "Zhiwen Tang",
          "Kun Yue"
        ],
        "abstract": "Recent advancements have increasingly focused on leveraging large language models (LLMs) to construct autonomous agents for complex problem-solving tasks. However, existing approaches predominantly employ a single-agent framework to generate search branches and estimate rewards during Monte Carlo Tree Search (MCTS) planning. This single-agent paradigm inherently limits exploration capabilities, often resulting in insufficient diversity among generated branches and suboptimal planning performance.\nTo overcome these limitations, we propose  $\\textbf{SY}$nergistic $\\textbf{M}$ulti-agent $\\textbf{P}$lanning with $\\textbf{H}$eter$\\textbf{O}$geneous la$\\textbf{N}$gauge model assembl$\\textbf{Y}$ ($\\textbf{SYMPHONY}$),  a novel multi-agent planning framework that integrates a pool of heterogeneous language model-based agents. \nBy leveraging diverse reasoning patterns across agents, SYMPHONY enhances rollout diversity and facilitates more effective exploration.\nEmpirical results across multiple benchmark tasks show that SYMPHONY achieves strong performance even when instantiated with open-source LLMs deployable on consumer-grade hardware. When enhanced with cloud-based LLMs accessible via API, SYMPHONY demonstrates further improvements, outperforming existing state-of-the-art baselines and underscoring the effectiveness of heterogeneous multi-agent coordination in planning tasks.",
        "keywords": [
          "MCTS; Multi-Agent; Sequential Decision Making; LLM"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=7Spt8cAJq0"
      },
      "similarity": 0.47511914372444153
    },
    {
      "paper": {
        "id": "stpe7UeETz",
        "number": 13366,
        "title": "Corrector Sampling in Language Models",
        "authors": [
          "Itai Gat",
          "Neta Shaul",
          "Uriel Singer",
          "Yaron Lipman"
        ],
        "abstract": "Autoregressive language models accumulate errors due to their fixed, irrevocable left-to-right token generation. To address this, we propose a new sampling method called Resample-Previous-Tokens (RPT). RPT mitigates error accumulation by iteratively revisiting and potentially replacing tokens in a window of previously generated text. Fine-tuning a pretrained 8B parameter model with RPT for only 100B resulted in ~10% relative improvements on reasoning and coding benchmarks compared to the standard sampling.",
        "keywords": [
          "Language modeling",
          "Sampling"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=stpe7UeETz"
      },
      "similarity": 0.4710582494735718
    },
    {
      "paper": {
        "id": "btZm6DUaDO",
        "number": 2855,
        "title": "Quadratic Coreset Selection: Certifying and Reconciling Sequence and Token Mining for Efficient Instruction Tuning",
        "authors": [
          "Ziliang Chen",
          "Yongsen Zheng",
          "Zhao-Rong Lai",
          "Zhanfu Yang",
          "Cuixi Li",
          "Yang Liu",
          "Liang Lin"
        ],
        "abstract": "Instruction-Tuning (IT) was recently found the impressive data efficiency in post-training large language models (LLMs). While the pursuit of efficiency predominantly focuses on sequence-level curation, often overlooking the nuanced impact of critical tokens and the inherent risks of token noise and biases. Drawing inspiration from bi-level coreset selection, our work provides the principled view of the motivation behind selecting instructions' responses. It leads to our approach Quadratic Coreset Selection (QCS) that reconciles sequence-level and token-level influence contributions, deriving more expressive LLMs with established theoretical result. Despite the original QCS framework challenged by prohibitive computation from inverted LLM-scale Hessian matrices, we overcome this barrier by proposing a novel QCS probabilistic variant, which relaxes the original formulation through re-parameterized densities. This innovative solver is efficiently learned using hierarchical policy gradients without requiring back-propagation, achieving provable convergence and certified asymptotic equivalence to the original objective. Our experiments demonstrate QCS's superior sequence-level data efficiency and reveal how strategically leveraging token-level influence elevates the performance ceiling of data-efficient IT. Furthermore, QCS's adaptability is showcased through its successes in regular IT and challenging targeted IT scenarios, particularly in the cases of free-form complex instruction-following and CoT reasoning. They underscore QCS's potential for a wide array of versatile post-training applications.",
        "keywords": [
          "instruction tuning;principled data selection for LLM"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=btZm6DUaDO"
      },
      "similarity": 0.4705711007118225
    },
    {
      "paper": {
        "id": "tM4cHBD7kD",
        "number": 10628,
        "title": "PseuZO: Pseudo-Zeroth-Order Algorithm for Training Deep Neural Networks",
        "authors": [
          "Pengyun Yue",
          "Xuanlin Yang",
          "Mingqing Xiao",
          "Zhouchen Lin"
        ],
        "abstract": "Zeroth-order Optimization (ZO) has received wide attention in machine learning, especially when computing full gradient is expensive or even impossible. Recently, ZO has emerged as an important paradigm for memory-efficient fine-tuning of large language models (LLMs), circumventing the memory overhead of backpropagation. However, existing ZO gradient estimators exhibit dimension-dependent variance scaling as $\\Theta(d)$, leading to dimension-dependent convergence rates without further assumptions on the objective function, which is prohibitive for large-scale LLM parameters. To address this problem, we present a Pseudo-Zeroth-Order (PseuZO) framework for optimizing composite objective functions, especially large-scale models: $ \\min_{\\mathbf{x} \\in \\mathcal{X}} \\mathcal{F}(\\mathbf{x})= \\bbE_{\\mathbf{z}} g\\circ h(\\mathbf{x};\\mathbf{z}) $, where $h$ represents complex, high-dimensional representations and $g$ is a task-specific loss. While existing zeroth-order methods estimate gradients with final loss functions, our PseuZO algorithm estimate the Jacobian matrix of $h(\\mathbf{x})$ with the model output $\\mathbf{o}= h(\\mathbf{x})$, and the gradient of the loss function on model output $\\mathbf{e} = \\nabla_{\\mathbf{o}} g(\\mathbf{o})$, and apply exponential moving average on Jacobian estimators to reduce the variance. Moreover, we use the sliding window technique to reduce memory costs. Our algorithm achieves an $O( \\max \\lbrace \\alpha_1 L\\epsilon^{-2}, \\alpha_1 L \\sigma_2^2\\epsilon^{-4} \\rbrace )$ convergence rate, where $\\alpha_1$ is the effective dimension of $\\mathcal{F}$.\n  Experimental results demonstrate that PseuZO outperforms MeZO and MeZO-SVRG in classification, multiple choice and generation tasks in both full-parameter and PEFT fine-tuning settings by boosting convergence in the early stages of training. For instance, under the same computation time, with respect to SST2 task, PesuZO gets 9.8\\% higher accuracy than MeZO (91.2\\% v.s. 82.4\\%). With the sliding window technique, our PseuZO achieves $70\\%\\sim80\\%$ memory reduction compared to FO-SGD for different model sizes as PseuZO only introduced a small dimension-independent memory overhead, which enables efficient scaling of the model size. The code is available at https://github.com/YangBigMn/PseuZO.\n$\\newcommand{\\bbE}{\\mathbb{E}}$",
        "keywords": [
          "zeroth-order optimization",
          "LLM fine-tuning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=tM4cHBD7kD"
      },
      "similarity": 0.47029784321784973
    },
    {
      "paper": {
        "id": "MLprqOvAAK",
        "number": 16961,
        "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
        "authors": [
          "Panagiotis Giannoulis",
          "Yorgos Pantis",
          "Christos Tzamos"
        ],
        "abstract": "Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures,  our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling.\nOur method integrates imitation learning of simple\nSudoku rules with an explicit Depth-First Search (DFS) exploration strategy\ninvolving informed guessing and backtracking.\nMoving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. \nWe provide a rigorous analysis of this setup by formalizing its connection to a contextual variant of $\\textit{Min-Sum Set Cover}$, a well-studied problem in algorithms and stochastic optimization.",
        "keywords": [
          "LLMs",
          "Transformers",
          "Combinatorial",
          "Sudoku",
          "SAT"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=MLprqOvAAK"
      },
      "similarity": 0.4683421552181244
    },
    {
      "paper": {
        "id": "QFjssnKdBI",
        "number": 21903,
        "title": "Reasoning Planning for Language Models",
        "authors": [
          "Bao Nguyen",
          "Hieu Trung Nguyen",
          "Ruifeng She",
          "Xiaojin Fu",
          "Viet Anh Nguyen"
        ],
        "abstract": "Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC.",
        "keywords": [
          "reasoning method selection",
          "two-tower framework",
          "accuracy-cost trade-off",
          "inference time scaling"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=QFjssnKdBI"
      },
      "similarity": 0.4682210385799408
    },
    {
      "paper": {
        "id": "aAxk0cw6GW",
        "number": 13327,
        "title": "Differentiable extensions with rounding guarantees for combinatorial optimization over permutations",
        "authors": [
          "Robert R Nerem",
          "Zhishang Luo",
          "Akbar Rafiey",
          "Yusu Wang"
        ],
        "abstract": "Continuously extending combinatorial optimization objectives is a powerful technique commonly applied to the optimization of set functions. However, few such methods exist for extending functions on permutations, despite the fact that many combinatorial optimization problems, such as the quadratic assignment problem (QAP) and the traveling salesperson problem (TSP), are inherently optimization over permutations. We present Birkhoff Extension (BE), an almost-everywhere-differentiable continuous polytime-computable extension of any real-valued function on permutations to doubly stochastic matrices. Key to this construction is our introduction of a continuous variant of the well-known Birkhoff decomposition. \nOur extension has several nice properties making it appealing for optimization problems. \nFirst, BE provides a rounding guarantee, namely any solution to the extension can be efficiently rounded to a permutation without increasing the function value. Furthermore, an approximate solution in the relaxed case will give rise to an approximate solution in the space of permutations. \nSecond, using BE, any real-valued optimization objective on permutations can be extended to an almost-everywhere-differentiable objective function over the space of doubly stochastic matrices. \nThis makes our BE amenable to not only gradient-descent based optimization, but also unsupervised neural combinatorial optimization where training often requires a differentiable loss. \nThird, based on the above properties, we present a simple optimization procedure which can be readily combined with existing optimization approaches to offer local improvements (i.e., the quality of the final solution is no worse than the initial solution). \nFinally, we also adapt our extension to optimization problems over a class of trees, such as Steiner tree and optimization-based hierarchical clustering.\nWe present experimental results to verify our theoretical results on several combinatorial optimization problems related to permutations.",
        "keywords": [
          "Combinatorial optimization",
          "Permutations",
          "Birkhoff-von-Neumann decomposition",
          "matching",
          "quadratic assignment"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=aAxk0cw6GW"
      },
      "similarity": 0.467504620552063
    },
    {
      "paper": {
        "id": "szBFUtBzWP",
        "number": 21447,
        "title": "TANDEM: Bi-Level Data Mixture Optimization with Twin Networks",
        "authors": [
          "Jiaxing Wang",
          "Deping Xiang",
          "Jin Xu",
          "Mingyang Yi",
          "Guoqiang Gong",
          "Zicheng Zhang",
          "Haoran Li",
          "pengzhang liu",
          "Zhen Chen",
          "Ke Zhang",
          "Ju Fan",
          "Qixia Jiang"
        ],
        "abstract": "The capabilities of large language models (LLMs) significantly depend on training data drawn from various domains. Optimizing domain-specific mixture ratios can be modeled as a bi-level optimization problem, which we simplify into a single-level penalized form and solve with twin networks: a proxy model trained on primary data and a dynamically updated reference model trained with additional data. Our proposed method, Twin Networks for bi-level DatA mixturE optiMization (TANDEM), measures the data efficacy through the difference between the twin models and up-weights domains that benefit more from the additional data. TANDEM provides theoretical guarantees and wider applicability, compared to prior approaches. Furthermore, our bi-level perspective suggests new settings to study domain reweighting such as data-restricted scenarios and supervised fine-tuning, where optimized mixture ratios significantly improve the performance. Extensive experiments validate TANDEM's effectiveness in all scenarios.",
        "keywords": [
          "data mixture optimization",
          "language models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=szBFUtBzWP"
      },
      "similarity": 0.467499315738678
    },
    {
      "paper": {
        "id": "FnFf7Ru2ur",
        "number": 13002,
        "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
        "authors": [
          "Mathurin VIDEAU",
          "Badr Youbi Idrissi",
          "Alessandro Leite",
          "Marc Schoenauer",
          "Olivier Teytaud",
          "David Lopez-Paz"
        ],
        "abstract": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.",
        "keywords": [
          "Language Modelling",
          "Large Language Models",
          "Hierarchical Models",
          "Byte Level Transformer",
          "Tokenization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=FnFf7Ru2ur"
      },
      "similarity": 0.4662628471851349
    },
    {
      "paper": {
        "id": "sbmYVM4zRr",
        "number": 4761,
        "title": "Chain-of-Model Learning for Language Model",
        "authors": [
          "Xiaohua Wang",
          "Kaitao Song",
          "Xu Tan",
          "Huiqiang Jiang",
          "Chengruidong Zhang",
          "Yongliang Shen",
          "Cen LU",
          "Zihao Li",
          "Zifan Song",
          "Caihua Shan",
          "Yansen Wang",
          "Kan Ren",
          "Xiaoqing Zheng",
          "Tao Qin",
          "Yuqing Yang",
          "Dongsheng Li",
          "Lili Qiu"
        ],
        "abstract": "In this paper, we propose a novel learning paradigm, termed *Chain-of-Model* (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style. thereby introducing great scaling efficiency in model training and inference flexibility in deployment.We introduce the concept of *Chain-of-Representation* (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains). In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise *Chain-of-Language-Model* (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a *KV sharing* mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models.",
        "keywords": [
          "Foundation Model",
          "Next-Gen Language Models",
          "Chain-of-Model",
          "Dynamic Activation",
          "Efficiency"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=sbmYVM4zRr"
      },
      "similarity": 0.4651855230331421
    },
    {
      "paper": {
        "id": "dEi1S731lk",
        "number": 9251,
        "title": "Parallel Scaling Law for Language Models",
        "authors": [
          "Mouxiang Chen",
          "Binyuan Hui",
          "Zeyu Cui",
          "Jiaxi Yang",
          "Dayiheng Liu",
          "Jianling Sun",
          "Junyang Lin",
          "Zhongxin Liu"
        ],
        "abstract": "It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce another and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $\\mathcal O(\\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\\times$ less memory increase and 6$\\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning. Our code and 67 trained model checkpoints are publicly available at https://github.com/QwenLM/ParScale and https://huggingface.co/ParScale.",
        "keywords": [
          "Large Language Models",
          "Scaling Law"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=dEi1S731lk"
      },
      "similarity": 0.4646950960159302
    },
    {
      "paper": {
        "id": "Tjw0ACu3NL",
        "number": 15669,
        "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
        "authors": [
          "Yong Liu",
          "Zirui Zhu",
          "Chaoyu Gong",
          "Minhao Cheng",
          "Cho-Jui Hsieh",
          "Yang You"
        ],
        "abstract": "While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, compared with exact gradients, ZO-based gradients usually exhibit an estimation error, which can significantly hurt the optimization process, leading to slower convergence and suboptimal solutions. In addition, we find that the estimation error will hurt more when adding to large weights instead of small weights. Based on this observation, this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO on the RTE task.",
        "keywords": [
          "Zeroth-Order Optimization",
          "Parameter-Efficient Fine Tuning"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=Tjw0ACu3NL"
      },
      "similarity": 0.46464937925338745
    },
    {
      "paper": {
        "id": "Hmepi1Fm2g",
        "number": 22897,
        "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression",
        "authors": [
          "Saibo Geng",
          "Nathan Ranchin",
          "Yunzhen Yao",
          "Maxime Peyrard",
          "Chris Wendler",
          "Michael Gastpar",
          "Robert West"
        ],
        "abstract": "Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized on general-purpose corpora. These tokenizers’ fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a novel method for achieving\ncontext-adaptive tokenization in LLMs at inference time. Leveraging an online data compression algorithm (Lempel–Ziv–Welch), zip2zip dynamically expands its active vocabulary at inference time by continuously replacing fragmented token sequences with more compact hypertokens, which it can immediately output during generation.  In doing so, the model refines its internal tokenization scheme to match\nthe token distribution of the current context, reducing redundancy and improving representational efficiency. zip2zip consists of three key components: (1) a tokenizer based on Lempel–Ziv–Welch compression that incrementally merges co-occurring tokens into reusable hypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that computes embeddings for newly formed hypertokens at runtime; and (3) a variant of autoregressive language modeling that pretrains the model to handle hypertokenized, compressed text sequences as inputs and outputs. We show that an existing LLM can be uptrained for zip2zip in 10 GPU-hours via parameter-efficient finetuning. The resulting LLM performs test-time adaptation, learning to use hypertokens in unseen contexts and reducing input and output tokens by 15–40%.\nCode and models are released at https://github.com/epfl-dlab/zip2zip.",
        "keywords": [
          "tokenization",
          "LLM",
          "language modeling",
          "inference efficiency",
          "large language model",
          "compression"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Hmepi1Fm2g"
      },
      "similarity": 0.4644194543361664
    },
    {
      "paper": {
        "id": "c6RDAutyNE",
        "number": 14435,
        "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
        "authors": [
          "Jiahao Yu",
          "Zelei Cheng",
          "Xian Wu",
          "Xinyu Xing"
        ],
        "abstract": "Large language models (LLMs) are increasingly used in various domains, showing impressive potential on various tasks. \n    Recently, reasoning LLMs have been proposed to improve the \\textit{reasoning} or \\textit{thinking} capabilities of LLMs to solve complex problems. \n    Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. \n    While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. In this paper, we introduce \\textbf{G}uided \\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. \n    GPO first identifies the `critical step' within a reasoning trajectory - a point that the model must carefully proceed so as to succeed at the problem. We locate the critical step by estimating the advantage function.\n    GPO then resets the policy to the critical step and samples the new rollout and prioritizes learning process on those rollouts. \n    This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance.\n    We demonstrate that GPO is not a standalone method, but rather a general strategy that can be integrated with various optimization methods to improve reasoning performance. \n    Besides theoretical analysis, our experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhances the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process.",
        "keywords": [
          "Large language model",
          "explainable reinforcement learning",
          "fine-tuning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=c6RDAutyNE"
      },
      "similarity": 0.4643624424934387
    },
    {
      "paper": {
        "id": "w2xl15ZbD3",
        "number": 1907,
        "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
        "authors": [
          "Shiwei Li",
          "Xiandi Luo",
          "Haozhao Wang",
          "Xing Tang",
          "Ziqiang Cui",
          "Dugang Liu",
          "Yuhua Li",
          "xiuqiang He",
          "Ruixuan Li"
        ],
        "abstract": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). \nLoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank.\nIn standard LoRA, all input tokens share the same weights and undergo an identical input-output projection.\nThis limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens.\nTo address this limitation, we propose **Token-wise Projected Low-Rank Adaptation (TopLoRA)**, which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner.\nFormally, the weights of TopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated from each input token $X$.\nNotably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections).\nExtensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. \nThe code is available at https://github.com/Leopold1423/toplora-neurips25.",
        "keywords": [
          "LoRA",
          "Token-wise Projection"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=w2xl15ZbD3"
      },
      "similarity": 0.4637869596481323
    },
    {
      "paper": {
        "id": "GuvQJGgbLm",
        "number": 25819,
        "title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones",
        "authors": [
          "Parsa Mirtaheri",
          "Ezra Edelman",
          "Samy Jelassi",
          "Eran Malach",
          "Enric Boix-Adserà"
        ],
        "abstract": "Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.",
        "keywords": [
          "reasoning",
          "chain-of-thought",
          "test-time scaling",
          "large language models",
          "transformers",
          "expressivity"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=GuvQJGgbLm"
      },
      "similarity": 0.46281182765960693
    },
    {
      "paper": {
        "id": "ZdmmOAN4h3",
        "number": 18773,
        "title": "Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining",
        "authors": [
          "Haochen Zhang",
          "Junze Yin",
          "Guanchu Wang",
          "Zirui Liu",
          "Lin Yang",
          "Tianyi Zhang",
          "Anshumali Shrivastava",
          "Vladimir Braverman"
        ],
        "abstract": "Low-rank optimization has emerged as a promising approach to enabling memory-efficient training of large language models (LLMs). Existing low-rank optimization methods typically project gradients onto a low-rank subspace, reducing the memory cost of storing optimizer states. A key challenge in these methods is selecting suitable subspaces to ensure an effective optimization trajectory. Most existing approaches select the dominant subspace to preserve gradient information, as this intuitively provides the best approximation. However, we find that in practice, the dominant subspace stops changing during pretraining, thereby constraining weight updates to similar subspaces. In this paper, we propose importance sampling for low-rank optimization in LLM pretraining with a provable convergence guarantee, which the dominant subspace approach does not have. Empirically, we demonstrate that our method significantly outperforms previous methods in LLM pretraining tasks.",
        "keywords": [
          "low-rank optimization",
          "large language models",
          "Adam",
          "memory efficiency"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=ZdmmOAN4h3"
      },
      "similarity": 0.46098601818084717
    },
    {
      "paper": {
        "id": "p3HBEtNDRY",
        "number": 21347,
        "title": "Streaming Attention Approximation via Discrepancy Theory",
        "authors": [
          "Ekaterina Kochetkova",
          "Kshiteej Sheth",
          "Insu Han",
          "Amir Zandieh",
          "Michael Kapralov"
        ],
        "abstract": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation. \n  \nOur main contribution is BalanceKV, a streaming algorithm for $\\epsilon$-approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks.",
        "keywords": [
          "Attention approximation",
          "Long-context Attention",
          "Large Language Models",
          "Discrepancy theory",
          "Self-Balancing Walk"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=p3HBEtNDRY"
      },
      "similarity": 0.46080687642097473
    },
    {
      "paper": {
        "id": "to1VYVar9W",
        "number": 16725,
        "title": "Breaking AR’s Sampling Bottleneck: Provable Acceleration via Diffusion Language Models",
        "authors": [
          "Gen Li",
          "Changxiao Cai"
        ],
        "abstract": "Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T<L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.",
        "keywords": [
          "diffusion model",
          "large language model (LLM)",
          "iteration complexity",
          "mutual information"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=to1VYVar9W"
      },
      "similarity": 0.4605422616004944
    },
    {
      "paper": {
        "id": "ZAu7sADxfh",
        "number": 15115,
        "title": "ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generation",
        "authors": [
          "Yuxuan Song",
          "Zhe Zhang",
          "Yu Pei",
          "Jingjing Gong",
          "Qiying Yu",
          "Zheng Zhang",
          "Mingxuan Wang",
          "Hao Zhou",
          "Jingjing Liu",
          "Wei-Ying Ma"
        ],
        "abstract": "Generative modeling of discrete variables is challenging yet crucial for applications in natural language processing and biological sequence design. We introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model inspired by progressive candidate pruning. SLM operates on simplex centroids, reducing generation complexity and enhancing scalability. Additionally, SLM incorporates a flexible implementation of classifier-free guidance, enhancing unconditional generation performance. Extensive experiments on DNA promoter and enhancer design, protein design, character-level and large-vocabulary language modeling demonstrate the competitive performance and strong potential of SLM. Our code can be found at https://github.com/GenSI-THUAIR/SLM.",
        "keywords": [
          "Discrete Generative Model; Biological Sequence Design;"
        ],
        "primary_area": "machine_learning_for_sciences",
        "forum_url": "https://openreview.net/forum?id=ZAu7sADxfh"
      },
      "similarity": 0.4590948820114136
    },
    {
      "paper": {
        "id": "aNpj43Uh35",
        "number": 15825,
        "title": "Multi-Objective One-Shot Pruning for Large Language Models",
        "authors": [
          "Weiyu Chen",
          "Hansi Yang",
          "Yunhao GOU",
          "Han Shi",
          "En-Liang Hu",
          "Zhenguo Li",
          "James Kwok"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but require substantial computational resources, limiting their deployment in resource-constrained environments. While one-shot pruning methods can reduce model size without expensive retraining, they typically optimize for single objectives, ignoring LLMs' multi-faceted applications. We introduce Multi-Objective One-Shot Pruning (MOSP), which formulates LLM pruning as a multi-objective optimization problem. MOSP efficiently generates a Pareto set of pruned models representing different capability trade-offs, allowing users to select solutions aligned with their preferences. The proposed approach identifies share core support while enabling specialized support. Experiments across various LLMs and sparsity levels demonstrate MOSP's superior performance in navigating multi-objective trade-offs compared to baseline methods.",
        "keywords": [
          "Multi-Objective Optimization",
          "Pruning",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=aNpj43Uh35"
      },
      "similarity": 0.45871078968048096
    },
    {
      "paper": {
        "id": "0VDmWjW456",
        "number": 1004,
        "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models",
        "authors": [
          "Xiaohao Liu",
          "Xiaobo Xia",
          "Weixiang Zhao",
          "Manyi Zhang",
          "Xianzhi Yu",
          "Xiu Su",
          "Shuo Yang",
          "See-Kiong Ng",
          "Tat-Seng Chua"
        ],
        "abstract": "Large language models (LLMs) have achieved notable progress. Despite their success, next-token prediction (NTP), the dominant method for LLM training and inference, is constrained in both contextual coverage and inference efficiency due to its inherently sequential process. To overcome these challenges, we propose leap multi-token prediction~(L-MTP), an innovative token prediction method that extends the capabilities of multi-token prediction (MTP) by introducing a leap-based mechanism. Unlike conventional MTP, which generates multiple tokens at adjacent positions, L-MTP strategically skips over intermediate tokens, predicting non-sequential ones in a single forward pass. This structured leap not only enhances the model's ability to capture long-range dependencies but also enables a decoding strategy specially optimized for non-sequential leap token generation, effectively accelerating inference. We theoretically demonstrate the benefit of L-MTP in improving inference efficiency. Experiments across diverse benchmarks validate its merit in boosting both LLM performance and inference speed. The source code is available at https://github.com/Xiaohao-Liu/L-MTP.",
        "keywords": [
          "large language models",
          "multi-token prediction",
          "inference acceleration"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=0VDmWjW456"
      },
      "similarity": 0.45762383937835693
    },
    {
      "paper": {
        "id": "d1dL1ymD6N",
        "number": 6806,
        "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
        "authors": [
          "haonan he",
          "Peng Ye",
          "Yuchen Ren",
          "yuan yuan",
          "LuyangZhou",
          "ShucunJu",
          "lei chen"
        ],
        "abstract": "Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning large language models (LLMs), with its effectiveness influenced by two key factors: rank selection and weight initialization. While numerous LoRA variants have been proposed to improve performance by addressing one of these aspects, they often compromise usability or computational efficiency. In this paper, we analyze and identify the core limitations of existing approaches and propose a novel framework—**GoRA** (**G**radient-driven Adaptive L**o**w **R**ank **A**daptation)—that simultaneously adapts both the rank and initialization strategy within a unified framework. GoRA leverages gradient information during training to dynamically assign optimal ranks and initialize low-rank adapter weights in an adaptive manner. To our knowledge, GoRA is the first method that not only addresses the limitations of prior approaches—which often focus on either rank selection or initialization in isolation—but also unifies both aspects within a single framework, enabling more effective and efficient adaptation. Extensive experiments across various architectures and modalities show that GoRA consistently outperforms existing LoRA-based methods while preserving the efficiency of vanilla LoRA. For example, when fine-tuning Llama3.1-8B-Base for mathematical reasoning, GoRA achieves a 5.13-point improvement over standard LoRA and even outperforms full fine-tuning by 2.05 points under high-rank settings. Code is available at: https://github.com/hhnqqq/MyTransformers.",
        "keywords": [
          "LLMs",
          "LoRA",
          "PEFT"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=d1dL1ymD6N"
      },
      "similarity": 0.45665210485458374
    },
    {
      "paper": {
        "id": "hSX7Dd8dxy",
        "number": 25917,
        "title": "Inference-Time Reward Hacking in Large Language Models",
        "authors": [
          "Hadi Khalaf",
          "Claudio Mayrink Verdun",
          "Alex Oesterling",
          "Himabindu Lakkaraju",
          "Flavio Calmon"
        ],
        "abstract": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM’s output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for  complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce $\\texttt{HedgeTune}$, an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups.",
        "keywords": [
          "reward hacking",
          "large language models",
          "inference time alignment",
          "information theory"
        ],
        "primary_area": "social_and_economic_aspects_of_machine_learning",
        "forum_url": "https://openreview.net/forum?id=hSX7Dd8dxy"
      },
      "similarity": 0.4558940827846527
    },
    {
      "paper": {
        "id": "zftxlb1AOo",
        "number": 25561,
        "title": "Structure-Aware Cooperative Ensemble Evolutionary Optimization on Combinatorial Problems with Multimodal Large Language Models",
        "authors": [
          "Jie Zhao",
          "Kang Hao Cheong"
        ],
        "abstract": "Evolutionary algorithms (EAs) have proven effective in exploring the vast solution spaces typical of graph-structured combinatorial problems. However, traditional encoding schemes, such as binary or numerical representations, often fail to straightforwardly capture the intricate structural properties of networks. Through employing the image-based encoding to preserve topological context, this study utilizes multimodal large language models (MLLMs) as evolutionary operators to facilitate structure-aware optimization over graph data. To address the visual clutter inherent in large-scale network visualizations, we leverage graph sparsification techniques to simplify structures while maintaining essential structural features. To further improve robustness and mitigate bias from different sparsification views, we propose a cooperative evolutionary optimization framework that facilitates cross-domain knowledge transfer and unifies multiple sparsified variants of diverse structures. Additionally, recognizing the sensitivity of MLLMs to network layout, we introduce an ensemble strategy that aggregates outputs from various layout configurations through consensus voting. Finally, experiments on real-world networks through various tasks demonstrate that our approach improves both the quality and reliability of solutions in MLLM-driven evolutionary optimization.",
        "keywords": [
          "Evolutionary Optimization",
          "Multimodal Large Language Models",
          "Complex Networks",
          "Combinatorial Optimization"
        ],
        "primary_area": "machine_learning_for_sciences",
        "forum_url": "https://openreview.net/forum?id=zftxlb1AOo"
      },
      "similarity": 0.45586347579956055
    },
    {
      "paper": {
        "id": "7kQjbCQwtT",
        "number": 20,
        "title": "Discovering Important Experts for Mixture-of-Experts Models Pruning Through a Theoretical Perspective",
        "authors": [
          "Weizhong Huang",
          "Yuxin Zhang",
          "Xiawu Zheng",
          "Fei Chao",
          "Rongrong Ji",
          "Liujuan Cao"
        ],
        "abstract": "Mixture-of-Experts (MoE) architectures enable efficient scaling of large language models but face prohibitive memory demands due to massive parameterization. Existing pruning methods rely on heuristic metrics or impractical enumeration of expert subsets, leading to suboptimal performance or scalability. In this paper, we propose Shapley-MoE, an efficient pruning method for MoE models inspired by cooperative game theory. By quantifying each expert’s contribution via Shapley value, our method identifies important experts without exhaustive combination evaluations. To overcome the NP-hard complexity of exact Shapley computation, we introduce a Monte Carlo sampling strategy for efficient approximation that reduces complexity to quadratic time. However, vanilla Monte Carlo sampling still faces issues of insufficient estimation accuracy and low sampling efficiency. To address these issues, we further propose two novel methods to improve sampling accuracy and efficiency: (1) Early Truncation, which early terminates unstable sampling steps caused by overly small expert subsets, and (2) Router-Guided Importance Sampling, which prioritize sampling important expert subsets using gating activation probabilities. Both theoretical and experimental analyses show that both methods can accelerate Shapley value estimation and improve accuracy. Extensive empirical evaluations demonstrate that our pruned MoE models outperform existing expert pruning methods. Notably, when applied to the Qwen2-57B-A14B model, our method reduces the number of experts by 25% with only a 0.92 increase in perplexity and over 96.4% of the average zero-shot accuracy is maintained.",
        "keywords": [
          "Mixture-of-Experts Models",
          "Network Pruning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=7kQjbCQwtT"
      },
      "similarity": 0.45473065972328186
    },
    {
      "paper": {
        "id": "j8XnFfTvXF",
        "number": 17612,
        "title": "KL-Regularized RLHF with Multiple Reference Models: Exact Solutions and Sample Complexity",
        "authors": [
          "Gholamali Aminian",
          "Amir R. Asadi",
          "Idan Shenfeld",
          "Youssef Mroueh"
        ],
        "abstract": "Recent methods for aligning large language models (LLMs) with human feedback predominantly rely on a single reference model, which limits diversity, model overfitting, and underutilizes the wide range of available pre-trained models. Incorporating multiple reference models has the potential to address these limitations by broadening perspectives, reducing bias, and leveraging the strengths of diverse open-source LLMs. However, integrating multiple reference models into reinforcement learning with human feedback (RLHF) frameworks poses significant theoretical challenges, where achieving exact solutions has remained an open problem. This paper presents the first \\emph{exact solution} to the multiple reference model problem in reverse KL-regularized RLHF. We introduce a comprehensive theoretical framework that includes rigorous statistical analysis and provides sample complexity guarantees. Additionally, we extend our analysis to forward KL-regularized RLHF, offering new insights into sample complexity requirements in multiple reference scenarios. Our contributions lay the foundation for more advanced and adaptable LLM alignment techniques, enabling the effective use of multiple reference models. This work paves the way for developing alignment frameworks that are both theoretically sound and better suited to the challenges of modern AI ecosystems.",
        "keywords": [
          "Alignment",
          "multiple reference model",
          "RLHF",
          "Sample complexity"
        ],
        "primary_area": "reinforcement_learning",
        "forum_url": "https://openreview.net/forum?id=j8XnFfTvXF"
      },
      "similarity": 0.4546118974685669
    },
    {
      "paper": {
        "id": "e8DrPuJekZ",
        "number": 5662,
        "title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA",
        "authors": [
          "Shuangyi Chen",
          "Yuanxin Guo",
          "Yue Ju",
          "Hardik Dalal",
          "Zhongwen Zhu",
          "Ashish J Khisti"
        ],
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) optimize federated training by reducing computational and communication costs.  We propose RoLoRA, a federated framework using alternating optimization to fine-tune LoRA adapters. Our approach emphasizes the importance of learning up and down projection matrices to enhance expressiveness and robustness. We use both theoretical analysis and extensive experiments to demonstrate the advantages of RoLoRA over prior approaches that either generate imperfect model updates or limit expressiveness of the model. We provide a theoretical analysis on a linear model to highlight the importance of learning both the down-projection and up-projection matrices in LoRA. We validate the insights on a non-linear model and separately provide a convergence proof under general conditions. To bridge theory and practice, we conducted extensive experimental evaluations on language models including RoBERTa-Large, Llama-2-7B on diverse tasks and FL settings to demonstrate the advantages of RoLoRA over other methods.",
        "keywords": [
          "LoRA",
          "Federated Learning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=e8DrPuJekZ"
      },
      "similarity": 0.4537273347377777
    },
    {
      "paper": {
        "id": "2T6GWekWSm",
        "number": 6313,
        "title": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs",
        "authors": [
          "Yi Hu",
          "Shijia Kang",
          "Haotong Yang",
          "Haotian Xu",
          "Muhan Zhang"
        ],
        "abstract": "Length generalization—the ability to solve problems longer than those seen during training—remains a critical challenge for large language models (LLMs). Previous work modifies positional encodings (PEs) and data formats to improve length generalization on specific symbolic tasks such as addition and sorting. However, these approaches are fundamentally limited to special tasks, often degrading general language performance. Furthermore, they are typically evaluated on small transformers trained from scratch on single tasks and can cause performance drop when applied during post-training stage of practical LLMs with general capabilities. Hu et al., (2024) proposed Rule-Following Fine-Tuning (RFFT) to improve length generalization in the post-training stage of LLMs. Despite its compatibility with practical models and strong performance, RFFT is proposed for single tasks too, requiring re-training for each individual task with extensive examples. In this paper, we study length generalization in multi-task settings and propose *Meta Rule-Following Fine-Tuning (Meta-RFFT)*, the first framework enabling robust *cross-task* length generalization. \nAs our first contribution, we construct a large length generalization dataset containing **86 tasks** spanning code execution, number processing, symbolic and logical reasoning tasks, beyond the common addition or multiplication tasks. Secondly, we show that cross-task length generalization is possible with Meta-RFFT—after training on a large number of tasks and instances, the models achieve remarkable length generalization ability on *unseen* tasks with *minimal fine-tuning or one-shot prompting*. For example, after fine-tuning on 1 to 5 digit addition, our 32B model **achieves 95% accuracy on 30 digit addition**, significantly outperforming the state-of-the-art reasoning models (DeepSeek-R1-671B: 72%; QwQ-32B: 32%), despite never seeing this task during RF-pretraining.",
        "keywords": [
          "length generalization",
          "systematic generalization",
          "algorithmic reasoning",
          "scratchpad",
          "LLM reasoning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=2T6GWekWSm"
      },
      "similarity": 0.4537140727043152
    },
    {
      "paper": {
        "id": "s3maemwE5M",
        "number": 20052,
        "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
        "authors": [
          "Zhuo Chen",
          "Oriol Mayné i Comas",
          "Zhuotao Jin",
          "Di Luo",
          "Marin Soljacic"
        ],
        "abstract": "We present a universal theoretical framework for understanding *long-context language modeling* based on a *bipartite* mutual information scaling law that we rigorously verify in natural language. We demonstrate that bipartite mutual information captures multi-token interactions distinct from and scaling independently of conventional two-point mutual information, and show that this provides a more complete characterization of the dependencies needed for accurately modeling long sequences. Leveraging this scaling law, we formulate the **L**ong-context **L**anguage **M**odeling (**L**$^2$**M**) condition, which lower bounds the necessary scaling of a model's history state—the latent variables responsible for storing past information—for effective long-context modeling. We validate the framework and its predictions on transformer and state-space models. Our work provides a principled foundation to understand long-context modeling and to design more efficient architectures with stronger long-context capabilities, with potential applications beyond natural language.",
        "keywords": [
          "language models",
          "information theory",
          "mutual information",
          "predictive information",
          "large language models",
          "long context length modeling",
          "long-range dependence",
          "scaling laws",
          "long-context language modeling",
          "sequence modeling",
          "autoregressive models",
          "mutual information estimation",
          "transformers",
          "recurrent neural networks",
          "state-space models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=s3maemwE5M"
      },
      "similarity": 0.45259958505630493
    },
    {
      "paper": {
        "id": "VUbwLjLkws",
        "number": 24344,
        "title": "Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf’s Law",
        "authors": [
          "Frederik Kunstner",
          "Francis Bach"
        ],
        "abstract": "Recent works have highlighted the optimization difficulties encountered by gradient descent in training the first and last layer of transformer-based language models, which are overcome by optimizers such as Adam. The problem appears linked to the heavy-tailed distribution of words in text data, where the frequency of the $k$th most frequent word $\\pi_k$ is proportional to $1/k$, following Zipf's law. To better understand the impact of the data distribution on training performance, we study a linear bigram model for next-token prediction when the tokens follow a power-law $\\pi_k \\propto 1/k^\\alpha$ parameterized by the exponent $\\alpha$. We derive optimization scaling laws for deterministic gradient descent and sign descent as a proxy for Adam as a function of the power $\\alpha \\geq 0$. This setting differs from existing theoretical investigations in scaling laws which assume that the eigenvalues of the data decay as a power with power $\\alpha > 1$. This assumption effectively makes the problem \"finite dimensional\" as most of the loss comes from a few of the largest eigencomponents. In comparison, we show that the problem is more difficult when the data have heavier tails. The case $\\alpha = 1$ as found in text is ``worst-case'' for gradient descent, in that the number of iterations required to reach a small relative error scales almost linearly with dimension. While the performance of sign descent also depends on the dimension, for Zipf-distributed data the number of iterations scales only with the square-root of the dimension, leading to a large improvement over gradient descent for large vocabularies.",
        "keywords": [
          "scaling law",
          "optimization",
          "gradient descent",
          "sign descent",
          "bigram model",
          "high dimension",
          "power-law",
          "zipf's law"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=VUbwLjLkws"
      },
      "similarity": 0.45198196172714233
    },
    {
      "paper": {
        "id": "8FN25PlktS",
        "number": 11557,
        "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization",
        "authors": [
          "Zixuan Huang",
          "Yikun Ban",
          "Lean Fu",
          "Xiaojie Li",
          "Zhongxiang Dai",
          "Jianxin Li",
          "deqing wang"
        ],
        "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies,  but these methods often overlook the impact of the evolving states of the language model during the optimization process.\nIn this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving batch-wise states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance.\nNotably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. \nThis work points to a promising new direction for improving LLM alignment through batch-wise sample selection, with potential generalization to RLHF and broader supervised learning paradigms.",
        "keywords": [
          "Reinforcement Learning",
          "Direct Preference Optimization"
        ],
        "primary_area": "general_machine_learning",
        "forum_url": "https://openreview.net/forum?id=8FN25PlktS"
      },
      "similarity": 0.4517660140991211
    },
    {
      "paper": {
        "id": "eHRFb3DSZS",
        "number": 16909,
        "title": "ZeCO: Zero-Communication Overhead Sequence Parallelism for Linear Attention",
        "authors": [
          "Yuhong Chou",
          "Zehao Liu",
          "Rui-Jie Zhu",
          "Xinyi Wan",
          "Tianjian Li",
          "Congying Chu",
          "Qian Liu",
          "Jibin Wu",
          "Zejun MA"
        ],
        "abstract": "Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary performance bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve practically end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a novel collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.",
        "keywords": [
          "sequence parallelism",
          "context parallelism",
          "linear attention",
          "long context training"
        ],
        "primary_area": "infrastructure",
        "forum_url": "https://openreview.net/forum?id=eHRFb3DSZS"
      },
      "similarity": 0.44970929622650146
    },
    {
      "paper": {
        "id": "DBybUx7ARy",
        "number": 5797,
        "title": "Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Expansion",
        "authors": [
          "Qijun Luo",
          "Yifei Shen",
          "Liangzu Peng",
          "Dongsheng Li",
          "Xiao Li"
        ],
        "abstract": "Finetuning large language models (LLMs) is a resource-intensive task for researchers in academia, with memory constraints posing a key bottleneck. A classic optimization method, block coordinate descent (BCD), significantly reduces memory cost by segmenting the trainable parameters into multiple blocks and optimizing one active block at a time while freezing the others. However, we identify that blindly applying BCD to train LLMs can be inefficient for two reasons. First, optimizing only the active block requires backpropagating through multiple deeper yet inactive blocks, resulting in wasteful computations. Second, the frozen blocks, when they are not quite close to optimality, can narrow the optimization landscape, potentially misguiding the training of the active block. To address these issues simultaneously, we propose integrating BCD with *landscape expansion*, which unfreezes the inactive blocks and updates them in a cost-efficient manner during the same backpropagation as the update to the active block. Experiments on 8B and 70B models demonstrate that our proposed method surpasses memory-efficient baselines and matches Adam's downstream performance while requiring only 24 GB of memory for the 8B model and 300 GB for the 70B model.",
        "keywords": [
          "Memory efficiency; optimization"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=DBybUx7ARy"
      },
      "similarity": 0.44938039779663086
    },
    {
      "paper": {
        "id": "SXxlb1miXS",
        "number": 25305,
        "title": "Aligning Transformers with Continuous Feedback via Energy Rank Alignment",
        "authors": [
          "Shriram Chennakesavalu",
          "Frank Hu",
          "Sebastian Ibarraran",
          "Grant M. Rotskoff"
        ],
        "abstract": "Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the \"alignment\" problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies.  \nWe show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. \nFurthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers and protein language models to generate molecules and protein sequences, respectively, with externally specified\nproperties and find that it does so robustly, searching through diverse parts of chemical space.",
        "keywords": [
          "transformers",
          "alignment",
          "preference optimization",
          "protein language models",
          "chemical language models",
          "property optimization"
        ],
        "primary_area": "machine_learning_for_sciences",
        "forum_url": "https://openreview.net/forum?id=SXxlb1miXS"
      },
      "similarity": 0.44925275444984436
    },
    {
      "paper": {
        "id": "wsR7VYXbdR",
        "number": 14076,
        "title": "HiMoLE: Towards OOD-Robust LoRA via Hierarchical Mixture of Experts",
        "authors": [
          "Yinuo Jiang",
          "Yan Xiaodong",
          "Keyan Ding",
          "Deng Zhao",
          "Lei Liang",
          "Qiang Zhang",
          "Huajun Chen"
        ],
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have enabled the efficient adaptation of large language models (LLMs) by updating only a small subset of parameters. However, their robustness under out-of-distribution (OOD) conditions remains insufficiently studied. In this paper, we identify the limitations of conventional LoRA in handling distributional shifts and propose $\\textbf{HiMoLE}$($\\textbf{Hi}$erarchical $\\textbf{M}$ixture of $\\textbf{L}$oRA $\\textbf{E}$xperts), a new framework designed to improve OOD generalization. HiMoLE integrates hierarchical expert modules and hierarchical routing strategies into the LoRA architecture and introduces a two-phase training procedure enhanced by a diversity-driven loss. This design mitigates negative transfer and promotes effective knowledge adaptation across diverse data distributions. We evaluate HiMoLE on three representative tasks in natural language processing. Experimental results evidence that HiMoLE consistently outperforms existing LoRA-based approaches, significantly reducing performance degradation on OOD data while improving in-distribution performance. Our work bridges the gap between parameter efficiency and distributional robustness, advancing the practical deployment of LLMs in real-world applications.",
        "keywords": [
          "OOD Robustness",
          "Large Language Models",
          "Parameter-Efficient Fine-Tuning",
          "Mixture of Experts."
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=wsR7VYXbdR"
      },
      "similarity": 0.4488561153411865
    },
    {
      "paper": {
        "id": "0RF80tUWuv",
        "number": 20273,
        "title": "RidgeLoRA: Matrix Ridge Enhanced Low-Rank Adaptation of Large Language Models",
        "authors": [
          "Junda Zhu",
          "Jun Ai",
          "Yujun Li",
          "Yichun Yin",
          "Yasheng Wang",
          "Lifeng Shang",
          "Qun Liu"
        ],
        "abstract": "As one of the state-of-the-art parameter-efficient fine-tuning~(PEFT) methods, Low-Rank Adaptation (LoRA) enables model optimization with reduced computational cost through trainable low-rank matrix. However, the low-rank nature makes it prone to produce a decrease in the representation ability, leading to suboptimal performance. In order to break this limitation, we propose RidgeLoRA, a lightweight architecture like LoRA that incorporates novel architecture and matrix ridge enhanced full-rank approximation, to match the performance of full-rank training, while eliminating the need for high memory and a large number of parameters to restore the rank of matrices. We provide a rigorous mathematical derivation to prove that RidgeLoRA has a better upper bound on the representations than vanilla LoRA. Furthermore, extensive experiments across multiple domains demonstrate that RidgeLoRA achieves better performance than other LoRA variants, and can even match or surpass full-rank training.",
        "keywords": [
          "Parameter-Efficient Fine-Tuning",
          "Large Language Model",
          "Low-Rank Adaptation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=0RF80tUWuv"
      },
      "similarity": 0.44873327016830444
    },
    {
      "paper": {
        "id": "CNxrp7u5gV",
        "number": 25090,
        "title": "Solving the Asymmetric Traveling Salesman Problem via Trace-Guided Cost Augmentation",
        "authors": [
          "Zhen Zhang",
          "Javen Qinfeng Shi",
          "Wee Sun Lee"
        ],
        "abstract": "The Asymmetric Traveling Salesman Problem (ATSP) ranks among the most fundamental and notoriously difficult problems in combinatorial optimization. We propose a novel continuous relaxation framework for the Asymmetric Traveling Salesman Problem (ATSP) by leveraging differentiable constraints that encourage acyclic structures and valid permutations. Our approach integrates a differentiable trace-based Directed Acyclic Graph (DAG) constraint with a doubly stochastic matrix relaxation of the assignment problem, enabling gradient-based optimization over soft permutations. We develop a projected exponentiated gradient method with adaptive step size to minimize tour cost while satisfying the relaxed constraints. To recover high-quality discrete tours, we introduce a greedy post-processing procedure that iteratively corrects subtours using cost-aware cycle merging. Our method achieves state-of-the-art performance on standard asymmetric TSP benchmarks and demonstrates competitive scalability and accuracy, particularly on large or asymmetric instances where heuristic solvers such as LKH-3 struggle.",
        "keywords": [
          "TSP",
          "Combinatorial Optimization"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=CNxrp7u5gV"
      },
      "similarity": 0.44757506251335144
    },
    {
      "paper": {
        "id": "Lwn1rLB8t7",
        "number": 22722,
        "title": "Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees",
        "authors": [
          "Fu Luo",
          "Yaoxin Wu",
          "Zhi Zheng",
          "Zhenkun Wang"
        ],
        "abstract": "Recent neural combinatorial optimization (NCO) methods have shown promising problem-solving ability without requiring domain-specific expertise. Most existing NCO methods use training and testing data with a fixed constraint value and lack research on the effect of constraint tightness on the performance of NCO methods. This paper takes the capacity-constrained vehicle routing problem (CVRP) as an example to empirically analyze the NCO performance under different tightness degrees of the capacity constraint. Our analysis reveals that existing NCO methods overfit the capacity constraint, and they can only perform satisfactorily on a small range of the constraint values but poorly on other values. To tackle this drawback of existing NCO methods, we develop an efficient training scheme that explicitly considers varying degrees of constraint tightness and propose a multi-expert module to learn a generally adaptable solving strategy. Experimental results show that the proposed method can effectively overcome the overfitting issue, demonstrating superior performance on the CVRP and CVRP with time windows (CVRPTW) with various constraint tightness degrees. The code is available at [https://github.com/CIAM-Group/Rethinking\\_Constraint\\_Tightness](https://github.com/CIAM-Group/Rethinking\\_Constraint\\_Tightness).",
        "keywords": [
          "Neural Combinatorial Optimization",
          "Vehicle Routing Problem",
          "Constraint Tightness"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=Lwn1rLB8t7"
      },
      "similarity": 0.44674617052078247
    },
    {
      "paper": {
        "id": "ZtB34bQI54",
        "number": 22686,
        "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models",
        "authors": [
          "Taha Entesari",
          "Arman Hatami",
          "Rinat Khaziev",
          "Anil Ramakrishna",
          "Mahyar Fazlyab"
        ],
        "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. \n    We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable, all without any extra computational overhead.\n    Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility.",
        "keywords": [
          "Machine Unlearning",
          "Large Language Models",
          "Constrained Optimization",
          "Primal‑Dual Algorithms",
          "Entropy‑Based Objectives"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=ZtB34bQI54"
      },
      "similarity": 0.4463762640953064
    },
    {
      "paper": {
        "id": "0lNwIIHWhZ",
        "number": 202,
        "title": "ComPO: Preference Alignment via Comparison Oracles",
        "authors": [
          "Peter Chen",
          "Xi Chen",
          "Wotao Yin",
          "Tianyi Lin"
        ],
        "abstract": "Direct alignment methods are increasingly used for aligning large language models (LLMs) with human preferences. However, these methods suffer from the issues of verbosity and likelihood displacement, which can be driven by the noisy preference pairs that induce similar likelihood for preferred and dispreferred responses. The contributions of this paper are two-fold. First, we propose a new preference alignment method based on zeroth-order, comparison-based optimization via comparison oracles and provide convergence guarantees for its basic scheme. Second, we improve our method using some heuristics and conduct the experiments to demonstrate the flexibility and compatibility of practical scheme in improving the performance of LLMs using noisy preference pairs. Evaluations are conducted across multiple base and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with benchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show the effectiveness of our method as an alternative to addressing the limitations of existing direct alignment methods. A highlight of our work is that we evidence the importance of designing specialized methods for preference pairs with distinct likelihood margin, which complements the recent findings in Razin et al (2025).",
        "keywords": [
          "LLM Preference Alignment",
          "Zeroth-order Optimization",
          "Comparison-based Optimization",
          "Noisy Preference Pairs"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=0lNwIIHWhZ"
      },
      "similarity": 0.4456877112388611
    },
    {
      "paper": {
        "id": "dCGQlVRa2B",
        "number": 14566,
        "title": "Beyond Value Functions: Single-Loop Bilevel Optimization under Flatness Conditions",
        "authors": [
          "Liuyuan Jiang",
          "Quan Xiao",
          "Lisha Chen",
          "Tianyi Chen"
        ],
        "abstract": "Bilevel optimization, a hierarchical optimization paradigm, has gained significant attention in a wide range of practical applications, notably in the fine-tuning of generative models. However, due to the nested problem structure, most existing algorithms require either the Hessian vector calculation or the nested loop updates, which are computationally inefficient in large language model (LLM) fine-tuning. In this paper, building upon the fully first-order penalty-based approach, we propose an efficient value function-free (\\textsf{PBGD-Free}) algorithm that eliminates the loop of solving the lower-level problem and admits fully single-loop updates. Inspired by the landscape analysis of representation learning-based LLM fine-tuning problem, we propose a relaxed flatness condition for the upper-level function and prove the convergence of the proposed value-function-free algorithm. We test the performance of the proposed algorithm in various applications and demonstrate its superior computational efficiency over the state-of-the-art bilevel methods.",
        "keywords": [
          "bilevel optimization",
          "Holder's condition",
          "value function",
          "penalty bilevel"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=dCGQlVRa2B"
      },
      "similarity": 0.4454345405101776
    },
    {
      "paper": {
        "id": "ZwBtDbuzjY",
        "number": 20361,
        "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models",
        "authors": [
          "Yanggan Gu",
          "Yuanyi Wang",
          "Zhaoyi Yan",
          "Yiming Zhang",
          "Qi Zhou",
          "Fei Wu",
          "Hongxia Yang"
        ],
        "abstract": "Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) —a critical phase for enhancing LLM performance—largely unexplored.\nThe current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion.\nInfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information.\nBy introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improves its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.",
        "keywords": [
          "Model Fusion",
          "Preference Optimization",
          "LLMs"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=ZwBtDbuzjY"
      },
      "similarity": 0.44529902935028076
    },
    {
      "paper": {
        "id": "8LMwwt8E2s",
        "number": 5214,
        "title": "Probabilistic Token Alignment for Large Language Model Fusion",
        "authors": [
          "Runjia Zeng",
          "James Chenhao Liang",
          "Cheng Han",
          "Zhiwen Cao",
          "Jiahao Liu",
          "Xiaojun Quan",
          "Yingjie Victor Chen",
          "Lifu Huang",
          "Tong Geng",
          "Qifan Wang",
          "Dongfang Liu"
        ],
        "abstract": "Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities.",
        "keywords": [
          "knowledge fusion",
          "model fusion",
          "LLMs",
          "token alignment",
          "optimal transport"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=8LMwwt8E2s"
      },
      "similarity": 0.4446065127849579
    },
    {
      "paper": {
        "id": "7aSBAw7tJf",
        "number": 5427,
        "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?",
        "authors": [
          "Xi Chen",
          "Kaituo Feng",
          "Changsheng Li",
          "Xunhao Lai",
          "Xiangyu Yue",
          "Ye Yuan",
          "Guoren Wang"
        ],
        "abstract": "Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. To resolve this, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes. \nFirst, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. In light of this, we propose a \\textit{norm-based scaling} method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to achieve this goal.\nMoreover, we find that there are potential loss spikes during training. To address this, we further put forward a norm-growth limiter to smooth the gradient.\nExtensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore. Notably, for pre-training LLaMA 7B, our Fira uses $8\\times$ smaller memory of optimizer states than Galore, yet outperforms it by a large margin.",
        "keywords": [
          "Large Language Model",
          "Memory Efficient Training",
          "Full-rank Training"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=7aSBAw7tJf"
      },
      "similarity": 0.44435203075408936
    },
    {
      "paper": {
        "id": "GBMzJLhsRj",
        "number": 12308,
        "title": "Provable Scaling Laws for the Test-Time Compute of Large Language Models",
        "authors": [
          "Yanxi Chen",
          "Xuchen Pan",
          "Yaliang Li",
          "Bolin Ding",
          "Jingren Zhou"
        ],
        "abstract": "We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.",
        "keywords": [
          "test-time compute",
          "inference scaling law",
          "large language model"
        ],
        "primary_area": "general_machine_learning",
        "forum_url": "https://openreview.net/forum?id=GBMzJLhsRj"
      },
      "similarity": 0.44358348846435547
    },
    {
      "paper": {
        "id": "2D4TuZyNnr",
        "number": 24649,
        "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving",
        "authors": [
          "Sujun Tang",
          "Christopher Priebe",
          "Rohan Mahapatra",
          "Lianhui Qin",
          "Hadi Esmaeilzadeh"
        ],
        "abstract": "While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.",
        "keywords": [
          "Compiler",
          "Optimization",
          "LLM",
          "LLMs for Code Reasoning and Optimization"
        ],
        "primary_area": "infrastructure",
        "forum_url": "https://openreview.net/forum?id=2D4TuZyNnr"
      },
      "similarity": 0.4431653618812561
    },
    {
      "paper": {
        "id": "xAHozxfuUW",
        "number": 7463,
        "title": "Conformal Information Pursuit for Interactively Guiding Large Language Models",
        "authors": [
          "Kwan Ho Ryan Chan",
          "Yuyan Ge",
          "Edgar Dobriban",
          "Hamed Hassani",
          "Rene Vidal"
        ],
        "abstract": "A significant use case of instruction-finetuned Large Language Models (LLMs) is to solve question-answering tasks interactively. In this setting, an LLM agent is tasked with making a prediction by sequentially querying relevant information from the user, as opposed to a single-turn conversation. This paper explores sequential querying strategies that aim to minimize the expected number of queries. One such strategy is Information Pursuit (IP), a greedy algorithm that at each iteration selects the query that maximizes information gain or equivalently minimizes uncertainty. However, obtaining accurate estimates of mutual information or conditional entropy for LLMs is very difficult in practice due to over- or under-confident LLM probabilities, which leads to suboptimal query selection and predictive performance. To better estimate the uncertainty at each iteration, we propose *Conformal Information Pursuit (C-IP)*, an alternative approach to sequential information gain based on conformal prediction sets. More specifically, C-IP leverages a relationship between prediction sets and conditional entropy at each iteration to estimate uncertainty based on the average size of conformal prediction sets. In contrast to conditional entropy, we find that conformal prediction sets are a distribution-free and robust method of measuring uncertainty. Experiments with 20 Questions show that C-IP obtains better predictive performance and shorter query-answer chains compared to previous approaches to IP and uncertainty-based chain-of-thought methods. Furthermore, extending to an interactive medical setting between a doctor and a patient on the MediQ dataset, C-IP achieves competitive performance with direct single-turn prediction while offering greater interpretability.",
        "keywords": [
          "Large Language Models",
          "Interactive Question Answering",
          "Uncertainty Quantification",
          "Twenty Questions"
        ],
        "primary_area": "general_machine_learning",
        "forum_url": "https://openreview.net/forum?id=xAHozxfuUW"
      },
      "similarity": 0.44209879636764526
    },
    {
      "paper": {
        "id": "iydmH9boLb",
        "number": 15912,
        "title": "Advancing Expert Specialization for Better MoE",
        "authors": [
          "Hongcan Guo",
          "Haolang Lu",
          "Guoshun Nan",
          "Bolun Chu",
          "Jialin Zhuang",
          "Yuan Yang",
          "Wenhao Che",
          "Xinye Cao",
          "Sicong Leng",
          "Qimei Cui",
          "Xudong Jiang"
        ],
        "abstract": "Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. \nHowever, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training.\nTo address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions.\nGradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process.\nExperimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. \nNotably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.",
        "keywords": [
          "Mixture of Experts",
          "Load Balancing",
          "Optimization",
          "Supervised Fine-Tuning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=iydmH9boLb"
      },
      "similarity": 0.44160374999046326
    },
    {
      "paper": {
        "id": "AiZxn84Wdo",
        "number": 18223,
        "title": "Training Language Models to Reason Efficiently",
        "authors": [
          "Daman Arora",
          "Andrea Zanette"
        ],
        "abstract": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in  problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.\n\nIn this work, we propose to train large reasoning models to reason efficiently. Our method incentivizes models to minimize unnecessary computational overhead while largely maintaining accuracy, thereby achieving substantial deployment efficiency gains. It  enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
        "keywords": [
          "Reasoning models",
          "efficiency"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=AiZxn84Wdo"
      },
      "similarity": 0.44126981496810913
    },
    {
      "paper": {
        "id": "QL3J1fyAFv",
        "number": 24918,
        "title": "Length Generalization via Auxiliary Tasks",
        "authors": [
          "Pranjal Awasthi",
          "Anupam Gupta",
          "Ravi Kumar"
        ],
        "abstract": "_Length generalization_, the ability of sequence models to\n  generalize to sequences longer than those encountered during\n  training, remains a key challenge for transformers,\n  especially in tasks requiring algorithmic reasoning. Existing\n  theoretical understanding of length generalization is limited, often providing\n  only asymptotic results or focusing on specific problem classes or\n  architectural variants, while empirical approaches frequently rely on\n  ad hoc and often fragile techniques.\n\n  In this work we introduce a novel framework for analyzing and\n  proving length generalization bounds under specified, verifiable assumptions. A key\n  outcome of the theory is the identification of a natural set of\n  _auxiliary_ tasks, intricately related to the primary task structure,\n  such that strong performance on these auxiliary tasks, alongside the\n  primary task, provably guarantees length generalization within the framework.  This\n  motivates a multi-task training procedure that explicitly optimizes\n  performance on both the primary and the identified auxiliary tasks.\n\n  Empirical evaluations on a variety of synthetic benchmarks\n  known to be challenging for length generalization, including sequence\n  sorting, and reversal, demonstrate that our proposed method yields\n  significant improvements in generalization to substantially longer\n  sequences.",
        "keywords": [
          "length generalization",
          "theory"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=QL3J1fyAFv"
      },
      "similarity": 0.4406917095184326
    },
    {
      "paper": {
        "id": "O1abxStFcy",
        "number": 13073,
        "title": "Exact Expressive Power of Transformers with Padding",
        "authors": [
          "William Merrill",
          "Ashish Sabharwal"
        ],
        "abstract": "Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer's expressive power without adding parameters? We consider transformers with *padding* tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding recognize precisely the class $\\mathsf{FO}$-uniform $\\mathsf{TC}^0$ of extremely parallelizable problems. While the $\\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via *looping*. Our core technical contribution is to show how padding helps bring the notions of *complete problems* and *reductions*, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $\\mathrm{O}(\\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\\mathsf{FO}$-uniform $\\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers' expressive power: with polylogarithmic looping, polynomially padded transformers recognize precisely the class $\\mathsf{FO}$-uniform $\\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\\mathsf{NC} = \\mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought for test-time compute.",
        "keywords": [
          "transformers",
          "padding tokens",
          "looped transformers",
          "expressivity",
          "circuit complexity",
          "inference-time compute"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=O1abxStFcy"
      },
      "similarity": 0.4397917687892914
    },
    {
      "paper": {
        "id": "Eyis2h3tba",
        "number": 23136,
        "title": "Set-LLM: A Permutation-Invariant LLM",
        "authors": [
          "Beni Egressy",
          "Jan Stühmer"
        ],
        "abstract": "While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs for multidocument tasks and as automated evaluators in AI pipelines. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while altogether eliminating order sensitivity.",
        "keywords": [
          "LLMs",
          "Large Language Models",
          "LLM evaluator",
          "LLM as a Judge",
          "Permutation Invariance",
          "Attention"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Eyis2h3tba"
      },
      "similarity": 0.43973517417907715
    },
    {
      "paper": {
        "id": "xpY3C8HxNh",
        "number": 13179,
        "title": "Escaping Collapse: The Strength of Weak Data for Large Language Model Training",
        "authors": [
          "Kareem Amin",
          "Sara Babakniya",
          "Alex Bie",
          "Weiwei Kong",
          "Umar Syed",
          "Sergei Vassilvitskii"
        ],
        "abstract": "Synthetically-generated data plays an increasingly larger role in training large language models. However, while synthetic data has been found to be useful, studies have also shown that without proper curation it can cause LLM performance to plateau, or even \"collapse\", after many training iterations. In this paper, we formalize this question and develop a theoretical framework to investigate how much curation is needed in order to ensure that LLM performance continually improves. Our analysis is inspired by boosting, a classic machine learning technique that leverages a very weak learning algorithm to produce an arbitrarily good classifier. The approach we analyze subsumes many recently proposed methods for training LLMs on synthetic data, and thus our analysis sheds light on why they are successful, and also suggests opportunities for future improvement. We present experiments that validate our theory, and show that dynamically focusing labeling resources on the most challenging examples --- in much the same way that boosting focuses the efforts of the weak learner --- leads to improved performance.",
        "keywords": [
          "Large-language models",
          "synthetic data"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=xpY3C8HxNh"
      },
      "similarity": 0.43824565410614014
    },
    {
      "paper": {
        "id": "vRVfgcoeIl",
        "number": 1794,
        "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models",
        "authors": [
          "Yulei Qin",
          "Gang Li",
          "Zongyi Li",
          "Zihan Xu",
          "Yuchen Shi",
          "Zhekai Lin",
          "Xiao Cui",
          "Ke Li",
          "Xing Sun"
        ],
        "abstract": "Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF.",
        "keywords": [
          "instruction following",
          "large language model",
          "reasoning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=vRVfgcoeIl"
      },
      "similarity": 0.4380235970020294
    },
    {
      "paper": {
        "id": "G7CiAs8xyw",
        "number": 16846,
        "title": "Foundations of Top-$k$ Decoding for Language Models",
        "authors": [
          "Georgy Noarov",
          "Soham Mallick",
          "Tao Wang",
          "Sunay Joshi",
          "Yan Sun",
          "Yangxinyu Xie",
          "Mengxin Yu",
          "Edgar Dobriban"
        ],
        "abstract": "Top-$k$ decoding is a widely used method for sampling from LLMs: at each token, only the largest $k$ next-token-probabilities are kept, and the next token is sampled after re-normalizing them to sum to unity. Top-$k$ and other sampling methods are motivated by the intuition that true next-token distributions are sparse, and the noisy LLM probabilities need to be truncated. However, to our knowledge, a precise theoretical motivation for the use of top-$k$ decoding is missing. In this work, we develop a theoretical framework that both explains and generalizes top-$k$ decoding. We view decoding at a fixed token as the recovery of a sparse probability distribution. We introduce *Bregman decoders* obtained by minimizing a separable Bregman divergence (for both the *primal* and *dual* cases) with a sparsity-inducing $\\ell_0$-regularization; in particular, these decoders are *adaptive* in the sense that the sparsity parameter $k$ is chosen depending on the underlying token distribution. Despite the combinatorial nature of the sparse Bregman objective, we show how to optimize it efficiently for a large class of divergences. We prove that (i) the optimal decoding strategies are greedy, and further that (ii) the objective is discretely convex in $k$, such that the optimal $k$ can be identified in logarithmic time. We note that standard top-$k$ decoding arises as a special case for the KL divergence, and construct new decoding strategies with substantially different behaviors (e.g., non-linearly up-weighting larger probabilities after re-normalization).",
        "keywords": [
          "Large Language Model",
          "Top-$k$ Decoding"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=G7CiAs8xyw"
      },
      "similarity": 0.4379522204399109
    },
    {
      "paper": {
        "id": "TDFSKAspoQ",
        "number": 11596,
        "title": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization",
        "authors": [
          "Da Chang",
          "Ganzhao Yuan"
        ],
        "abstract": "Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP.",
        "keywords": [
          "Stochastic Optimization; Convergence Analysis; Adaptive Optimizers ; Nonconvex Optimization; Large Language Models; Momentum-Gradient Alignment"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=TDFSKAspoQ"
      },
      "similarity": 0.43793511390686035
    },
    {
      "paper": {
        "id": "f4pvPNf9ox",
        "number": 1256,
        "title": "MURKA: Multi-Reward Reinforcement Learning with Knowledge Alignment for Optimization Tasks",
        "authors": [
          "Wantong Xie",
          "Yi-Xiang Hu",
          "Jieyang Xu",
          "Feng Wu",
          "Xiangyang Li"
        ],
        "abstract": "Optimization plays a central role in Operations Research (OR) and numerous industrial applications, yet automating the end-to-end process of translating natural language descriptions into executable optimization programs remains a formidable challenge. While recent efforts have applied Large Language Models (LLMs) to this task, existing approaches are hindered by high inference costs, limited robustness across domains, and weak verification mechanisms. In this work, we propose MURKA, a reinforcement learning and knowledge distillation-based framework that enhances LLM-driven optimization modeling via collaborative agent alignment. MURKA orchestrates three specialized agents---Extractor, Solver, and Checker---to achieve accurate problem understanding, robust formulation, and verifiable execution. The Extractor is trained using group relative policy optimization with a composite reward function that incorporates semantic correctness and execution fidelity. The Solver benefits from knowledge distillation from a powerful teacher model, yielding structurally valid and executable formulations in AMPL. The Checker iteratively verifies solution correctness via solver feedback. \nWe validate MURKA's generalizability through extensive experiments across diverse OR benchmarks, demonstrating its robustness and scalability.\nExperimental results on eight diverse OR benchmarks, including NLP4LP, ComplexOR, and NL4Opt, demonstrate that MURKA, built on the LLaMa3-8B backbone, achieves a 5.9\\% absolute improvement in solution accuracy and a 5.1\\% increase in execution success rate compared to leading baselines. These results establish MURKA as an effective and scalable paradigm for LLM-driven optimization, with strong potential for deployment in real-world OR applications.",
        "keywords": [
          "Large Language Models",
          "Reinforcement Learning",
          "Optimization Modeling",
          "Multi-Agent Systems",
          "Knowledge Distillation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=f4pvPNf9ox"
      },
      "similarity": 0.43679332733154297
    },
    {
      "paper": {
        "id": "hdT7UC7oG6",
        "number": 23358,
        "title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization",
        "authors": [
          "Colin Doumont",
          "Victor Picheny",
          "Viacheslav Borovitskiy",
          "Henry Moss"
        ],
        "abstract": "Bayesian Optimization (BO) has the potential to solve various combinatorial tasks, ranging from materials science to neural architecture search. However, BO requires specialized kernels to effectively model combinatorial domains. Recent efforts have introduced several combinatorial kernels, but the relationships among them are not well understood. To bridge this gap, we develop a unifying framework based on heat kernels, which we derive in a systematic way and express as simple closed-form expressions. Using this framework, we prove that many successful combinatorial kernels are either related or equivalent to heat kernels, and validate this theoretical claim in our experiments. Moreover, our analysis confirms and extends the results presented in Bounce: certain algorithms' performance decreases substantially when the unknown optima of the function do not have a certain structure. In contrast, heat kernels are not sensitive to the location of the optima. Lastly, we show that a fast and simple pipeline, relying on heat kernels, is able to achieve state-of-the-art results, matching or even outperforming certain slow or complex algorithms.",
        "keywords": [
          "Bayesian Optimization",
          "Combinatorial Spaces",
          "Heat Kernels"
        ],
        "primary_area": "probabilistic_methods",
        "forum_url": "https://openreview.net/forum?id=hdT7UC7oG6"
      },
      "similarity": 0.43631836771965027
    },
    {
      "paper": {
        "id": "BcKYVmh3yH",
        "number": 7777,
        "title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding",
        "authors": [
          "Yiming Wang",
          "Pei Zhang",
          "Siyuan Huang",
          "Baosong Yang",
          "Zhuosheng Zhang",
          "Fei Huang",
          "Rui Wang"
        ],
        "abstract": "Test-time scaling enhances large language model performance by allocating additional compute resources during decoding. Best-of-$N$ (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost–performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling:\n(1) Generating $N$ full samples consumes substantial GPU memory, reducing inference capacity under limited resources.\n(2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs.\nAlthough some studies have explored efficiency improvements, none have addressed both challenges at once.\nTo address this gap, we propose **Self-Truncation Best-of-$N$ (ST-BoN)**, a decoding method that avoids fully generating all $N$ samples and eliminates the need for reward models. It leverages early sampling consistency in the model’s internal states to identify the most promising path and truncate suboptimal ones.\nIn terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%.\nIn terms of cost–performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%–80%, and under the same cost, it can improve accuracy by 3–4 points.",
        "keywords": [
          "Large Language Models",
          "Test-Time Scaling",
          "Best-of-N Sampling",
          "Efficient Decoding",
          "Self-Estimation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=BcKYVmh3yH"
      },
      "similarity": 0.4363029897212982
    },
    {
      "paper": {
        "id": "4qVWY12KQT",
        "number": 20952,
        "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code",
        "authors": [
          "Hainan Fang",
          "Yuanbo Wen",
          "Jun Bi",
          "Yihan Wang",
          "Tonghui He",
          "Yanlin Tang",
          "Di Huang",
          "Jiaming Guo",
          "Rui Zhang",
          "Qi Guo",
          "Yunji Chen"
        ],
        "abstract": "Compilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques. However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge. Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance. These consistent improvements across diverse architectures (x86_64 and aarch64) and program distributions (NeuComBack L1 and L2) validate our method's superiority over conventional approaches and its potential for broader adoption in low-level neural compilation.",
        "keywords": [
          "Large Language Models",
          "Neural Compilation",
          "Compiler",
          "Code Generation",
          "Automatic Prompt Learning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=4qVWY12KQT"
      },
      "similarity": 0.4343623220920563
    },
    {
      "paper": {
        "id": "Hmd8CqMo3E",
        "number": 11978,
        "title": "Semantic-guided Diverse Decoding for Large Language Model",
        "authors": [
          "Weijie Shi",
          "Yue Cui",
          "Yaguang Wu",
          "Jingzhi Fang",
          "Shibo Zhang",
          "Mengze Li",
          "Sirui Han",
          "Jia Zhu",
          "Jiajie Xu",
          "Xiaofang Zhou"
        ],
        "abstract": "Diverse decoding of large language models is crucial for applications requiring multiple semantically distinct responses, yet existing methods primarily achieve lexical rather than semantic diversity. This limitation significantly constrains Best-of-N strategies, group-based reinforcement learning, and data synthesis. While temperature sampling and diverse beam search modify token distributions or apply n-gram penalties, they fail to ensure meaningful semantic differentiation. We introduce Semantic-guided Diverse Decoding (SemDiD), operating directly in embedding space that balances quality with diversity through three complementary mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment. SemDiD harmonizes these competing objectives using adaptive gain functions and constraint optimization, ensuring both quality thresholds and maximal semantic differentiation. Experiments show SemDiD consistently outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15% while increasing accuracy by up to 2.1%.",
        "keywords": [
          "Diverse Decoding",
          "Sampling Strategy",
          "Large Language Model"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Hmd8CqMo3E"
      },
      "similarity": 0.43391913175582886
    },
    {
      "paper": {
        "id": "MR7Fn23hSE",
        "number": 27385,
        "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows",
        "authors": [
          "Ruixiang ZHANG",
          "Shuangfei Zhai",
          "Jiatao Gu",
          "Yizhe Zhang",
          "Huangjie Zheng",
          "Tianrong Chen",
          "Miguel Ángel Bautista",
          "Joshua M. Susskind",
          "Navdeep Jaitly"
        ],
        "abstract": "Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility.\n  In this work, we explore an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space.\n  We propose a novel framework that employs transformer-based autoregressive normalizing flows to model these continuous representations.\n  This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process.\n  We further propose new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models.\n  Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework.",
        "keywords": [
          "Normalizing Flows",
          "Autoregressive Models",
          "Latent Autoregressive Flows"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=MR7Fn23hSE"
      },
      "similarity": 0.43390390276908875
    },
    {
      "paper": {
        "id": "v6vBK4t8vB",
        "number": 5483,
        "title": "Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training",
        "authors": [
          "Reza Shirkavand",
          "Peiran Yu",
          "Qi He",
          "Heng Huang"
        ],
        "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for downstream tasks using First-Order (FO) optimizers presents significant computational challenges. Parameter-Efficient Fine-Tuning~(PEFT) methods have been proposed to address these challenges by freezing most model parameters and training only a small subset. While PEFT is efficient, it may not outperform full fine-tuning when high task-specific performance is required.\nZeroth-Order (ZO) methods offer an alternative for fine-tuning the entire pre-trained model by approximating gradients using only the forward pass, thus eliminating the computational burden of back-propagation,\n% in first-order methods, \nbut they converge painfully slowly and are very sensitive to the choice of task prompts.\nWe bridge these worlds with Bilevel‑ZOFO, a penalty‑based bilevel formulation that treats adapter parameters as a lower‑level learner coupled to an upper‑level ZO optimizer of the full backbone. This double-loop optimization strategy only requires the gradient of the PEFT model and the forward pass of the base model. We provide theoretical convergence guarantees for Bilevel ZOFO. Empirically, we demonstrate that Bilevel-ZOFO significantly outperforms existing ZO methods, achieves 2–4$\\times$ faster training, and reduces sensitivity to prompts. Bilevel-ZOFO also outperforms FO PEFT methods while maintaining similar memory efficiency. Additionally, we show its strong potential for meta learning.",
        "keywords": [
          "arge language model",
          "Multi-Task Learning",
          "Zeroth-order Fine-tuning",
          "Parameter Efficient Fine-tuning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=v6vBK4t8vB"
      },
      "similarity": 0.43385493755340576
    },
    {
      "paper": {
        "id": "jHWCeU39Ft",
        "number": 28254,
        "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining",
        "authors": [
          "Zhixun Chen",
          "Ping Guo",
          "Wenhan Han",
          "Yifan Zhang",
          "BINBINLIU",
          "Haobin Lin",
          "Fengze Liu",
          "Yan Zhao",
          "Bingni Zhang",
          "Taifeng Wang",
          "Yin Zheng",
          "Trevor Cohn",
          "Meng Fang"
        ],
        "abstract": "Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English, neglecting other languages that are essential in the training mix for multilingual LLMs. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a multilingual autorater, capable of handling 17 languages. MuRating aggregates multiple English autoraters via pairwise comparisons to learn unified document quality scores, then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain LLaMA-architecture models of 1.2B and 7B parameters. Compared to strong baselines, including QuRater, FineWeb2-HQ, AskLLM, DCLM, our approach increases average accuracy on both English benchmarks and multilingual evaluations. Extensive analyses further validate that pairwise training provides greater stability and robustness than pointwise scoring, underscoring the effectiveness of MuRating as a general multilingual data-selection framework.",
        "keywords": [
          "Natural Language Processing"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=jHWCeU39Ft"
      },
      "similarity": 0.4329559803009033
    },
    {
      "paper": {
        "id": "I9VNWQ15Ni",
        "number": 12346,
        "title": "PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs",
        "authors": [
          "Jaewon Chu",
          "Seunghun Lee",
          "Hyunwoo J. Kim"
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable success across diverse domains, due to their strong instruction-following capabilities. This raised interest in optimizing instructions for black-box LLMs, whose internal parameters are inaccessible but popular for their strong performance and ease of use. Recent approaches leverage white-box LLMs to assist instruction optimization for black-box LLMs by generating instructions from soft prompts. However, white-box LLMs often map different soft prompts to the same instruction, leading to redundant queries to the black-box model. While previous studies regarded this many-to-one mapping as a redundancy to be avoided, we reinterpret it as useful prior knowledge that can enhance the optimization performance. To this end, we introduce PREimage-informed inSTruction Optimization (PRESTO), a novel framework that leverages the preimage structure of soft prompts to improve query efficiency. PRESTO consists of three key components: (1) score sharing, which shares the evaluation score with all soft prompts in a preimage; (2) preimage-based initialization, which select initial data points that maximize search space coverage using preimage information; and (3) score consistency regularization, which enforces prediction consistency within each preimage. By leveraging preimages, PRESTO observes 14 times more scored data under the same query budget, resulting in more efficient optimization. Experimental results on 33 instruction optimization tasks demonstrate the superior performance of PRESTO.",
        "keywords": [
          "Large Language Models",
          "Instruction Optimization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=I9VNWQ15Ni"
      },
      "similarity": 0.43256136775016785
    },
    {
      "paper": {
        "id": "KMbl8lg5Rv",
        "number": 22655,
        "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models",
        "authors": [
          "Haoyi Song",
          "Ruihan Ji",
          "Naichen Shi",
          "Fan Lai",
          "Raed Al Kontar"
        ],
        "abstract": "Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a fully probabilistic foundation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input–output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods.",
        "keywords": [
          "Uncertainty Quantification",
          "Probabilistic Modeling",
          "Random Walks",
          "Perturbations",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=KMbl8lg5Rv"
      },
      "similarity": 0.43255922198295593
    },
    {
      "paper": {
        "id": "byNNv5Et10",
        "number": 24116,
        "title": "3BASiL: An Algorithmic Framework for Sparse plus Low-Rank Compression of LLMs",
        "authors": [
          "Mehdi Makni",
          "Xiang Meng",
          "Rahul Mazumder"
        ],
        "abstract": "Sparse plus Low-Rank $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of Large Language Models (LLMs) has emerged as a promising direction in $\\textit{model compression}$, aiming to decompose pre-trained model weights into a sum of sparse and low-rank matrices $\\mathbf{W} \\approx \\mathbf{S} + \\mathbf{LR}$. Despite recent progress, existing methods often suffer from substantial performance degradation compared to dense models. In this work, we introduce $\\texttt{3BASiL-TM}$, an efficient one-shot post-training method for $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of LLMs that addresses this gap. Our approach first introduces a novel 3-Block Alternating Direction Method of Multipliers (ADMM) method, termed $\\texttt{3BASiL}$, to minimize the layer-wise reconstruction error with convergence guarantees. \nWe then design a transformer-matching ($\\texttt{TM}$) refinement step that jointly optimizes the sparse and low-rank components across transformer layers. This step minimizes a novel memory-efficient  loss that aligns outputs at the transformer level.\nNotably, the $\\texttt{TM}$ procedure is universal as it can enhance any $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition, including pure sparsity. Our numerical experiments show that $\\texttt{3BASiL-TM}$ reduces the WikiText2 perplexity gap to dense LLaMA-8B model by over 30% under a (2:4 Sparse + 64 LR) configuration, compared to prior methods. Moreover, our method achieves over 2.5x faster compression runtime on an A100 GPU compared to SOTA $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ method.\nOur code is available at https://github.com/mazumder-lab/3BASiL.",
        "keywords": [
          "Sparse plus Low-Rank",
          "Model Compression",
          "Large Language Models",
          "LoRA",
          "PEFT",
          "ADMM",
          "Optimization"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=byNNv5Et10"
      },
      "similarity": 0.43216484785079956
    },
    {
      "paper": {
        "id": "LbNL8xGai2",
        "number": 6835,
        "title": "S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient LLM Fine-tuning",
        "authors": [
          "Hanqing Zeng",
          "Yinglong Xia",
          "Zhuokai Zhao",
          "Chuan Jiang",
          "Qiang Zhang",
          "Jiayi Liu",
          "Qunshu Zhang",
          "Lizhu Zhang",
          "Xiangjun Fan",
          "Benyu Zhang"
        ],
        "abstract": "Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S’MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Conceptually, S’MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S’MoRE emulates the capacity of numerous experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S’MoRE’s residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S’MoRE improves structural flexibility of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S’MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation. Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.",
        "keywords": [
          "Mixture-of-experts",
          "low-rank adaptation",
          "language models",
          "graph neural networks"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=LbNL8xGai2"
      },
      "similarity": 0.43177658319473267
    },
    {
      "paper": {
        "id": "XUmGMBRv4M",
        "number": 19798,
        "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
        "authors": [
          "Akhiad Bercovich",
          "Mohammed Dabbah",
          "Omri Puny",
          "Ido Galil",
          "Amnon Geifman",
          "Yonatan Geifman",
          "Izhak Golan",
          "Ehud Dov Karpas",
          "Itay Levy",
          "Zach Moshe",
          "Najeeb Nabwani",
          "Tomer Ronen",
          "Itamar Schen",
          "Ido Shahaf",
          "Oren Tropp",
          "Ran Zilberstein",
          "Ran El-Yaniv"
        ],
        "abstract": "We introduce \\textit{FFN Fusion}, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create a 253B model (253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71$\\times$ speedup in inference latency and 35$\\times$ lower per-token cost while maintaining strong performance across benchmarks. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.",
        "keywords": [
          "LLM",
          "Architecture optimization",
          "Inference efficiency"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=XUmGMBRv4M"
      },
      "similarity": 0.43083304166793823
    },
    {
      "paper": {
        "id": "e4IlBqhbTO",
        "number": 23462,
        "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning",
        "authors": [
          "Antonios Valkanas",
          "Soumyasundar Pal",
          "Pavel Rumiantsev",
          "Yingxue Zhang",
          "Mark Coates"
        ],
        "abstract": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost.\nWe introduce **C3PO** (*Cost Controlled Cascaded Prediction Optimization*), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget.\nWe provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.",
        "keywords": [
          "large language model",
          "efficiency",
          "inference",
          "cascade"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=e4IlBqhbTO"
      },
      "similarity": 0.43076977133750916
    },
    {
      "paper": {
        "id": "X4SCxcgb3O",
        "number": 22663,
        "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo",
        "authors": [
          "Zachary Charles",
          "Gabriel Teston",
          "Lucio M. Dery",
          "J Keith Rush",
          "Nova Fallen",
          "Zachary Garrett",
          "Arthur Szlam",
          "Arthur Douillard"
        ],
        "abstract": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.",
        "keywords": [
          "distributed training",
          "communication-efficiency",
          "diloco",
          "parallel training"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=X4SCxcgb3O"
      },
      "similarity": 0.4298713207244873
    },
    {
      "paper": {
        "id": "4wnhbcppot",
        "number": 20901,
        "title": "Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms",
        "authors": [
          "William Réveillard",
          "Richard Combes"
        ],
        "abstract": "We consider a stochastic multi-armed bandit problem with i.i.d. rewards where the expected reward function is multimodal with at most $m$ modes. We propose the first known computationally tractable algorithm for computing the solution to the Graves-Lai optimization problem, which in turn enables the implementation of asymptotically optimal algorithms for this bandit problem.",
        "keywords": [
          "Multi-armed bandits",
          "Structured bandits",
          "Non-convex optimization"
        ],
        "primary_area": "reinforcement_learning",
        "forum_url": "https://openreview.net/forum?id=4wnhbcppot"
      },
      "similarity": 0.4293491244316101
    },
    {
      "paper": {
        "id": "zYEZ5KqtDO",
        "number": 17969,
        "title": "Heterogeneous Swarms: Jointly Optimizing Model Roles and Weights for Multi-LLM Systems",
        "authors": [
          "Shangbin Feng",
          "Zifeng Wang",
          "Palash Goyal",
          "Yike Wang",
          "Weijia Shi",
          "Huang Xia",
          "Hamid Palangi",
          "Luke Zettlemoyer",
          "Yulia Tsvetkov",
          "Chen-Yu Lee",
          "Tomas Pfister"
        ],
        "abstract": "We propose Heterogeneous Swarms, an algorithm to design multi-LLM systems by jointly optimizing model roles and weights. We represent multi-LLM systems as directed acyclic graphs (DAGs) of LLMs with topological message passing for collaborative generation. Given a pool of LLM experts and a utility function, Heterogeneous Swarms employs two iterative steps: role-step and weight-step. For role-step, we interpret model roles as learning a DAG that specifies the flow of inputs and outputs between LLMs. Starting from a swarm of random continuous adjacency matrices, we decode them into discrete DAGs, call the LLMs in topological order, evaluate on the utility function (e.g. accuracy on a task), and optimize the adjacency matrices with particle swarm optimization based on the utility score. For weight-step, we assess the contribution of individual LLMs in the multi-LLM systems and optimize model weights with swarm intelligence. We propose JFK-score to quantify the individual contribution of each LLM in the best-found DAG of the role-step, then optimize model weights with particle swarm optimization based on the JFK-score. Experiments demonstrate that Heterogeneous Swarms outperforms 17 role- and/or weight-based baselines by 18.5% on average across 12 tasks. Further analysis reveals that Heterogeneous Swarms discovers multi-LLM systems with heterogeneous model roles and substantial collaborative gains, and benefits from the diversity of language models.",
        "keywords": [
          "model collaboration",
          "multi-LLM systems"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=zYEZ5KqtDO"
      },
      "similarity": 0.42926302552223206
    },
    {
      "paper": {
        "id": "X5B2yTT97A",
        "number": 26755,
        "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
        "authors": [
          "Justin Wong",
          "Yury Orlovskiy",
          "Alexander Shypula",
          "Michael Luo",
          "Sanjit A. Seshia",
          "Joseph E. Gonzalez"
        ],
        "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as adversarial testing, search, and synthetic data generation, where diversity provides distinct answers across generations. Previous approaches rely solely on increasing the temperature, sacrificing quality. Furthermore, the model's next-token probabilities may not be representative of the true answer distribution. To combat these challenges, we propose SimpleStrat, an alternative that uses the language model itself to partition the solution space into strata from which to sample. \nTo measure resampling diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers. We propose measuring resampling diversity as the KL Divergence between the output distribution and the uniform distribution over valid ground truth answers and use recall as an alternative when assessing proprietary models. On CoverageQA, SimpleStrat improves diversity across all temperatures, showing orthogonal benefits. Quantifiably, we achieve as much as 2X better recall when applied to GPT-4o, and an average reduction in KL divergence by 0.36 when applied to Llama 3. Furthermore, we show that SimpleStrat achieves more resampling diversity at temperature T=0 than scaling temperature to T=1 on creative writing, an open-ended domain. Implementation and dataset available at https://github.com/jwong8314/simplestrat.",
        "keywords": [
          "Coverage",
          "Language Model Sampling",
          "Stratified Sampling",
          "Evaluation",
          "Dataset"
        ],
        "primary_area": "probabilistic_methods",
        "forum_url": "https://openreview.net/forum?id=X5B2yTT97A"
      },
      "similarity": 0.42872339487075806
    },
    {
      "paper": {
        "id": "aokiuaM7Lt",
        "number": 27164,
        "title": "Inference-time Alignment in Continuous Space",
        "authors": [
          "Yige Yuan",
          "Teng Xiao",
          "Li Yunfan",
          "Bingbing Xu",
          "Shuchang Tao",
          "Yunqi Qiu",
          "Huawei Shen",
          "Xueqi Cheng"
        ],
        "abstract": "Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \\textbf{77.51\\%}$ on AdvBench and $\\textbf{16.36\\%}$ on MATH. Code is publicly available at [this link](https://github.com/yuanyige/sea).",
        "keywords": [
          "Alignment"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=aokiuaM7Lt"
      },
      "similarity": 0.4286915063858032
    },
    {
      "paper": {
        "id": "x3qnrhfhX0",
        "number": 19273,
        "title": "Mixtures of Subspaces for Bandwidth Efficient Context Parallel Training",
        "authors": [
          "Sameera Ramasinghe",
          "Thalaiyasingam Ajanthan",
          "Hadi Mohaghegh Dolatabadi",
          "Gil Avraham",
          "Violetta Shevchenko",
          "Yan Zuo",
          "Chamin P Hewa Koneputugodage",
          "Alexander Long"
        ],
        "abstract": "Pretraining language models with extended context windows enhances their ability to leverage rich information during generation. Existing methods split input sequences into chunks, broadcast them across multiple devices, and compute attention block by block which incurs significant communication overhead. While feasible in high-speed clusters, these methods are impractical for decentralized training over low-bandwidth connections. We propose a compression method for communication-efficient context parallelism in decentralized settings, achieving a remarkable compression rate of over 95% with negligible overhead and no loss in convergence. Our key insight is to exploit the intrinsic low-rank structure of activation outputs by dynamically constraining them to learned mixtures of subspaces via efficient reparameterizations. We demonstrate scaling billion-parameter decentralized models to context lengths exceeding 100K tokens on networks as slow as 300Mbps, matching the wall-clock convergence speed of centralized models on 100Gbps interconnects.",
        "keywords": [
          "Decentralized training",
          "LLMs",
          "Compressio",
          "Distributed training"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=x3qnrhfhX0"
      },
      "similarity": 0.42713987827301025
    },
    {
      "paper": {
        "id": "Ice2BHIumz",
        "number": 8401,
        "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization",
        "authors": [
          "Jason Yang",
          "Wenda Chu",
          "Daniel Khalil",
          "Raul Astudillo",
          "Bruce James Wittmann",
          "Frances H. Arnold",
          "Yisong Yue"
        ],
        "abstract": "Protein fitness optimization involves finding a protein sequence that maximizes desired quantitative properties in a combinatorially large design space of possible sequences. Recent advances in steering protein generative models (e.g., diffusion models and language models) with labeled data offer a promising approach. However, most previous studies have optimized surrogate rewards and/or utilized large amounts of labeled data for steering, making it unclear how well existing methods perform and compare to each other in real-world optimization campaigns where fitness is measured through low-throughput wet-lab assays. In this study, we explore fitness optimization using small amounts (hundreds) of labeled sequence-fitness pairs and comprehensively evaluate strategies such as classifier guidance and posterior sampling for guiding generation from different discrete diffusion models of protein sequences. We also demonstrate how guidance can be integrated into adaptive sequence selection akin to Thompson sampling in Bayesian optimization, showing that plug-and-play guidance strategies offer advantages over alternatives such as reinforcement learning with protein language models. Overall, we provide practical insights into how to effectively steer modern generative models for next-generation protein fitness optimization.",
        "keywords": [
          "protein",
          "generative model",
          "diffusion model",
          "discrete diffusion model",
          "guidance",
          "language model",
          "active learning",
          "Bayesian optimization"
        ],
        "primary_area": "machine_learning_for_sciences",
        "forum_url": "https://openreview.net/forum?id=Ice2BHIumz"
      },
      "similarity": 0.4268583059310913
    },
    {
      "paper": {
        "id": "enhFXzKii4",
        "number": 22642,
        "title": "Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation",
        "authors": [
          "Fei Wang",
          "Li Shen",
          "Liang Ding",
          "Chao Xue",
          "Ye Liu",
          "Changxing Ding"
        ],
        "abstract": "Large Language Models (LLMs) excel at natural language processing tasks, but their massive size leads to high computational and storage demands.\nRecent works have sought to reduce their model size through layer-wise structured pruning.\nHowever, they tend to ignore retaining the capabilities in the pruned part. \nIn this work, we re-examine structured pruning paradigms and uncover several key limitations: 1) notable performance degradation due to direct layer removal, 2) incompetent linear weighted layer aggregation, and 3) the lack of effective post-training recovery mechanisms.\nTo address these limitations, we propose CoMe, including a progressive layer pruning framework with a Concatenation-based Merging technology and a hierarchical distillation post-training process. \nSpecifically, we introduce a channel sensitivity metric that utilizes activation intensity and weight norms for fine-grained channel selection. \nSubsequently, we employ a concatenation-based layer merging method to fuse the most critical channels in the adjacent layers, enabling a progressive model size reduction. \nFinally, we propose a hierarchical distillation protocol, which leverages the correspondences between the original and pruned model layers established during pruning, enabling efficient knowledge transfer.\nExperiments on seven benchmarks show that CoMe achieves state-of-the-art performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model retains 83% of its original average accuracy.",
        "keywords": [
          "Structured Pruning; Post Training"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=enhFXzKii4"
      },
      "similarity": 0.426757276058197
    },
    {
      "paper": {
        "id": "RlqYCpTu1P",
        "number": 18747,
        "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
        "authors": [
          "Enzhe Lu",
          "Zhejun Jiang",
          "Jingyuan Liu",
          "Yulun Du",
          "Tao Jiang",
          "Chao Hong",
          "Shaowei Liu",
          "Weiran He",
          "Enming Yuan",
          "Yuzhi Wang",
          "Zhiqi Huang",
          "Huan Yuan",
          "Suting Xu",
          "Xinran Xu",
          "Guokun Lai",
          "Yanru Chen",
          "Huabin Zheng",
          "Junjie Yan",
          "Jianlin Su",
          "Yuxin Wu",
          "Yutao Zhang",
          "Zhilin Yang",
          "Xinyu Zhou",
          "Mingxing Zhang",
          "Jiezhong Qiu"
        ],
        "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.\n\nIn this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to handle actual production workloads with long-context requirements, demonstrating significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.",
        "keywords": [
          "LLM",
          "Sparse Attention",
          "Long Context"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=RlqYCpTu1P"
      },
      "similarity": 0.42652627825737
    },
    {
      "paper": {
        "id": "KnqiC0znVF",
        "number": 15375,
        "title": "Large Language Diffusion Models",
        "authors": [
          "Shen Nie",
          "Fengqi Zhu",
          "Zebin You",
          "Xiaolu Zhang",
          "Jingyang Ou",
          "Jun Hu",
          "JUN ZHOU",
          "Yankai Lin",
          "Ji-Rong Wen",
          "Chongxuan Li"
        ],
        "abstract": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}.",
        "keywords": [
          "diffusion language models",
          "large language models",
          "masked diffusion models",
          "discrete diffusion models",
          "diffusion models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=KnqiC0znVF"
      },
      "similarity": 0.42635583877563477
    },
    {
      "paper": {
        "id": "GdrBPyUNPL",
        "number": 24172,
        "title": "Non-monotone Submodular Optimization: $p$-Matchoid Constraints and Fully Dynamic Setting",
        "authors": [
          "Kiarash Banihashem",
          "Samira Goudarzi",
          "MohammadTaghi Hajiaghayi",
          "Peyman Jabbarzade",
          "Morteza Monemizadeh"
        ],
        "abstract": "Submodular maximization subject to a $p$-matchoid constraint has various applications in machine learning, particularly in tasks such as feature selection, video and text summarization, movie recommendation, graph-based learning, and constraint-based optimization. We study this problem in the dynamic setting, where a sequence of insertions and deletions of elements to a $p$-matchoid $\\mathcal{M}(\\mathcal{V},\\mathcal{I})$ occurs over time and the goal is to efficiently maintain an approximate solution.\nWe propose a dynamic algorithm for non-monotone submodular maximization under a $p$-matchoid constraint. For a $p$-matchoid $\\mathcal{M}(\\mathcal{V},\\mathcal{I})$ of rank $k$, defined by a collection of $m$ matroids, our algorithm guarantees a $(2p + 2\\sqrt{p(p+1)} + 1 + \\epsilon)$-approximate solution at any time $t$ in the update sequence, with an expected amortized query complexity of $O(\\epsilon^{-3} pk^4 \\log^2(k))$ per update.",
        "keywords": [
          "optimization",
          "submodular",
          "p-matchoid",
          "non-monotone",
          "dynamic"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=GdrBPyUNPL"
      },
      "similarity": 0.4261167645454407
    },
    {
      "paper": {
        "id": "HvIRFV0J90",
        "number": 18776,
        "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning",
        "authors": [
          "Yeongbin Seo",
          "Dongha Lee",
          "Jaehyung Kim",
          "Jinyoung Yeo"
        ],
        "abstract": "Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the \\textbf{long decoding-window problem}, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks (sacrificing bidirectionality), but we find that this also leads to \\textbf{time-interval expansion problem}, sacrificing the speed. Therefore, semi-AR eliminates the main advantages of diffusion models. To overcome this, we propose Convolutional decoding (\\textit{Conv}), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements. The code is available online (\\url{https://github.com/ybseo-ac/Conv}).",
        "keywords": [
          "diffusion LLM",
          "large language models",
          "decoding",
          "semi-autoregressive",
          "non-autoregressive decoding"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=HvIRFV0J90"
      },
      "similarity": 0.4260803461074829
    },
    {
      "paper": {
        "id": "24tuzE5KZc",
        "number": 12076,
        "title": "OPTFM: A Scalable Multi-View Graph Transformer for Hierarchical Pre-Training in Combinatorial Optimization",
        "authors": [
          "Hao Yuan",
          "Wenli Ouyang",
          "Changwen Zhang",
          "Congrui Li",
          "Yong Sun"
        ],
        "abstract": "Foundation Models (FMs) have demonstrated remarkable success in fields like computer vision and natural language processing, yet their application to combinatorial optimization remains underexplored. Optimization problems, often modeled as graphs, pose unique challenges due to their diverse structures, varying distributions, and NP-hard complexity. To address these challenges, we propose OPTFM, the first graph foundation model for general combinatorial optimization. OPTFM introduces a scalable multi-view graph transformer with hybrid self-attention and cross-attention to model large-scale heterogeneous graphs in $O(N)$ time complexity while maintaining semantic consistency throughout the attention computation. A Dual-level pre-training framework integrates node-level graph reconstruction and instance-level contrastive learning, enabling robust and adaptable representations at multiple levels. Experimental results across diverse optimization tasks show that models trained on OPTFM embeddings without fine-tuning consistently outperform task-specific approaches, establishing a new benchmark for solving combinatorial optimization problems.",
        "keywords": [
          "combinatorial optimization",
          "foundation model",
          "graph transformer",
          "hierarchical pre-training"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=24tuzE5KZc"
      },
      "similarity": 0.426031231880188
    },
    {
      "paper": {
        "id": "Awl3ZhDHuQ",
        "number": 14313,
        "title": "Learning to Better Search with Language Models via Guided Reinforced Self-Training",
        "authors": [
          "Seungyong Moon",
          "Bumsoo Park",
          "Hyun Oh Song"
        ],
        "abstract": "While language models have shown remarkable performance across diverse tasks, they still encounter challenges in complex reasoning scenarios. Recent research suggests that language models trained on linearized search traces toward solutions, rather than solely on the final solutions, exhibit improved generalization, despite the search traces being potentially noisy or suboptimal. However, relying on such imperfect traces can result in inefficient use of test-time compute. To address this, we propose guided reinforced self-training (Guided-ReST), a fine-tuning algorithm designed to improve the model’s capability for effective search during inference. The key insight behind Guided-ReST is that optimal solutions can serve as valuable step-by-step landmarks to guide the model’s search process. Based on this insight, we introduce a novel data generation method that seamlessly incorporates optimal solutions into the model’s search procedure, enabling the generation of high-quality search traces. By fine-tuning the model on these search traces, we effectively distill improved search strategies into the model. Our method significantly enhances the search capabilities of language models on arithmetic reasoning and code self-repair tasks, including Countdown, CodeContests, and CodeForces. We release the source code at https://github.com/snu-mllab/guided-rest.",
        "keywords": [
          "learning to search",
          "self-improvement",
          "reinforcement learning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Awl3ZhDHuQ"
      },
      "similarity": 0.42578721046447754
    },
    {
      "paper": {
        "id": "KaYMGsnZ4R",
        "number": 25872,
        "title": "DINGO: Constrained Inference for Diffusion LLMs",
        "authors": [
          "Tarun Suresh",
          "Debangshu Banerjee",
          "Shubham Ugare",
          "Sasa Misailovic",
          "Gagandeep Singh"
        ],
        "abstract": "Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering substantial potential for improving runtime efficiency. However, existing diffusion models fail to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models, which generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, designed to enforce constraints with sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model’s predicted distribution while strictly adhering to any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a $68$\\% points of improvement over unconstrained inference.  The code is available at [**DINGO**](https://github.com/uiuc-focal-lab/DINGO).",
        "keywords": [
          "Constrained Decoding",
          "Diffusion LLM"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=KaYMGsnZ4R"
      },
      "similarity": 0.425673246383667
    },
    {
      "paper": {
        "id": "l8razJItEy",
        "number": 24038,
        "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study",
        "authors": [
          "Riccardo Alberghi",
          "Elizaveta Demyanenko",
          "Luca Biggio",
          "Luca Saglietti"
        ],
        "abstract": "Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question–trace–answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, under the same training-token budget, the latter models generalize better to unseen graphs. This benefit is not due to length alone—injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.",
        "keywords": [
          "Reasoning",
          "Chain-of-thought",
          "Next-token prediction",
          "Transformers",
          "Algorithm",
          "Shortest-Path"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=l8razJItEy"
      },
      "similarity": 0.4252799153327942
    },
    {
      "paper": {
        "id": "52Ehpe0Lu5",
        "number": 24003,
        "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation is Wasteful",
        "authors": [
          "Martin Marek",
          "Sanae Lotfi",
          "Aditya Somasundaram",
          "Andrew Gordon Wilson",
          "Micah Goldblum"
        ],
        "abstract": "Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes.  In particular, rather than holding the decay rate of the second moment fixed across batch sizes, we propose to hold its half-life fixed in terms of tokens. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas. Finally, we show that a small batch size combined with an optimizer with a small state size can provide the performance benefits of full fine-tuning while maintaining a similar memory footprint to LoRA.",
        "keywords": [
          "batch size",
          "language model",
          "LLM",
          "SGD",
          "Adam"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=52Ehpe0Lu5"
      },
      "similarity": 0.42490410804748535
    },
    {
      "paper": {
        "id": "tQZK5frjVU",
        "number": 5800,
        "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
        "authors": [
          "Xinran Gu",
          "Kaifeng Lyu",
          "Jiazheng Li",
          "Jingzhao Zhang"
        ],
        "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge.\nIn this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets—unlike training exclusively on knowledge-dense data—does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.",
        "keywords": [
          "knowledge acquisition",
          "memorization",
          "scaling laws",
          "LLMs"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=tQZK5frjVU"
      },
      "similarity": 0.4247751235961914
    },
    {
      "paper": {
        "id": "BLLixcuZgl",
        "number": 1685,
        "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
        "authors": [
          "Qizhe Zhang",
          "Mengzhen Liu",
          "Lichen Li",
          "Ming Lu",
          "Yuan Zhang",
          "Junwen Pan",
          "Qi She",
          "Shanghang Zhang"
        ],
        "abstract": "In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named **CDPruner**, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by **95\\%** and CUDA latency by **78\\%**, while maintaining **94\\%** of the original accuracy. Our code is available at https://github.com/Theia-4869/CDPruner.",
        "keywords": [
          "multimodal large language model",
          "visual token pruning",
          "training-free acceleration"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=BLLixcuZgl"
      },
      "similarity": 0.4245626628398895
    },
    {
      "paper": {
        "id": "g9sWQsqemL",
        "number": 1526,
        "title": "Boosting Resilience of Large Language Models through Causality-Driven Robust Optimization",
        "authors": [
          "Xiaoling Zhou",
          "Mingjie Zhang",
          "Zhemg Lee",
          "YUNCHENG HUA",
          "chengli xing",
          "Wei Ye",
          "Flora D. Salim",
          "Shikun Zhang"
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable achievements across diverse applications; however, they remain plagued by spurious correlations and the generation of hallucinated content. Despite extensive efforts to enhance the resilience of LLMs, existing approaches either rely on indiscriminate fine-tuning of all parameters, resulting in parameter inefficiency and lack of specificity, or depend on post-processing techniques that offer limited adaptability and flexibility. This study introduces a novel Causality-driven Robust Optimization (CdRO) approach that selectively updates model components sensitive to causal reasoning, enhancing model causality while preserving valuable pretrained knowledge to mitigate overfitting. Our method begins by identifying the parameter components within LLMs that capture causal relationships, achieved through comparing the training dynamics of parameter matrices associated with the original samples, as well as augmented counterfactual and paraphrased variants. These comparisons are then fed into a lightweight logistic regression model, optimized in real time to dynamically identify and adapt the causal components within LLMs. The identified parameters are subsequently optimized using an enhanced policy optimization algorithm, where the reward function is designed to jointly promote both model generalization and robustness. Extensive experiments across various tasks using twelve different LLMs demonstrate the superior performance of our framework, underscoring its significant effectiveness in reducing the model’s dependence on spurious associations and mitigating hallucinations.",
        "keywords": [
          "Large language models",
          "Spurious correlations",
          "Hallucination",
          "Knowledge localization",
          "Logistic regression model",
          "Policy optimization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=g9sWQsqemL"
      },
      "similarity": 0.4243531823158264
    },
    {
      "paper": {
        "id": "roKj4IwaVT",
        "number": 17082,
        "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
        "authors": [
          "Gleb Rodionov",
          "Roman Garipov",
          "Alina Shutova",
          "George Yakushev",
          "Erik Schultheis",
          "Vage Egiazarian",
          "Anton Sinitsin",
          "Denis Kuznedelev",
          "Dan Alistarh"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",
        "keywords": [
          "LLM",
          "reasoning",
          "parallel generation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=roKj4IwaVT"
      },
      "similarity": 0.42410022020339966
    },
    {
      "paper": {
        "id": "DqRbfiTdKK",
        "number": 11247,
        "title": "LLM at Network Edge: A Layer-wise Efficient Federated Fine-tuning Approach",
        "authors": [
          "Jinglong Shen",
          "Nan Cheng",
          "Wenchao Xu",
          "Haozhao Wang",
          "Yifan guo",
          "Jiajie Xu"
        ],
        "abstract": "Fine-tuning large language models (LLMs) poses significant computational burdens, especially in federated learning (FL) settings. We introduce Layer-wise Efficient Federated Fine-tuning (LEFF), a novel method designed to enhance the efficiency of FL fine-tuning while preserving model performance and minimizing client-side computational overhead. LEFF strategically selects layers for fine-tuning based on client computational capacity, thereby mitigating the straggler effect prevalent in heterogeneous environments. Furthermore, LEFF incorporates an importance-driven layer sampling mechanism, prioritizing layers with greater influence on model performance. Theoretical analysis demonstrates that LEFF achieves a convergence rate of $\\mathcal{O}(1/\\sqrt{T})$. Extensive experiments on diverse datasets demonstrate that LEFF attains superior computational efficiency and model performance compared to existing federated fine-tuning methods, particularly under heterogeneous conditions.",
        "keywords": [
          "federated learning",
          "large language model",
          "fine-tuning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=DqRbfiTdKK"
      },
      "similarity": 0.4239715337753296
    },
    {
      "paper": {
        "id": "lCsVtkMusN",
        "number": 10002,
        "title": "Fast Computation and Optimization for Opinion-Based Quantities of Friedkin-Johnsen Model",
        "authors": [
          "Haoxin Sun",
          "Yubo Sun",
          "Xiaotian Zhou",
          "Zhongzhi Zhang"
        ],
        "abstract": "In this paper, we address the problem of fast computation and optimization of opinion-based quantities in the Friedkin–Johnsen (FJ) model. We first introduce the concept of partial rooted forests and present an efficient algorithm for computing these quantities using this method. Furthermore, we study two optimization problems in the FJ model: the Opinion Minimization Problem and the Polarization and Disagreement Minimization Problem. For both problems, we propose fast algorithms based on partial rooted forest sampling. Our methods reduce the time complexity from linear to sublinear. Extensive experiments on real-world networks demonstrate that our algorithms are both accurate and efficient, outperforming state-of-the-art methods and scaling effectively to large-scale networks.",
        "keywords": [
          "forest matrix",
          "fast algorithm",
          "FJ model",
          "opinion dynamics，optimization problems"
        ],
        "primary_area": "social_and_economic_aspects_of_machine_learning",
        "forum_url": "https://openreview.net/forum?id=lCsVtkMusN"
      },
      "similarity": 0.4238779544830322
    },
    {
      "paper": {
        "id": "3oQDkmW72a",
        "number": 26133,
        "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models",
        "authors": [
          "Simeng Han",
          "Howard Dai",
          "Stephen Xia",
          "Grant Zhang",
          "Chen Liu",
          "Lichang Chen",
          "Hoang H Nguyen",
          "Hongyuan Mei",
          "Jiayuan Mao",
          "R. Thomas McCoy"
        ],
        "abstract": "Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. \nIn this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. \nWe investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. \nWe investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) self-correcting solutions based on gold solutions; (3) producing step-by-step sketches of solutions; and (4) making use of hints.\nWe find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.",
        "keywords": [
          "Brainteaser; Reasoning LLMs"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=3oQDkmW72a"
      },
      "similarity": 0.4233286678791046
    },
    {
      "paper": {
        "id": "1rUj9ZN6Bz",
        "number": 18204,
        "title": "FlexOLMo: Open Language Models for Flexible Data Use",
        "authors": [
          "Weijia Shi",
          "Akshita Bhagia",
          "Kevin Farhat",
          "Niklas Muennighoff",
          "Jacob Morrison",
          "Evan Pete Walsh",
          "Dustin Schwenk",
          "Shayne Longpre",
          "Jake Poznanski",
          "Allyson Ettinger",
          "Daogao Liu",
          "Margaret Li",
          "Mike Lewis",
          "Wen-tau Yih",
          "Dirk Groeneveld",
          "Luca Soldaini",
          "Kyle Lo",
          "Noah A. Smith",
          "Luke Zettlemoyer",
          "Pang Wei Koh",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Sewon Min"
        ],
        "abstract": "We introduce FlexOLMo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on private datasets, and (2) data-flexible inference, where these parameters along with their associated data can be easily included or excluded from model inferences with no further training. FlexOLMo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on private datasets and later integrated through a new nonparametric routing without any joint training across datasets. FlexOLMo is trained on FLEXMIX, a corpus we curate comprising seven restricted sets, either real or realistic approximations, alongside publicly available datasets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners significantly benefiting from these restricted sets (an average 41% relative improvement) while allowing flexible opt-out at inference time (e.g., for users without appropriate licenses or permissions). Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, FlexOLMo enables training on restricted data while keeping data local and supports fine-grained control of data access at inference.",
        "keywords": [
          "Language Model"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=1rUj9ZN6Bz"
      },
      "similarity": 0.4227740466594696
    },
    {
      "paper": {
        "id": "tu3P6KSHGN",
        "number": 5493,
        "title": "Safety Depth in Large Language Models: A Markov Chain Perspective",
        "authors": [
          "Ching-Chia Kao",
          "Chia-Mu Yu",
          "Chun-Shien Lu",
          "Chu-Song Chen"
        ],
        "abstract": "Large Language Models (LLMs) are increasingly adopted in high-stakes scenarios, yet their safety mechanisms often remain fragile. Simple jailbreak prompts or even benign fine-tuning can bypass internal safeguards, underscoring the need to understand the failure modes of current safety strategies.  Recent findings suggest that vulnerabilities emerge when alignment is confined to only the initial output tokens. To address this, we introduce the notion of safety depth, a designated output position where the model refuses to generate harmful content. While deeper alignment appears promising, identifying the optimal safety depth remains an open and underexplored challenge.\n\nWe leverage the equivalence between autoregressive language models and Markov chains to derive the first theoretical result on identifying the optimal safety depth. To reach this safety depth effectively, we propose a cyclic group augmentation strategy that improves safety scores across six LLMs. In addition, we uncover a critical interaction between safety depth and ensemble width, demonstrating that larger ensembles can offset shallower alignments. These results suggest that test-time computation, often overlooked in safety alignment, can play a key role. Our approach provides actionable insights for building safer LLMs.",
        "keywords": [
          "Safety Alignment",
          "Large Language Models",
          "Markov Chain",
          "Group Theory",
          "Ensemble"
        ],
        "primary_area": "social_and_economic_aspects_of_machine_learning",
        "forum_url": "https://openreview.net/forum?id=tu3P6KSHGN"
      },
      "similarity": 0.4227210581302643
    },
    {
      "paper": {
        "id": "wmweEDugTZ",
        "number": 10446,
        "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
        "authors": [
          "Sheng Wang",
          "Pengan CHEN",
          "Jingqi Zhou",
          "Qintong Li",
          "Jingwei Dong",
          "Jiahui Gao",
          "Boyang XUE",
          "Jiyue Jiang",
          "Lingpeng Kong",
          "Chuan Wu"
        ],
        "abstract": "Model customization necessitates high-quality and diverse datasets, but acquiring such data remains time-consuming and labor-intensive. \nDespite the great potential of large language models (LLMs) for data synthesis, current approaches are constrained by limited seed data, model biases and low-variation prompts, resulting in limited diversity and biased distribution with the increase of data scales.\nTo tackle this challenge, we introduce TreeSynth, a tree-guided subspace-based data synthesis approach inspired by decision trees. \nIt constructs a spatial partitioning tree to recursively divide a task-specific full data space (i.e., root node) into numerous atomic subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes to ensure both distinctiveness and comprehensiveness, before synthesizing samples within each atomic subspace. \nThis globally divide-and-synthesize method finally collects subspace samples into a comprehensive dataset, effectively circumventing repetition and space collapse to ensure the diversity of large-scale data synthesis.\nFurthermore, the spatial partitioning tree enables sample allocation into atomic subspaces, allowing the re-balancing of existing datasets for more balanced and comprehensive distributions.\nEmpirically, extensive experiments across diverse benchmarks consistently validates the superior data diversity, model performance, and robust scalability of TreeSynth compared to both human-crafted datasets and peer data synthesis methods, with the average performance gain reaching 10%.\nBesides, the consistent improvements of TreeSynth-balanced datasets highlight its efficacious application to redistribute existing datasets for more comprehensive coverage and the induced performance enhancement. The code is available at https://github.com/cpa2001/TreeSynth.",
        "keywords": [
          "Data Synthesis",
          "Tree-Guided Space Partitioning",
          "Data Diversity Enhancement"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=wmweEDugTZ"
      },
      "similarity": 0.42185190320014954
    },
    {
      "paper": {
        "id": "dgQRSJdo6a",
        "number": 27574,
        "title": "Uniform Wrappers: Bridging Concave to Quadratizable Functions in Online Optimization",
        "authors": [
          "Mohammad Pedramfar",
          "Christopher John Quinn",
          "Vaneet Aggarwal"
        ],
        "abstract": "This paper presents novel contributions to the field of online optimization, particularly focusing on the adaptation of algorithms from concave optimization to more challenging classes of functions.\nKey contributions include the introduction of uniform wrappers, a class of meta-algorithms that could be used for algorithmic conversions such as converting algorithms for convex optimization into those for quadratizable optimization.\nMoreover, we propose a guideline that, given a base algorithm $\\mathcal{A}$ for concave optimization and a uniform wrapper $\\mathcal{W}$, describes how to convert a proof of the regret bound of $\\mathcal{A}$ in the concave setting into a proof of the regret bound of $\\mathcal{W}(\\mathcal{A})$ for quadratizable setting.\nThrough this framework, the paper demonstrates improved regret guarantees for various classes of DR-submodular functions under zeroth-order feedback. Furthermore, the paper extends zeroth-order online algorithms to bandit feedback and offline counterparts, achieving notable improvements in regret/sample complexity compared to existing approaches.",
        "keywords": [
          "DR-submodular Optimization",
          "Convex Optimization"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=dgQRSJdo6a"
      },
      "similarity": 0.4215882420539856
    },
    {
      "paper": {
        "id": "ruzMpz4rBC",
        "number": 10120,
        "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding",
        "authors": [
          "Chenheng Zhang",
          "Tianqi Du",
          "Jizhe Zhang",
          "Mingqing Xiao",
          "Yifei Wang",
          "Yisen Wang",
          "Zhouchen Lin"
        ],
        "abstract": "Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scaling the computation of inference time with reward models, have underscored the importance of decoding, but these methods often suffer from high computational costs and limited applicability.\nIn this paper, we revisit LLM generation through the lens of recommender systems, conceptualizing the decoding process as analogous to the ranking stage in recommendation pipelines. From this perspective, we observe that both traditional decoding methods and reward models exhibit clear limitations such as redundancy.\nMotivated by this insight, we propose Language Ranker, a novel framework that introduces a lightweight module to rerank candidate responses using features extracted by the base model. Experiments across a wide range of tasks show that Language Ranker achieves performance comparable to large-scale reward models, while requiring only <0.5M additional parameters, significantly reducing the computational overhead during both training and inference stages.  This highlights the efficiency and effectiveness of our method, showcasing its potential to fully unlock the capabilities of LLMs.",
        "keywords": [
          "language model",
          "recommender system",
          "inference-time computing"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=ruzMpz4rBC"
      },
      "similarity": 0.4210585057735443
    },
    {
      "paper": {
        "id": "nbMeRvNb7A",
        "number": 17710,
        "title": "Nested Learning: The Illusion of Deep Learning Architectures",
        "authors": [
          "Ali Behrouz",
          "Meisam Razaviyayn",
          "Peilin Zhong",
          "Vahab Mirrokni"
        ],
        "abstract": "Over the last decades, developing more powerful neural architectures and simultaneously designing optimization algorithms to effectively train them have been the core of research efforts to enhance the capability of machine learning models. Despite the recent progresses, particularly in developing Language Models (LMs), there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improved, and find ''effective solutions,''. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own ''context flow''. NL reveals that existing deep learning methods learns from data through \\emph{compressing} their own context flow, and explain how in-context learning emerges in large models. NL suggests a path (a new dimension to deep learning) to design more expressive learning algorithms with more ''levels'', resulting in higher-order in-context learning abilities. In addition to its neuroscientifically plausible and mathematically white-box nature, we advocate for its importance by presenting three core contributions: (1) Deep Optimizers: Based on NL, we show that well-known gradient-based optimizers (e.g., Adam, SGD with Momentum, etc.) are in fact associative memory modules that aim to compress the gradients with gradient descent. Building on this insight, we present a set of more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Titans: Taking advantage of NL's insights on learning algorithms, we present a novel sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of ``long-term/short-term memory''. Combining our self-modifying sequence model with the continuum memory system, we present a learning module, called Hope, showing promising results in language modeling, continual learning, and long-context reasoning tasks.",
        "keywords": [
          "Neural Learning Module",
          "Deep Learning",
          "Memory",
          "Recurrent Neural Networks"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=nbMeRvNb7A"
      },
      "similarity": 0.4206279516220093
    },
    {
      "paper": {
        "id": "oanhUGY6un",
        "number": 16977,
        "title": "Gradient Multi-Normalization for Efficient LLM Training",
        "authors": [
          "Meyer Scetbon",
          "Chao Ma",
          "Wenbo Gong",
          "Edward Meeds"
        ],
        "abstract": "Training large language models (LLMs) commonly relies on adaptive optimizers such as Adam (Kingma & Ba 2015), which accelerate convergence through moment estimates but incur substantial memory overhead. Recent stateless approaches such as SWAN (Ma et al., 2024) have shown that appropriate preprocessing of instantaneous gradient matrices can match the performance of adaptive methods without storing optimizer states. Building on this insight, we introduce \\emph{gradient multi-normalization}, a principled framework for designing stateless optimizers that normalize gradients with respect to multiple norms simultaneously. Whereas standard first-order methods can be viewed as gradient normalization under a single norm (Bernstein & Newhouse, 2024), our formulation generalizes this perspective to a multi-norm setting. We derive an efficient alternating scheme that enforces these normalization constraints and show that our procedure can produce, up to an arbitrary precision, a fixed-point of the problem. This unifies and extends prior stateless optimizers, showing that SWAN arises as a specific instance with particular norm choices. Leveraging this principle, we develop SinkGD, a lightweight matrix optimizer that retains the memory footprint of SGD while substantially reducing computation relative to whitening-based methods. On the memory-efficient LLaMA training benchmark (Zhao et al., 2024), SinkGD achieves state-of-the-art performance, reaching the same evaluation perplexity as Adam using only 40\\% of the training tokens.",
        "keywords": [
          "LLM",
          "Optimizer",
          "Gradient",
          "Normalization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=oanhUGY6un"
      },
      "similarity": 0.4205241799354553
    },
    {
      "paper": {
        "id": "fru52tkjHf",
        "number": 18201,
        "title": "ASGO: Adaptive Structured Gradient Optimization",
        "authors": [
          "Kang An",
          "Yuxing Liu",
          "Rui Pan",
          "Yi Ren",
          "Shiqian Ma",
          "Donald Goldfarb",
          "Tong Zhang"
        ],
        "abstract": "Training deep neural networks (DNNs) is a structured optimization problem, because the parameters are naturally represented by matrices and tensors rather than simple vectors. Under this structural representation, it has been widely observed that gradients are low-rank and Hessians are approximately block-wise diagonal. These structured properties are crucial for designing efficient optimization algorithms but may not be utilized by current popular optimizers like Adam. In this paper, we present a novel optimization algorithm ASGO that capitalizes on these properties by employing a preconditioner that is adaptively updated using structured gradients. By fine-grained theoretical analysis, ASGO is proven to achieve superior convergence rates compared to existing structured gradient methods. Based on the convergence theory, we further demonstrate that ASGO can benefit from the low-rank and block-wise diagonal properties. We also discuss practical modifications of ASGO and empirically verify the effectiveness of the algorithm on language model tasks.",
        "keywords": [
          "optimization",
          "optimization theory",
          "adaptive gradient method",
          "optimization for deep learning",
          "convex optimization"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=fru52tkjHf"
      },
      "similarity": 0.42040789127349854
    },
    {
      "paper": {
        "id": "dRjt4vlYVQ",
        "number": 9573,
        "title": "Weaver: Shrinking the Generation-Verification Gap by Scaling Compute for Verification",
        "authors": [
          "Jon Saad-Falcon",
          "E. Kelly Buchanan",
          "Mayee F Chen",
          "Tzu-Heng Huang",
          "Brendan McLaughlin",
          "Tanvir Bhathal",
          "Shang Zhu",
          "Ben Athiwaratkun",
          "Frederic Sala",
          "Scott Linderman",
          "Azalia Mirhoseini",
          "Christopher Re"
        ],
        "abstract": "Verifiers can improve language model (LM) capabilities by providing feedback or selecting the best response from a pool of generated candidates. Currently, high-quality verifiers are either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean for formal proofs). While LM judges and reward models have become broadly useful as general-purpose verifiers, a significant performance gap remains between them and oracle verifiers. To help close this gap, we introduce Weaver, a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. First we find that weighted ensembles of verifiers, which typically require learning from labeled data, significantly outperform unweighted combinations due to differences in the verifiers. To reduce the dependency on labeled data, Weaver leverages weak supervision to estimate each verifier’s accuracy and combines their outputs into a unified score that better reflects true response quality. However, directly applying weak supervision algorithms poses several challenges, including inconsistent verifier output formats and handling low-quality verifiers. Weaver addresses these challenges by using dataset statistics to normalize outputs and filter specific verifiers. We study the effectiveness of Weaver in repeated sampling settings, where a model generates multiple candidate responses at test time and a verifier is used to select the correct one. Our evaluations demonstrate that Weaver significantly improves the pass@1 performance across several reasoning and math tasks, achieving o3-mini level accuracy with Llama 3.3 70B Instruct (a much cheaper non-reasoning model) as the generator, and an ensemble of smaller judge and reward models as the verifiers (86.2% average). This gain mirrors the jump achieved between GPT-4o and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and post-training interventions. To make Weaver more efficient, we train a compact 400M cross-encoder using Weaver's combined output scores. This distilled model retains 98.7% of Weaver's full accuracy while reducing verification compute by up to 99.97%.",
        "keywords": [
          "test-time compute",
          "repeated sampling",
          "weak supervision",
          "weak verification"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=dRjt4vlYVQ"
      },
      "similarity": 0.42035147547721863
    },
    {
      "paper": {
        "id": "WAFD6VYIEa",
        "number": 19175,
        "title": "Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization",
        "authors": [
          "Subhojyoti Mukherjee",
          "Viet Dac Lai",
          "Raghavendra Addanki",
          "Ryan A. Rossi",
          "Seunghyun Yoon",
          "Trung Bui",
          "Anup Rao",
          "Jayakumar Subramanian",
          "Branislav Kveton"
        ],
        "abstract": "Offline reinforcement learning (RL) is a variant of RL where the policy is learned from a previously collected dataset of trajectories and rewards. In our work, we propose a practical approach to offline RL with large language models (LLMs). We recast the problem as reward-weighted fine-tuning, which can be solved using similar techniques to supervised fine-tuning (SFT). To showcase the value of our approach, we apply it to learning short-horizon question-answering policies of a fixed length, where the agent reasons about potential answers or asks clarifying questions. Our work stands in a stark contrast to state-of-the-art methods in this domain, based on SFT and direct preference optimization, which have additional hyper-parameters and do not directly optimize for rewards. We compare to them empirically, and report major gains in both optimized rewards and language quality.",
        "keywords": [
          "offline reinforcement learning",
          "fine-tuning",
          "LLMs",
          "question answering",
          "clarifying questions"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=WAFD6VYIEa"
      },
      "similarity": 0.4202445447444916
    },
    {
      "paper": {
        "id": "M1b7IuY6Co",
        "number": 13457,
        "title": "Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective",
        "authors": [
          "Emmanuel Anaya Gonzalez",
          "Sairam Vaidya",
          "Kanghee Park",
          "Ruyi Ji",
          "Taylor Berg-Kirkpatrick",
          "Loris D'Antoni"
        ],
        "abstract": "Constrained decoding enables Language Models (LMs) to produce samples that provably satisfy hard constraints.\nHowever, existing constrained-decoding approaches often distort the underlying model distribution, a limitation that is especially problematic in applications like program fuzzing, where one wants to generate diverse and valid program inputs for testing purposes. \nWe propose a new constrained sampling framework based on Markov Chain Monte Carlo (MCMC) that simultaneously satisfies three core desiderata: constraint satisfying (every sample satisfies the constraint), monotonically converging (the sampling process converges to the true conditional distribution), and efficient (high-quality samples emerge in few steps). Our method constructs a proposal distribution over valid outputs and applies a Metropolis-Hastings acceptance criterion based on the LM’s likelihood, ensuring principled and efficient exploration of the constrained space. Empirically, our sampler outperforms existing methods on both synthetic benchmarks and real-world program fuzzing tasks.",
        "keywords": [
          "Language Models",
          "Context-Free Grammars",
          "Markov Chain Monte Carlo"
        ],
        "primary_area": "probabilistic_methods",
        "forum_url": "https://openreview.net/forum?id=M1b7IuY6Co"
      },
      "similarity": 0.4201880395412445
    },
    {
      "paper": {
        "id": "v4AT18kysa",
        "number": 15151,
        "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation",
        "authors": [
          "Chenyang Le",
          "Bing Han",
          "Jinshun Li",
          "Chen Songyong",
          "Yanmin Qian"
        ],
        "abstract": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read/write policies hinder unified strategy learning. In this paper, we present SimulMEGA(Simultaneous Generation by Mixture-of-Experts GAting), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read/write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500 M-parameter speech-to-text model outperforms the Seamless baseline, achieving under 7% BLEU degradation at 1.5 s average lag and under 3% at 3 s. We further demonstrate SimulMEGA’s versatility by extending it to streaming TTS via a unidirectional backbone, yielding superior latency–quality trade-offs.",
        "keywords": [
          "Simultaneous Speech Translation",
          "Streaming Text-To-Speech"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=v4AT18kysa"
      },
      "similarity": 0.4200291037559509
    },
    {
      "paper": {
        "id": "9e2H9DhKPa",
        "number": 9077,
        "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
        "authors": [
          "Tianteng Gu",
          "Bei Liu",
          "Bo Xiao",
          "Ke Zeng",
          "Jiacheng Liu",
          "Yanmin Qian"
        ],
        "abstract": "Pruning is a widely used technique to compress large language models (LLMs) by removing unimportant weights, but it often suffers from significant performance degradation—especially under semi-structured sparsity constraints. Existing pruning methods primarily focus on estimating the importance of individual weights, which limits their ability to preserve critical capabilities of the model. In this work, we propose a new perspective: rather than merely selecting which weights to prune, we first redistribute parameter importance to make the model inherently more amenable to pruning. By minimizing the information entropy of normalized importance scores, our approach concentrates importance onto a smaller subset of weights, thereby enhancing pruning robustness. We instantiate this idea through DenoiseRotator, which applies learnable orthogonal transformations to the model’s weight matrices. Our method is model-agnostic and can be seamlessly integrated with existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the dense model by 58%, narrowing the degradation from 8.1 to 3.4 points.",
        "keywords": [
          "Large language model",
          "Model compression",
          "Pruning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=9e2H9DhKPa"
      },
      "similarity": 0.41973766684532166
    },
    {
      "paper": {
        "id": "T4qJuQCFAK",
        "number": 23408,
        "title": "Activation-Informed Merging of Large Language Models",
        "authors": [
          "Amin Heyrani Nobari",
          "Kaveh Alim",
          "Ali ArjomandBigdeli",
          "Akash Srivastava",
          "Faez Ahmed",
          "Navid Azizan"
        ],
        "abstract": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40% increase in benchmark performance. Our code is publicly available at https://github.com/ahnobari/ActivationInformedMerging",
        "keywords": [
          "Model Merging",
          "Activation‑Informed Merging",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=T4qJuQCFAK"
      },
      "similarity": 0.4194276034832001
    },
    {
      "paper": {
        "id": "5k0AHYc4MJ",
        "number": 22350,
        "title": "Generalizable Reasoning through Compositional Energy Minimization",
        "authors": [
          "Alexandru Oarga",
          "Yilun Du"
        ],
        "abstract": "Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/",
        "keywords": [
          "Reasoning Generalization",
          "Compositional Inference",
          "Test-Time Computation",
          "Test-Time Inference",
          "Combinatorical Optimization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=5k0AHYc4MJ"
      },
      "similarity": 0.419103741645813
    },
    {
      "paper": {
        "id": "jFaFCc5978",
        "number": 25135,
        "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
        "authors": [
          "Penghui Qi",
          "Zichen Liu",
          "Tianyu Pang",
          "Chao Du",
          "Wee Sun Lee",
          "Min Lin"
        ],
        "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present **AnytimeReasoner**, a novel framework for optimizing reasoning performance under varying thinking budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, **Budget Relative Policy Optimization (BRPO)**, to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.",
        "keywords": [
          "Anytime Reasoning",
          "Reinforcement Learning",
          "LLM Reasoning",
          "GRPO"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=jFaFCc5978"
      },
      "similarity": 0.4184618592262268
    },
    {
      "paper": {
        "id": "0NdS4xCngO",
        "number": 9822,
        "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models",
        "authors": [
          "Sagnik Mukherjee",
          "Lifan Yuan",
          "Dilek Hakkani-Tür",
          "Hao Peng"
        ],
        "abstract": "Reinforcement learning (RL) yields substantial improvements in large language models’ (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5%-30% of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely-used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity-promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning.\nThe subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance.  Our analysis suggests that this sparsity is not due to updating only a subset of layers; instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank,\nsuggesting RL updates a small subset of parameters that nevertheless span almost the full  subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution; \ntechniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.",
        "keywords": [
          "Large Language Models",
          "Reinforcement Learning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=0NdS4xCngO"
      },
      "similarity": 0.41833972930908203
    },
    {
      "paper": {
        "id": "6Mah2bx7ZI",
        "number": 4535,
        "title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
        "authors": [
          "Pascal Kesseli",
          "Peter O'Hearn",
          "Ricardo Silveira Cabral"
        ],
        "abstract": "We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on benchmarks like the logic puzzles tasks in ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused, human-readable domain-specific language (DSL) called Logic.py. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.",
        "keywords": [
          "formal reasoning",
          "machine learning",
          "large language models",
          "constraint solvers"
        ],
        "primary_area": "reinforcement_learning",
        "forum_url": "https://openreview.net/forum?id=6Mah2bx7ZI"
      },
      "similarity": 0.4176458716392517
    },
    {
      "paper": {
        "id": "sXpyn3lAb5",
        "number": 24668,
        "title": "Accelerating data-driven algorithm selection for combinatorial partitioning problems",
        "authors": [
          "Vaggos Chatziafratis",
          "Ishani Karmarkar",
          "Yingxi Li",
          "Ellen Vitercik"
        ],
        "abstract": "Data-driven algorithm selection is a powerful approach for choosing effective heuristics for computational problems. It operates by evaluating a set of candidate algorithms on a collection of representative training instances and selecting the one with the best empirical performance. However, running each algorithm on every training instance is computationally expensive, making scalability a central challenge. In practice, a common workaround is to evaluate algorithms on smaller proxy instances derived from the original inputs. However, this practice has remained largely ad hoc and lacked theoretical grounding. We provide the first theoretical foundations for this practice by formalizing the notion of size generalization: predicting an algorithm's performance on a large instance by evaluating it on a smaller, representative instance, subsampled from the original instance. We provide size generalization guarantees for three widely used clustering algorithms (single-linkage, k-means++, and Gonzalez's k-centers heuristic) and two canonical max-cut algorithms (Goemans-Williamson and Greedy). We characterize the subsample size sufficient to ensure that performance on the subsample reflects performance on the full instance, and our experiments support these findings.",
        "keywords": [
          "data-driven algorithm selection",
          "sub-sampling",
          "clustering",
          "max-cut",
          "Goemans-williamson"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=sXpyn3lAb5"
      },
      "similarity": 0.4176347851753235
    },
    {
      "paper": {
        "id": "tdwRIP6NG2",
        "number": 26398,
        "title": "Linearization Explains Fine-Tuning in Large Language Models",
        "authors": [
          "Zahra Rahimi Afzal",
          "Tara Esmaeilbeig",
          "Mojtaba Soltanalian",
          "Mesrob I Ohannessian"
        ],
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an $\\ell_2$-distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.",
        "keywords": [
          "Parameter-Efficient Fine-tuning",
          "LLMs",
          "Neural Tangent Kernel",
          "Linearization",
          "Low Rank Adaptation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=tdwRIP6NG2"
      },
      "similarity": 0.41705164313316345
    },
    {
      "paper": {
        "id": "TkEdQv0bXB",
        "number": 20160,
        "title": "Hyperbolic Fine-Tuning for Large Language Models",
        "authors": [
          "Menglin Yang",
          "Ram Samarth B B",
          "Aosong Feng",
          "Bo Xiong",
          "Jiahong Liu",
          "Irwin King",
          "Rex Ying"
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs.\n   In this study, we investigate the non-Euclidean characteristics of LLMs. \n   Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. \n   Motivated by these observations, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. \n   However, we find that this hyperbolic fine-tuning cannot be achieved through the naive application of exponential and logarithmic maps when the embedding and weight matrices both reside in Euclidean space.\n   To address this technical issue, we introduce hyperbolic low-rank efficient fine-tuning, HypLoRA, which performs low-rank adaptation directly on the hyperbolic manifold, preventing the cancellation effect produced by consecutive exponential and logarithmic maps and thereby preserving hyperbolic modeling capabilities.\n   Extensive experiments across various base models and two different reasoning benchmarks, specifically arithmetic and commonsense reasoning tasks, demonstrate that HypLoRA substantially improves LLM performance.",
        "keywords": [
          "Large Language Models",
          "Hyperbolic Space",
          "Low-Rank Adaptation",
          "Embedding Space"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=TkEdQv0bXB"
      },
      "similarity": 0.41674327850341797
    },
    {
      "paper": {
        "id": "Dkgx2pS4Ww",
        "number": 27289,
        "title": "Quantifying Elicitation of Latent Capabilities in Language Models",
        "authors": [
          "Elizabeth Donoway",
          "Hailey Joren",
          "Arushi Somani",
          "Henry Sleight",
          "Julian Michael",
          "Michael R DeWeese",
          "John Schulman",
          "Ethan Perez",
          "Fabien Roger",
          "Jan Leike"
        ],
        "abstract": "Large language models often possess latent capabilities that lie dormant unless explicitly elicited, or surfaced, through fine-tuning or prompt engineering. Predicting, assessing, and understanding these latent capabilities pose significant challenges in the development of effective, safe AI systems. In this work, we recast elicitation as an information-constrained fine-tuning problem and empirically characterize upper bounds on the minimal number of parameters needed to achieve specific task performances. We find that training as few as 10–100 randomly chosen parameters—several orders of magnitude fewer than state-of-the-art parameter-efficient methods—can recover up to 50\\% of the performance gap between pretrained-only and full fine-tuned models, and 1,000s to 10,000s of parameters can recover 95\\% of this performance gap. We show that a logistic curve fits the relationship between the number of trained parameters and model performance gap recovery. This scaling generalizes across task formats and domains, as well as model sizes and families, extending to reasoning models and remaining robust to increases in inference compute. To help explain this behavior, we consider a simplified picture of elicitation via fine-tuning where each trainable parameter serves as an encoding mechanism for accessing task-specific knowledge. We observe a relationship between the number of trained parameters and how efficiently relevant model capabilities can be accessed and elicited, offering a potential route to distinguish elicitation from teaching.",
        "keywords": [
          "elicitation",
          "large language models",
          "LLMs",
          "latent capabilities",
          "minimum description length"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Dkgx2pS4Ww"
      },
      "similarity": 0.4167085289955139
    },
    {
      "paper": {
        "id": "ycnc9aLnQu",
        "number": 19406,
        "title": "Probing Neural Combinatorial Optimization Models",
        "authors": [
          "Zhiqin Zhang",
          "Yining Ma",
          "Zhiguang Cao",
          "Hoong Chuin Lau"
        ],
        "abstract": "Neural combinatorial optimization (NCO) has achieved remarkable performance, yet its learned model representations and decision rationale remain a black box. This impedes both academic research and practical deployment, since researchers and stakeholders require deeper insights into NCO models. In this paper, we take the first critical step towards interpreting NCO models by investigating their representations through various probing tasks. Moreover, we introduce a novel probing tool named Coefficient Significance Probing (CS-Probing) to enable deeper analysis of NCO representations by examining the coefficients and statistical significance during probing. Extensive experiments and analysis reveal that NCO models encode low-level information essential for solution construction, while capturing high-level knowledge to facilitate better decisions. Using CS-Probing, we find that prevalent NCO models impose varying inductive biases on their learned representations, uncover direct evidence related to model generalization, and identify key embedding dimensions associated with specific knowledge. These insights can be potentially translated into practice, for example, with minor code modifications, we improve the generalization of the analyzed model. Our work represents a first systematic attempt to interpret black-box NCO models, showcasing probing as a promising tool for analyzing their internal mechanisms and revealing insights for the NCO community. The source code is publicly available.",
        "keywords": [
          "Neural Combinatorial Optimization",
          "Probing",
          "Vehicle Routing Problem"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=ycnc9aLnQu"
      },
      "similarity": 0.4166449308395386
    },
    {
      "paper": {
        "id": "u0YyIHljXO",
        "number": 19145,
        "title": "Objective Soups: Multilingual Multi-Task Modeling for Speech Processing",
        "authors": [
          "A F M Saif",
          "Lisha Chen",
          "Xiaodong Cui",
          "Songtao Lu",
          "Brian Kingsbury",
          "Tianyi Chen"
        ],
        "abstract": "The need for training multilingual multi-task speech processing (MSP) models that perform both automatic speech recognition and speech-to-text translation is increasingly evident. However, a significant challenge arises from the conflicts among multiple objectives when using a single model. Multi-objective optimization can address this challenge by facilitating the optimization of multiple conflicting objectives and aligning the gradient updates in a common descent direction. While multi-objective optimization helps avoid conflicting gradient updates, a critical issue is that when there are many objectives, such as in MSP, it is often {\\em difficult to find} a common descent direction. This leads to an important question: Is it more effective to separate highly conflicting objectives into different optimization levels or to keep them in a single level? To address this question, this paper investigates three multi-objective MSP formulations, which we refer to as \\textbf{objective soup recipes}. These formulations apply multi-objective optimization at different optimization levels to mitigate potential conflicts among all objectives. To keep computation and memory overhead low, we incorporate a lightweight layer‑selection strategy that detects the most conflicting layers and uses only their gradients when computing the conflict‑avoidance direction. We conduct an extensive investigation using the CoVoST v2 dataset for combined multilingual ASR and ST tasks, along with the LibriSpeech and AISHELL-1 datasets for multilingual ASR, to identify highly conflicting objectives and determine the most effective training recipe among the three proposed multi-objective optimization algorithms.",
        "keywords": [
          "Multilingual speech recognition",
          "automatic speech translation",
          "multi-objective optimization",
          "conflict-aware training"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=u0YyIHljXO"
      },
      "similarity": 0.41660797595977783
    },
    {
      "paper": {
        "id": "4zea5Bcemp",
        "number": 7417,
        "title": "Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data",
        "authors": [
          "Ishika Agarwal",
          "Dilek Hakkani-Tür"
        ],
        "abstract": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0007% the size of full language models (we average across 1.5B-22B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT/tree/main.",
        "keywords": [
          "Influence Estimation",
          "Data Valuation",
          "Data Attribution"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=4zea5Bcemp"
      },
      "similarity": 0.4165973663330078
    },
    {
      "paper": {
        "id": "E8adS5srds",
        "number": 963,
        "title": "Anchored Diffusion Language Model",
        "authors": [
          "Litu Rout",
          "Constantine Caramanis",
          "Sanjay Shakkottai"
        ],
        "abstract": "Diffusion Language Models (DLMs) promise parallel generation and bidirectional context, yet they underperform autoregressive (AR) models in both *likelihood modeling* and *generated text quality*. We identify that this performance gap arises when important tokens (e.g., key words or low-frequency words that anchor a sentence) are masked early in the forward process, limiting contextual information for accurate reconstruction. To address this, we introduce the *Anchored Diffusion Language Model (ADLM)*, a novel two-stage framework that first predicts distributions over important tokens via an anchor network, and then predicts the likelihoods of missing tokens conditioned on the anchored predictions. ADLM significantly improves test perplexity on LM1B and OpenWebText, achieving up to 25.4\\% gains over prior DLMs, and narrows the gap with strong AR baselines. It also achieves state-of-the-art zero-shot generalization across seven benchmarks and surpasses AR models in MAUVE score, which marks the first time a DLM generates better human-like text than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower Bound (ANELBO) objective and show that anchoring improves sample complexity and likelihood modeling. Beyond diffusion, anchoring boosts performance in AR models and enhances reasoning in math and logic tasks, outperforming existing chain-of-thought approaches. Please see our project page: [anchored-diffusion-llm.github.io](https://anchored-diffusion-llm.github.io/) for code and demo.",
        "keywords": [
          "discrete diffusion",
          "diffusion language models",
          "large-language models",
          "generative modeling",
          "planning",
          "reasoning",
          "chain-of-thought"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=E8adS5srds"
      },
      "similarity": 0.41646790504455566
    },
    {
      "paper": {
        "id": "jDxFD45kkc",
        "number": 4079,
        "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation",
        "authors": [
          "Kai Lion",
          "Liang Zhang",
          "Bingcong Li",
          "Niao He"
        ],
        "abstract": "We show that low-rank adaptation of large-scale models suffers from a low stable rank that is well below the linear algebraic rank of the subspace, degrading fine-tuning performance. To mitigate the underutilization of the allocated subspace, we propose PoLAR, a parameterization inspired by the polar decomposition that factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Pairing the parameterization with Riemannian optimization leads to consistent gains on three different benchmarks testing general language understanding, commonsense reasoning, and mathematical problem solving with base model sizes ranging from 350M to 27B.",
        "keywords": [
          "low-rank adaptation",
          "architecture-optimizer co-design",
          "large language models",
          "lora",
          "low-rank adapter",
          "fine-tuning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=jDxFD45kkc"
      },
      "similarity": 0.415871798992157
    },
    {
      "paper": {
        "id": "V13dSX1wAs",
        "number": 10620,
        "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models",
        "authors": [
          "Lancheng Zou",
          "Shuo Yin",
          "Zehua Pei",
          "Tsung-Yi Ho",
          "Farzan Farnia",
          "Bei Yu"
        ],
        "abstract": "Channel permutation is a powerful technique for enhancing the accuracy of N:M sparse models by reordering the channels of weight matrices to prioritize the retention of important weights. \nHowever, traditional channel permutation methods rely on handcrafted quality metrics, which often fail to accurately capture the true impact of pruning on model performance. \nTo address this limitation, we propose PermLLM, a novel post-training pruning framework that introduces learnable channel permutation (LCP) for N:M sparsity. \nLCP leverages Sinkhorn normalization to transform discrete permutation matrices into differentiable soft permutation matrices, enabling end-to-end optimization. \nAdditionally, PermLLM incorporates an efficient block-wise channel permutation strategy, which significantly reduces the number of learnable parameters and computational complexity. \nPermLLM seamlessly integrates with existing one-shot pruning methods to adaptively optimize channel permutations, effectively mitigating pruning-induced errors. \nExtensive experiments on the LLaMA series, Qwen, and OPT models demonstrate that PermLLM achieves superior performance in optimizing N:M sparse models.",
        "keywords": [
          "Channel Permutation",
          "N:M Sparsity"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=V13dSX1wAs"
      },
      "similarity": 0.4157264828681946
    },
    {
      "paper": {
        "id": "Huw15LqglI",
        "number": 27594,
        "title": "Born a Transformer -- Always a Transformer? On the Effect of Pretraining on Architectural Abilities",
        "authors": [
          "Mayank Jobanputra",
          "Yana Veitsman",
          "Yash Sarrof",
          "Aleksandra Bakalova",
          "Vera Demberg",
          "Ellie Pavlick",
          "Michael Hahn"
        ],
        "abstract": "Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining by studying a family of *retrieval* and *copying* tasks inspired by Liu et al. [2024a]. We use a recently proposed framework for studying length generalization [Huang et al., 2025] to provide guarantees for each of our settings. Empirically, we observe an *induction-versus-anti-induction asymmetry*, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain transformer capabilities, but does not overcome fundamental length-generalization limits.",
        "keywords": [
          "transformers",
          "length generalization",
          "empirical analysis",
          "explainability"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Huw15LqglI"
      },
      "similarity": 0.41556787490844727
    },
    {
      "paper": {
        "id": "rZ2nSt1X58",
        "number": 3259,
        "title": "Optimization Inspired Few-Shot Adaptation for Large Language Models",
        "authors": [
          "Boyan Gao",
          "Xin Wang",
          "Yibo Yang",
          "David A. Clifton"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as In-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: In-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments.",
        "keywords": [
          "Large Language Models",
          "Opitmizer learning",
          "Few-shot adaptation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=rZ2nSt1X58"
      },
      "similarity": 0.4151952564716339
    },
    {
      "paper": {
        "id": "ZE9cxnEBpy",
        "number": 25818,
        "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language",
        "authors": [
          "Yuxin Chen",
          "Yiran Zhao",
          "Yang Zhang",
          "An Zhang",
          "Kenji Kawaguchi",
          "Shafiq Joty",
          "Junnan Li",
          "Tat-Seng Chua",
          "Michael Qizhe Shieh",
          "Wenxuan Zhang"
        ],
        "abstract": "As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may ``think'' in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space—a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model’s ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons—those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach. Our codes are available at https://anonymous.4open.science/status/S-C393.",
        "keywords": [
          "Large Language Models",
          "Language-Agnostic Neuron",
          "Abstract Thought"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=ZE9cxnEBpy"
      },
      "similarity": 0.4151744842529297
    },
    {
      "paper": {
        "id": "tuA2R6gZEA",
        "number": 12406,
        "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions",
        "authors": [
          "Chaochen Gao",
          "Xing W",
          "Zijia Lin",
          "Debing Zhang",
          "Songlin Hu"
        ],
        "abstract": "High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.",
        "keywords": [
          "Long-context Model",
          "Synthesis Data"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=tuA2R6gZEA"
      },
      "similarity": 0.41514426469802856
    },
    {
      "paper": {
        "id": "tY8ctrD4W2",
        "number": 1556,
        "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning",
        "authors": [
          "Haolin Pan",
          "Hongyu Lin",
          "Haoran Luo",
          "Yang Liu",
          "Kaichun Yao",
          "Libo Zhang",
          "Mingjie Xing",
          "Yanjun Wu"
        ],
        "abstract": "Compiler auto-tuning optimizes pass sequences to improve performance metrics such as Intermediate Representation (IR) instruction count. Although recent advances leveraging Large Language Models (LLMs) have shown promise in automating compiler tuning, two significant challenges still remain: the absence of high-quality reasoning datasets for agents training, and limited effective interactions with the compilation environment. In this work, we introduce Compiler-R1, the first reinforcement learning (RL)-driven framework specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1 features a curated, high-quality reasoning dataset and a novel two-stage end-to-end RL training pipeline, enabling efficient environment exploration and learning through an outcome-based reward. Extensive experiments across seven datasets demonstrate Compiler-R1 achieving an average 8.46\\% IR instruction count reduction compared to opt -Oz, showcasing the strong potential of RL-trained LLMs for compiler optimization. Our code and datasets are publicly available at https://github.com/Panhaolin2001/Compiler-R1.",
        "keywords": [
          "LLM",
          "Compiler Tuning",
          "Reinforcement Learning",
          "Agent",
          "Compilation Pass"
        ],
        "primary_area": "machine_learning_for_sciences",
        "forum_url": "https://openreview.net/forum?id=tY8ctrD4W2"
      },
      "similarity": 0.41506409645080566
    },
    {
      "paper": {
        "id": "tcisuhGsQZ",
        "number": 19607,
        "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
        "authors": [
          "Yuan Feng",
          "Junlin Lv",
          "Yukun Cao",
          "Xike Xie",
          "S Kevin Zhou"
        ],
        "abstract": "Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.",
        "keywords": [
          "Efficient AI",
          "Large Language Model",
          "LLM Inference",
          "KV Cache"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=tcisuhGsQZ"
      },
      "similarity": 0.4149380922317505
    },
    {
      "paper": {
        "id": "xwqTt26NJf",
        "number": 27131,
        "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
        "authors": [
          "Daniel Mingyi Israel",
          "Guy Van den Broeck",
          "Aditya Grover"
        ],
        "abstract": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.",
        "keywords": [
          "LLM",
          "discrete diffusion",
          "autoregression",
          "sequential",
          "fast",
          "throughput",
          "speculative decoding"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=xwqTt26NJf"
      },
      "similarity": 0.41490626335144043
    },
    {
      "paper": {
        "id": "v5ru9MGjsW",
        "number": 7423,
        "title": "Generalization Guarantees for Learning Score-Based Branch-and-Cut Policies in Integer Programming",
        "authors": [
          "Hongyu Cheng",
          "Amitabh Basu"
        ],
        "abstract": "Mixed-integer programming (MIP) provides a powerful framework for optimization problems, with Branch-and-Cut (B&C) being the predominant algorithm in state-of-the-art solvers. The efficiency of B&C critically depends on heuristic policies for making sequential decisions, including node selection, cut selection, and branching variable selection. While traditional solvers often employ heuristics with manually tuned parameters, recent approaches increasingly leverage machine learning, especially neural networks, to learn these policies directly from data. A key challenge is to understand the theoretical underpinnings of these learned policies, particularly their generalization performance from finite data. This paper establishes rigorous sample complexity bounds for learning B&C policies where the scoring functions guiding each decision step (node, cut, branch) have a certain piecewise polynomial structure. This structure generalizes the linear models that form the most commonly deployed policies in practice and investigated recently in a foundational series of theoretical works by Balcan et al. Such piecewise polynomial policies also cover the neural network architectures (e.g., using ReLU activations) that have been the focal point of contemporary practical studies. Consequently, our theoretical framework closely reflects the models utilized by practitioners investigating machine learning within B&C, offering a unifying perspective relevant to both established theory and modern empirical research in this area. Furthermore, our theory applies to quite general sequential decision making problems beyond B&C.",
        "keywords": [
          "Integer programming",
          "branch-and-cut",
          "cutting planes",
          "branch-and-bound",
          "sample complexity",
          "learning theory"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=v5ru9MGjsW"
      },
      "similarity": 0.4143344759941101
    },
    {
      "paper": {
        "id": "6h7HLx1kbH",
        "number": 18302,
        "title": "On Union-Closedness of Language Generation",
        "authors": [
          "Steve Hanneke",
          "Amin Karbasi",
          "Anay Mehrotra",
          "Grigoris Velegkas"
        ],
        "abstract": "We investigate language generation in the limit – a model by Kleinberg and Mullainathan and extended by Li, Raman, and Tewari. While Kleinberg and Mullainathan proved generation is possible for all countable collections, Li, Raman, and Tewari defined a hierarchy of generation notions (uniform, non-uniform, and generatable) and explored their feasibility for uncountable collections.\n    Our first set of results resolve two open questions of Li et al. by proving finite unions of generatable or non-uniformly generatable classes need not be generatable. These follow from a stronger result: there is non-uniformly generatable class and a uniformly generatable class  whose union is non-generatable.\n    This adds to the aspects along which language generation in the limit is different from traditional tasks in statistical learning theory like classification, which are closed under finite unions. \n    In particular, it implies that given two generators for different collections, one cannot combine them to obtain a single \"more powerful\" generator, prohibiting this notion of boosting. Our construction also addresses a third of Li et al.'s open questions on whether there are uncountable classes that are non-uniformly generatable and do not satisfy the eventually unbounded closure (EUC) condition introduced by Li et al.\n    Our approach utilizes carefully constructed classes along with a novel diagonalization argument that could be of independent interest in the growing area of language generation.",
        "keywords": [
          "language generation",
          "union-closedness",
          "online learning",
          "learning theory"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=6h7HLx1kbH"
      },
      "similarity": 0.4141976237297058
    },
    {
      "paper": {
        "id": "9F2Cmgo17M",
        "number": 20887,
        "title": "PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization",
        "authors": [
          "Federico Berto",
          "Chuanbo Hua",
          "Laurin Luttmann",
          "Jiwoo Son",
          "Junyoung Park",
          "Kyuree Ahn",
          "Changhyun Kwon",
          "Lin Xie",
          "Jinkyoo Park"
        ],
        "abstract": "Combinatorial optimization problems involving multiple agents are notoriously challenging due to their NP-hard nature and the necessity for effective agent coordination. Despite advancements in learning-based methods, existing approaches often face critical limitations, including suboptimal agent coordination, poor generalization, and high computational latency. To address these issues, we propose PARCO (Parallel AutoRegressive Combinatorial Optimization), a general reinforcement learning framework designed to construct high-quality solutions for multi-agent combinatorial tasks efficiently. To this end, PARCO integrates three key novel components: (1) transformer-based communication layers to enable effective agent collaboration during parallel solution construction, (2) a multiple pointer mechanism for low-latency, parallel agent decision-making, and (3) priority-based conflict handlers to resolve decision conflicts via learned priorities. We evaluate PARCO in multi-agent vehicle routing and scheduling problems, where our approach outperforms state-of-the-art learning methods, demonstrating strong generalization ability and remarkable computational efficiency. We make our source code publicly available to foster future research: https://github.com/ai4co/parco.",
        "keywords": [
          "Multi-agent Combinatorial Optimization; Multi-agent Reinforcement Learning; Neural Combinatorial Optimization"
        ],
        "primary_area": "reinforcement_learning",
        "forum_url": "https://openreview.net/forum?id=9F2Cmgo17M"
      },
      "similarity": 0.4139830470085144
    },
    {
      "paper": {
        "id": "wIM0y07NGX",
        "number": 17761,
        "title": "MESS+: Dynamically Learned Inference-Time LLM Routing in Model Zoos with Service Level Guarantees",
        "authors": [
          "Herbert Woisetschläger",
          "Ryan Zhang",
          "Shiqiang Wang",
          "Hans Arno Jacobsen"
        ],
        "abstract": "Open-weight large language model (LLM) zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. \nWe introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction.\nAcross a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of $2\\times$ cost savings compared to existing LLM routing techniques.",
        "keywords": [
          "inference optimization",
          "model routing",
          "stochastic optimization algorithms",
          "convergence analysis"
        ],
        "primary_area": "general_machine_learning",
        "forum_url": "https://openreview.net/forum?id=wIM0y07NGX"
      },
      "similarity": 0.4136708676815033
    },
    {
      "paper": {
        "id": "Zfgvo65gxm",
        "number": 6399,
        "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making",
        "authors": [
          "Larkin Liu",
          "Jalal Etesami"
        ],
        "abstract": "We explore the use of expert-guided bandit learning, which we refer to as online mixture-of-experts (OMoE). In this setting, given a context, a candidate committee of experts must determine how to aggregate their outputs to achieve optimal results in terms of aggregate accuracy. We propose two algorithms to address this problem. The first algorithm combines aggregate voting with UCB-driven successive elimination, efficiently pruning suboptimal exploration actions. The second algorithm employs an online weighted-majority-voting mechanism, leveraging the respective voting power of each expert proportional to their predictive power. We derive theoretical guarantees for the regret properties in the bandit setting under ideal circumstances, and empirical results are provided accordingly. As a modern study on applications, these methods are applied to the online fine-tuning of a set of expert large language models (LLMs), where after each response, the generative LLM dynamically reweighs its set of experts and/or selects the optimal committee of experts to generate the most accurate response. Our results introduce new methodologies and no-regret guarantees for combining multiple experts to improve on the performance of the an aggregate model overall.",
        "keywords": [
          "Online Learning",
          "Mixture of Experts"
        ],
        "primary_area": "general_machine_learning",
        "forum_url": "https://openreview.net/forum?id=Zfgvo65gxm"
      },
      "similarity": 0.4133177697658539
    },
    {
      "paper": {
        "id": "XUKUx7Xu89",
        "number": 13206,
        "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
        "authors": [
          "William Merrill",
          "Shane Arora",
          "Dirk Groeneveld",
          "Hannaneh Hajishirzi"
        ],
        "abstract": "The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is *too large* will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a *critical batch size* (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to *directly* measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate *batch size warmup* as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance.",
        "keywords": [
          "large language models",
          "pretraining",
          "critical batch size"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=XUKUx7Xu89"
      },
      "similarity": 0.4132295846939087
    },
    {
      "paper": {
        "id": "G2kMroO9UV",
        "number": 28009,
        "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
        "authors": [
          "Hyungjoo Chae",
          "Sunghwan Kim",
          "Junhee Cho",
          "Seungone Kim",
          "Seungjun Moon",
          "Gyeom Hwangbo",
          "Dongha Lim",
          "Minjin Kim",
          "Yeonjun Hwang",
          "Minju Gwak",
          "Dongwook Choi",
          "Minseok Kang",
          "Gwanhoon Im",
          "ByeongUng Cho",
          "Hyojun Kim",
          "Jun Hee Han",
          "Taeyoon Kwon",
          "Minju Kim",
          "Beong-woo Kwak",
          "Dongjin Kang",
          "Jinyoung Yeo"
        ],
        "abstract": "Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks.\nYet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. \nFurthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10x less cost compared to using GPT-4o-mini as the verifier. \nOur model, dataset, and code are publicly available at https://github.com/kyle8581/Web-Shepherd.",
        "keywords": [
          "Web Agent",
          "Reward Model",
          "LLM"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=G2kMroO9UV"
      },
      "similarity": 0.41307348012924194
    },
    {
      "paper": {
        "id": "W5Ht05jF4c",
        "number": 17792,
        "title": "Diffusion Beats Autoregressive in Data-Constrained Settings",
        "authors": [
          "Mihir Prabhudesai",
          "Mengning Wu",
          "Amir Zadeh",
          "Katerina Fragkiadaki",
          "Deepak Pathak"
        ],
        "abstract": "Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings—where training involves repeated passes over limited data—and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. Finally, we explain why diffusion models excel in this regime: their randomized masking objective implicitly trains over a rich distribution of token orderings, acting as an implicit data augmentation that AR’s fixed left-to-right factorization lacks. Our results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io",
        "keywords": [
          "diffusion",
          "autoregressive",
          "data-constrainted",
          "generative modelling",
          "discrete diffusion",
          "diffusion llms"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=W5Ht05jF4c"
      },
      "similarity": 0.41301217675209045
    },
    {
      "paper": {
        "id": "N4OFsehhi7",
        "number": 12063,
        "title": "Algorithm- and Data-Dependent Generalization Bounds for Diffusion Models",
        "authors": [
          "Benjamin Dupuis",
          "Dario Shariatian",
          "Maxime Haddouche",
          "Alain Oliviero Durmus",
          "Umut Simsekli"
        ],
        "abstract": "Score-based generative models (SGMs) have emerged as one of the most popular classes of generative models. A substantial body of work now exists on the analysis of SGMs, focusing either on discretization aspects or on their statistical performance. In the latter case, bounds have been derived, under various metrics, between the true data distribution and the distribution induced by the SGM, often demonstrating polynomial convergence rates with respect to the number of training samples. However, these approaches adopt a largely approximation theory viewpoint, which tends to be overly pessimistic and relatively coarse. In particular, they fail to fully explain the empirical success of SGMs or capture the role of the optimization algorithm used in practice to train the score network. To support this observation, we first present simple experiments illustrating the concrete impact of optimization hyperparameters on the generalization ability of the generated distribution. Then, this paper aims to bridge this theoretical gap by providing the first algorithmic- and data-dependent generalization analysis for SGMs. In particular, we establish bounds that explicitly account for the optimization dynamics of the learning algorithm, offering new insights into the generalization behavior of SGMs. Our theoretical findings are supported by empirical results on several datasets.",
        "keywords": [
          "Diffusion models",
          "Generalization bounds",
          "Score-based generative models"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=N4OFsehhi7"
      },
      "similarity": 0.4127486050128937
    },
    {
      "paper": {
        "id": "0Y7AxxNCYh",
        "number": 23284,
        "title": "Explaining and Mitigating Crosslingual Tokenizer Inequities",
        "authors": [
          "Catherine Arnett",
          "Tyler A. Chang",
          "Stella Biderman",
          "Ben Bergen"
        ],
        "abstract": "The number of tokens it takes to encode parallel text in different languages is known to vary. These disparities are called *token premiums*. Having high token premiums leads to less throughput during training and increases costs at inference. \nIn this paper, we show that even after controlling for dataset size, vocabulary size, and data content, monolingual tokenizers exhibit a wide range of token premiums across languages. To understand the cross-linguistic differences that cause these token premiums,\nwe train a suite of approximately 7,000 comparable monolingual tokenizers for 97 languages, manipulating tokenization algorithm vocabulary size, and dataset size. We measure token premiums and test for a relationship between factors such as data similarity (between tokenizer training and evaluation), vocabulary size, and pre-tokenization. We also investigate the role of language-specific features such as writing system and word length. We find that similarity between training and test data does not impact token premiums, but vocabulary size and pre-tokenization do. While simply increasing vocabulary size does not lead to reduced token premium effects, we can determine an \"optimal\" vocabulary size for each language to achieve significantly reduced token premium effects. We also train superword tokenizers which allow merges over whitespaces, and we find that they both reduce token premium effects and improve compression overall. Thus, intervening on the vocabulary size or the pre-tokenizer significantly reduces crosslingual token premium effects.",
        "keywords": [
          "tokenizers",
          "subword tokenization",
          "multilingual"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=0Y7AxxNCYh"
      },
      "similarity": 0.41248053312301636
    },
    {
      "paper": {
        "id": "853SwC2dMZ",
        "number": 15024,
        "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
        "authors": [
          "Zhixuan Pan",
          "Shaowen Wang",
          "Liao Pengfei",
          "Jian Li"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap’s and Zipf’s laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions.",
        "keywords": [
          "Large Language Model",
          "Scaling Law",
          "Information Theory"
        ],
        "primary_area": "theory",
        "forum_url": "https://openreview.net/forum?id=853SwC2dMZ"
      },
      "similarity": 0.4124276340007782
    },
    {
      "paper": {
        "id": "aTlVrJRrNg",
        "number": 13544,
        "title": "Online Two-Stage Submodular Maximization",
        "authors": [
          "Iasonas Nikolaou",
          "Miltiadis Stouras",
          "Stratis Ioannidis",
          "Evimaria Terzi"
        ],
        "abstract": "Given a collection of monotone submodular functions, the goal of Two-Stage Submodular Maximization (2SSM) (Balkanski et al. 2016) is to restrict the ground set so an objective selected u.a.r. from the collection attains a high maximal value, on average, when optimized over the restricted ground set. We introduce the Online Two-Stage Submodular Maximization (O2SSM) problem, in which the submodular objectives are revealed in an online fashion. \nWe study this problem for weighted threshold potential functions, a large and important subclass of monotone submodular functions that includes influence maximization, data summarization, and facility location, to name a few.\nWe design an algorithm that achieves sublinear $(1 - 1/e)^2$-regret under general matroid constraints and $(1 - 1/e)(1-e^{-k}k^k/k!)$-regret in the case of uniform matroids of rank $k$; the latter also yields a state-of-the-art bound for the (offline) 2SSM problem. \nWe empirically validate the performance of our online algorithm with experiments on real datasets.",
        "keywords": [
          "online",
          "two-stage",
          "submodular maximization",
          "recommender systems",
          "data summarization"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=aTlVrJRrNg"
      },
      "similarity": 0.412206768989563
    },
    {
      "paper": {
        "id": "mN3CMpfWR6",
        "number": 18795,
        "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search",
        "authors": [
          "Dong Li",
          "Xujiang Zhao",
          "Linlin Yu",
          "Yanchi Liu",
          "Wei Cheng",
          "Zhengzhang Chen",
          "Zhong Chen",
          "Feng Chen",
          "Chen Zhao",
          "Haifeng Chen"
        ],
        "abstract": "Large Language Models (LLMs) offer promising capabilities for tackling complex reasoning tasks, including optimization problems. However, existing methods either rely on prompt engineering, which leads to poor generalization across problem types, or require costly supervised training. We introduce SolverLLM, a training-free framework that leverages test-time scaling to solve diverse optimization problems. Rather than solving directly, SolverLLM generates mathematical formulations and translates them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the search process, we modify classical MCTS with (1) dynamic expansion for adaptive formulation generation, (2) prompt backpropagation to guide exploration via outcome-driven feedback, and (3) uncertainty backpropagation to incorporate reward reliability into decision-making. Experiments on six standard benchmark datasets demonstrate that SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.",
        "keywords": [
          "MCTS",
          "LLM",
          "Optimization Problem"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=mN3CMpfWR6"
      },
      "similarity": 0.41180622577667236
    },
    {
      "paper": {
        "id": "UE0cxjNnIw",
        "number": 13181,
        "title": "Scaling Up Active Testing to Large Language Models",
        "authors": [
          "Gabrielle Berrada",
          "Jannik Kossen",
          "Freddie Bickford Smith",
          "Muhammed Razzak",
          "Yarin Gal",
          "Tom Rainforth"
        ],
        "abstract": "Active testing enables label-efficient evaluation of predictive models through careful data acquisition, but it can pose a significant computational cost. We identify cost-saving measures that enable active testing to be scaled up to large language models (LLMs). In particular we show that the surrogate model used to guide data acquisition can be constructed cheaply using in-context learning, does not require updating within an active-testing loop, and can be smaller than the target model. We even find we can make good data-acquisition decisions without making predictions with the target model. As a result we are able to achieve much more accurate evaluations of LLM performance relative to using randomly acquired data. We additionally introduce a bootstrap estimator of evaluation error, which we show to be a useful indicator of how well active testing is working within a single run.",
        "keywords": [
          "active testing",
          "active evaluation",
          "model evaluation",
          "data curation",
          "large language models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=UE0cxjNnIw"
      },
      "similarity": 0.41150611639022827
    },
    {
      "paper": {
        "id": "JRmIvBcnWc",
        "number": 13350,
        "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
        "authors": [
          "Simin Fan",
          "Maria Ios Glarou",
          "Martin Jaggi"
        ],
        "abstract": "The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora.\nExisting domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks.\nThis paper introduces $\\textbf{G}$roup $\\textbf{R}$obust Multi-target $\\textbf{A}$daptive $\\textbf{P}$r$\\textbf{E}$training (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously.\nGRAPE dynamically adjusts sampling weights across source domains ($\\textit{domain weights}$) while concurrently modulating $\\textit{task weights}$ that quantify the relative importance of each individual target task.\nThis adaptive process prioritizes tasks based on their learning difficulty throughout training. \nWe formulate this interleaved reweighting mechanism as a minimax optimization problem: \nThe inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; \nThe outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks.\nExperiments on $\\texttt{ClimbLab}$ and $\\texttt{SlimPajama}$ datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning accuracies across 6 benchmarks. \nFurthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.",
        "keywords": [
          "LLM pretrain",
          "data selection"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=JRmIvBcnWc"
      },
      "similarity": 0.4109877347946167
    },
    {
      "paper": {
        "id": "YJx8AofTF5",
        "number": 21350,
        "title": "Program Synthesis via Test-Time Transduction",
        "authors": [
          "Kang-il Lee",
          "Jahyun Koo",
          "Seunghyun Yoon",
          "Minbeom Kim",
          "Hyukhun Koh",
          "Dongryeol Lee",
          "Kyomin Jung"
        ],
        "abstract": "We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC, and programmatic world modeling on MiniGrid. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.",
        "keywords": [
          "program synthesis",
          "code generation",
          "large language models",
          "world models"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=YJx8AofTF5"
      },
      "similarity": 0.41059619188308716
    },
    {
      "paper": {
        "id": "NM8Apk61NA",
        "number": 5784,
        "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
        "authors": [
          "Zelin Peng",
          "Zhengqin Xu",
          "Qingyang Liu",
          "Xiaokang Yang",
          "Wei Shen"
        ],
        "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters.",
        "keywords": [
          "Efficient Training",
          "Multi-modal Large Language Models",
          "Granularity Levels",
          "Hyperbolic Space"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=NM8Apk61NA"
      },
      "similarity": 0.41027694940567017
    },
    {
      "paper": {
        "id": "pXXKKBx8G0",
        "number": 1021,
        "title": "CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization",
        "authors": [
          "Yichen Yan",
          "Ming Zhong",
          "Qi Zhu",
          "Xiaoling Gu",
          "Jinpeng Chen",
          "Huan Li"
        ],
        "abstract": "Multimodal large language models (MLLMs) rely heavily on instruction tuning to align vision and language capabilities, yet the computational cost of training on large-scale datasets remains a major bottleneck. Existing data selection methods aim to mitigate this by selecting important and diverse subsets, but they often suffer from two critical drawbacks: high computational overhead from processing the entire dataset and suboptimal data selection due to separate treatment of importance and diversity.\n\nWe introduce CoIDO, a novel dual-objective framework that jointly optimizes data importance and diversity to overcome these challenges. Unlike existing approaches that require costly evaluations across the whole dataset, CoIDO employs a lightweight plug-in scorer. This scorer is trained on just a small random sample of data to learn the distribution of the candidate set, drastically reducing computational demands. By leveraging a homoscedastic uncertainty-based formulation, CoIDO effectively balances importance and diversity during training, enabling the scorer to assign CoIDO scores to all data points. This unified scoring approach allows for direct ranking and selection of the most valuable subsets, completely bypassing the need for specialized algorithms.\n\nIn our experiments, we trained the CoIDO Scorer using only 20% of randomly sampled data. Once trained, CoIDO was applied to the entire dataset to select a 20% subset for instruction tuning. On the widely used LLaVA-1.5-7B model across ten downstream tasks, this selected subset achieved an impressive 98.2% of the performance of full-data fine-tuning, on average. Moreover, CoIDO outperforms all competitors in terms of both efficiency (lowest training FLOPs) and aggregated accuracy. Our code is available at: https://github.com/SuDIS-ZJU/CoIDO",
        "keywords": [
          "Large Language Model",
          "Visual Instruction Tuning",
          "Data Selection"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=pXXKKBx8G0"
      },
      "similarity": 0.4102442264556885
    },
    {
      "paper": {
        "id": "4Qe2Hga43N",
        "number": 16804,
        "title": "Cost-Aware Contrastive Routing for LLMs",
        "authors": [
          "Reza Shirkavand",
          "Shangqian Gao",
          "Peiran Yu",
          "Heng Huang"
        ],
        "abstract": "We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single $k$‑NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy–cost tradeoff by up to 25\\%, while generalizing robustly to unseen LLMs and out-of-distribution prompts.",
        "keywords": [
          "LLM Routing",
          "Inference Efficiency",
          "Contrastive Learning",
          "Model fingerprints",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=4Qe2Hga43N"
      },
      "similarity": 0.40999260544776917
    },
    {
      "paper": {
        "id": "gH4BRa4ZP3",
        "number": 11133,
        "title": "Scaling Embedding Layers in Language Models",
        "authors": [
          "Da Yu",
          "Edith Cohen",
          "Badih Ghazi",
          "Yangsibo Huang",
          "Pritish Kamath",
          "Ravi Kumar",
          "Daogao Liu",
          "Chiyuan Zhang"
        ],
        "abstract": "We propose SCONE (**S**calable, **C**ontextualized, **O**ffloaded, **N**-gram **E**mbedding), a new method for extending input embedding layers to enhance language model performance. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. After training, embeddings are precomputed and stored in off-accelerator memory; during inference, querying them has minimal impact on latency due to the low complexity of embedding lookups. SCONE enables two new scaling strategies: increasing the number of $n$-gram embeddings and scaling the model used to learn them, both while maintaining fixed accelerator usage  during inference (in terms of FLOPS and memory). We show that scaling both aspects enables a model with 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline across diverse corpora, while using only about half the FLOPS and accelerator memory during inference.",
        "keywords": [
          "embedding layer scalability",
          "contextualized token embeddings",
          "scaling with fixed accelerator usage"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=gH4BRa4ZP3"
      },
      "similarity": 0.40991732478141785
    },
    {
      "paper": {
        "id": "IYVknFxsJb",
        "number": 11461,
        "title": "System Prompt Optimization with Meta-Learning",
        "authors": [
          "Yumin Choi",
          "Jinheon Baek",
          "Sung Ju Hwang"
        ],
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.",
        "keywords": [
          "Prompt Optimization",
          "Meta-Learning",
          "System Prompt Optimization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=IYVknFxsJb"
      },
      "similarity": 0.4098244309425354
    },
    {
      "paper": {
        "id": "1SCMFCGliM",
        "number": 26744,
        "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
        "authors": [
          "Ke Ji",
          "Jiahao Xu",
          "Tian Liang",
          "Qiuzhi Liu",
          "Zhiwei He",
          "Xiaoyuan Liu",
          "Xingyu Chen",
          "Junying Chen",
          "Benyou Wang",
          "Zhaopeng Tu",
          "Haitao Mi",
          "Dong Yu"
        ],
        "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT  removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75\\% and sampling cost by 99\\%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model’s structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches.",
        "keywords": [
          "Large Language Models; Efficient Reasoning; Unsupervised Learning"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=1SCMFCGliM"
      },
      "similarity": 0.4095538258552551
    },
    {
      "paper": {
        "id": "mlU9KqdZUS",
        "number": 10342,
        "title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement",
        "authors": [
          "J Rosser",
          "Jakob Nicolaus Foerster"
        ],
        "abstract": "Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In \"blue\" mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In \"red\" mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at \\url{https://github.com/jrosseruk/AgentBreeder}.",
        "keywords": [
          "AI Safety",
          "Multi-Agent Systems",
          "LLMs",
          "Large Language Models",
          "Jailbreaking",
          "Agent Scaffolds"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=mlU9KqdZUS"
      },
      "similarity": 0.409068763256073
    },
    {
      "paper": {
        "id": "E6ZdfjtoiX",
        "number": 21907,
        "title": "Efficient Data Selection at Scale via Influence Distillation",
        "authors": [
          "Mahdi Nikdan",
          "Vincent Cohen-Addad",
          "Dan Alistarh",
          "Vahab Mirrokni"
        ],
        "abstract": "Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample's influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a $\\textit{landmark-based approximation}$: influence is precisely computed for a small subset of \"landmark\" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to $3.5\\times$ faster selection.",
        "keywords": [
          "Data Selection",
          "Data Weighting",
          "Influence Distillation",
          "Fine Tuning",
          "Efficient Training",
          "Large Language Models"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=E6ZdfjtoiX"
      },
      "similarity": 0.4089396595954895
    },
    {
      "paper": {
        "id": "eIojV2epgX",
        "number": 12174,
        "title": "Synergistic Tensor and Pipeline Parallelism",
        "authors": [
          "Mengshi Qi",
          "Jiaxuan Peng",
          "Jie Zhang",
          "Juan Zhu",
          "Yong Li",
          "Huadong Ma"
        ],
        "abstract": "In the machine learning system, the hybrid model parallelism combining tensor parallelism (TP) and pipeline parallelism (PP) has become the dominant solution for distributed training of Large Language Models~(LLMs) and Multimodal LLMs (MLLMs). However, TP introduces significant collective communication overheads, while PP suffers from synchronization inefficiencies such as pipeline bubbles. Existing works primarily address these challenges from isolated perspectives, focusing either on overlapping TP communication or on flexible PP scheduling to mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor and pipeline parallelism schedule that simultaneously reduces both types of bubbles. Our proposed schedule decouples the forward and backward passes in PP into fine-grained computation units, which are then braided to form a composite computation sequence. This compositional structure enables near-complete elimination of TP-related bubbles. Building upon this structure, we further design the PP schedule to minimize PP bubbles. Experimental results demonstrate that our approach improves training throughput by up to 12\\% for LLMs and 16\\% for MLLMs compared to existing scheduling methods. Our source code is avaiable at https://github.com/MICLAB-BUPT/STP.",
        "keywords": [
          "Distributed System",
          "Pipeline Parallelism",
          "Large-Scale Model Training",
          "LLM",
          "Machine Learning System"
        ],
        "primary_area": "infrastructure",
        "forum_url": "https://openreview.net/forum?id=eIojV2epgX"
      },
      "similarity": 0.40890294313430786
    },
    {
      "paper": {
        "id": "l75RyRcevf",
        "number": 9747,
        "title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking",
        "authors": [
          "Zifeng Cheng",
          "Jinwei Gan",
          "Zhiwei Jiang",
          "Cong Wang",
          "Yafeng Yin",
          "Xiang Luo",
          "Yuchen Fu",
          "Qing Gu"
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable performance across many generation tasks.\nNevertheless, effectively aligning them with desired behaviors remains a significant challenge.\nActivation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning.\nExisting methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength.\nTo this end, we propose the **F**lexible **A**ctivation **S**teering with **B**acktracking (**FASB**) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content.\nSince intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior.\nExtensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines.\nOur code will be released at https://github.com/gjw185/FASB.",
        "keywords": [
          "Activation steering"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=l75RyRcevf"
      },
      "similarity": 0.40881413221359253
    },
    {
      "paper": {
        "id": "Ox3U97Svtl",
        "number": 19322,
        "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation",
        "authors": [
          "Zhenwen Liang",
          "Linfeng Song",
          "Yang Li",
          "TAO YANG",
          "Haitao Mi",
          "Dong Yu"
        ],
        "abstract": "Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40\\% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.",
        "keywords": [
          "Theorem Proving",
          "Large Language Model",
          "Tree Search"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Ox3U97Svtl"
      },
      "similarity": 0.4087003469467163
    },
    {
      "paper": {
        "id": "cCYUFaR6En",
        "number": 10280,
        "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training",
        "authors": [
          "Kaichen Zhang",
          "Yuzhong Hong",
          "Junwei Bao",
          "Hongfei Jiang",
          "yang song",
          "Hong Dingqian",
          "Hui Xiong"
        ],
        "abstract": "Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. As a next step, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training.",
        "keywords": [
          "Large Language Model",
          "Post Training",
          "Reinforcement Learning",
          "Reasoning",
          "GVPO",
          "GRPO"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=cCYUFaR6En"
      },
      "similarity": 0.4081404209136963
    },
    {
      "paper": {
        "id": "isATAFP71B",
        "number": 21709,
        "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents",
        "authors": [
          "Yifu Guo",
          "Jiaye Lin",
          "Huacan Wang",
          "Yuzhen Han",
          "Sen Hu",
          "Ziyi Ni",
          "Licheng Wang",
          "Mingguang Chen"
        ],
        "abstract": "Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process—agents' interaction trajectory leading to task completion—remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified.",
        "keywords": [
          "LLM-Based Agents",
          "Self-Evolution",
          "Multi-Step Reasoning"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=isATAFP71B"
      },
      "similarity": 0.40776509046554565
    },
    {
      "paper": {
        "id": "jaMPaFDAaZ",
        "number": 16277,
        "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing",
        "authors": [
          "Shoutao Guo",
          "Shaolei Zhang",
          "Qingkai Fang",
          "Zhengrui Ma",
          "Min zhang",
          "Yang Feng"
        ],
        "abstract": "The rapid advancement of Large Language Models (LLMs) has spurred significant progress in Large Speech-Language Models (LSLMs), enhancing their capabilities in both speech understanding and generation. While existing LSLMs often concentrate on augmenting speech generation or tackling a diverse array of short-speech tasks, the efficient processing of long-form speech remains a critical yet underexplored challenge. This gap is primarily attributed to the scarcity of long-speech training datasets and the high computational costs associated with long sequences. To address these limitations, we introduce FastLongSpeech, a novel framework designed to extend LSLM capabilities for efficient long-speech processing without necessitating dedicated long-speech training data. FastLongSpeech incorporates an iterative fusion strategy that can compress excessively long-speech sequences into manageable lengths. To adapt LSLMs for long-speech inputs, it introduces a dynamic compression training approach, which exposes the model to short-speech sequences at varying compression ratios, thereby transferring the capabilities of LSLMs to long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop a long-speech understanding benchmark called LongSpeech-Eval. Experiments show that our method exhibits strong performance in both long-speech and short-speech tasks, while greatly improving inference efficiency.",
        "keywords": [
          "Spoken Language Understanding",
          "Large Speech-Language Models",
          "Speech Technologies"
        ],
        "primary_area": "applications",
        "forum_url": "https://openreview.net/forum?id=jaMPaFDAaZ"
      },
      "similarity": 0.40760859847068787
    },
    {
      "paper": {
        "id": "8ZdWmpYxT0",
        "number": 13461,
        "title": "AdaMSS: Adaptive Multi-Subspace Approach for Parameter-Efficient Fine-Tuning",
        "authors": [
          "Jingjing Zheng",
          "Wanglong Lu",
          "Yiming Dong",
          "Chaojie Ji",
          "Yankai Cao",
          "Zhouchen Lin"
        ],
        "abstract": "In this paper, we propose AdaMSS, an adaptive multi-subspace approach for parameter-efficient fine-tuning  of large models. Unlike traditional parameter-efficient fine-tuning methods that operate within a large single subspace of the network weights, AdaMSS leverages subspace segmentation to obtain multiple smaller subspaces and adaptively reduces the number of trainable parameters during training, ultimately updating only those associated with a small subset of subspaces most relevant to the target downstream task. By using  the lowest-rank representation, AdaMSS achieves more compact expressiveness and finer tuning of the model parameters. Theoretical analyses demonstrate that AdaMSS has better generalization  guarantee than LoRA, PiSSA, and other single-subspace low-rank-based methods. Extensive experiments across image classification, natural language understanding, and natural language generation tasks show that AdaMSS achieves comparable performance to full fine-tuning  and outperforms other parameter-efficient fine-tuning   methods in most cases, all while requiring fewer trainable parameters. Notably, on the ViT-Large model, AdaMSS achieves 4.7\\% higher average accuracy than LoRA across seven tasks, using just 15.4\\% of the trainable parameters. On RoBERTa-Large, AdaMSS outperforms PiSSA by 7\\% in average accuracy across six tasks while reducing the number of trainable parameters by approximately 94.4\\%. These results demonstrate the effectiveness of AdaMSS in parameter-efficient fine-tuning. The code for AdaMSS is available at https://github.com/jzheng20/AdaMSS.",
        "keywords": [
          "Parameter-Efficient Fine-Tuning",
          "Low-Rank Adaptation",
          "Low-Rank Representation"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=8ZdWmpYxT0"
      },
      "similarity": 0.407540500164032
    },
    {
      "paper": {
        "id": "1zKElu2MuQ",
        "number": 21892,
        "title": "Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences",
        "authors": [
          "Hadi Hosseini",
          "Samarth Khanna",
          "Ronak Singh"
        ],
        "abstract": "The rise of Large Language Models (LLMs) has driven progress in reasoning tasks, from program synthesis to scientific hypothesis generation, yet their ability to handle ranked preferences and structured algorithms in combinatorial domains remains underexplored. \nWe study matching markets, a core framework behind applications like resource allocation and ride-sharing, which require reconciling individual ranked preferences to ensure stable outcomes. We evaluate seven state‐of‐the‐art models on a hierarchy of preference‐based reasoning tasks---ranging from stable‐matching generation to instability detection, instability resolution, and fine-grained preference queries---to systematically expose their logical and algorithmic limitations in handling ranked inputs. Surprisingly, even top-performing models with advanced reasoning struggle to resolve instability in large markets, often failing to identify blocking pairs or execute algorithms iteratively. We further show that parameter-efficient fine-tuning (LoRA) significantly improves performance in small markets, but fails to bring about a similar improvement on large instances, suggesting the need for more sophisticated strategies to improve LLMs' reasoning with larger-context inputs.",
        "keywords": [
          "Preference reasoning",
          "matching markets",
          "stability",
          "comprehension"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=1zKElu2MuQ"
      },
      "similarity": 0.40708595514297485
    },
    {
      "paper": {
        "id": "Tdl89SZItB",
        "number": 27280,
        "title": "Accurate KV Cache Eviction via Anchor Direction Projection for Efficient LLM Inference",
        "authors": [
          "Zijie Geng",
          "Jie Wang",
          "Ziqi Liu",
          "Feng Ju",
          "Yiming Li",
          "Xing Li",
          "Mingxuan Yuan",
          "Jianye HAO",
          "Defu Lian",
          "Enhong Chen",
          "Feng Wu"
        ],
        "abstract": "Key-Value (KV) cache eviction---which retains the KV pairs of the most important tokens while discarding less important ones---is a critical technique for optimizing both memory usage and inference latency in large language models (LLMs).\nHowever, existing approaches often rely on simple heuristics---such as attention weights---to measure token importance, overlooking the spatial relationships between token value states in the vector space.\nThis often leads to suboptimal token selections and thus performance degradation.\nTo tackle this problem, we propose a novel method, namely **AnDPro** (**An**chor **D**irection **Pro**jection), which introduces a projection-based scoring function to more accurately measure token importance.\nSpecifically, AnDPro operates in the space of value vectors and leverages the projections of these vectors onto an *``Anchor Direction''*---the direction of the pre-eviction output---to measure token importance and guide more accurate token selection.\nExperiments on $16$ datasets from the LongBench benchmark demonstrate that AnDPro can maintain $96.07\\\\%$ of the full cache accuracy using only $3.44\\\\%$ KV cache budget, reducing KV cache budget size by $46.0\\\\%$ without compromising quality compared to previous state-of-the-arts.",
        "keywords": [
          "Large Language Models",
          "KV Cache Eviction",
          "LLM Inference Acceleration"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=Tdl89SZItB"
      },
      "similarity": 0.40693455934524536
    },
    {
      "paper": {
        "id": "k2wzVFXZmC",
        "number": 12985,
        "title": "Explainable Reinforcement Learning from Human Feedback to Improve Alignment",
        "authors": [
          "Shicheng Liu",
          "Siyuan Xu",
          "Wenjie Qiu",
          "Hangfan Zhang",
          "Minghui Zhu"
        ],
        "abstract": "A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.",
        "keywords": [
          "explainable reinforcement learning from human feedback",
          "alignment"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=k2wzVFXZmC"
      },
      "similarity": 0.4068310260772705
    },
    {
      "paper": {
        "id": "ZSAWYtIwGg",
        "number": 11058,
        "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning",
        "authors": [
          "Zongqian Li",
          "Yixuan Su",
          "Nigel Collier"
        ],
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating either matrix decomposition or mixture-of-experts (MoE) individually decreases performance across tasks, though decomposition improves results on specific domains despite reducing parameters, while MoE increases parameter count without corresponding decrease in training efficiency. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with MoE routing for efficient PT. Evaluation results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while improving mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods.",
        "keywords": [
          "Natural Language Processing"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=ZSAWYtIwGg"
      },
      "similarity": 0.4064229428768158
    },
    {
      "paper": {
        "id": "x0i7wvRLHK",
        "number": 12243,
        "title": "Exploring the limits of strong membership inference attacks on large language models",
        "authors": [
          "Jamie Hayes",
          "Ilia Shumailov",
          "Christopher A. Choquette-Choo",
          "Matthew Jagielski",
          "Georgios Kaissis",
          "Milad Nasr",
          "Meenatchi Sundaram Muthu Selva Annamalai",
          "Niloofar Mireshghallah",
          "Igor Shilov",
          "Matthieu Meeus",
          "Yves-Alexandre de Montjoye",
          "Katherine Lee",
          "Franziska Boenisch",
          "Adam Dziedzic",
          "A. Feder Cooper"
        ],
        "abstract": "State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training references (e.g., fine-tuning attacks), or on stronger attacks applied to small models and datasets. However, weaker attacks have been shown to be brittle and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges prompt an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs?\nWe address this question by scaling LiRA--one of the strongest MIAs--to GPT-2 architectures ranging from 10M to 1B parameters, training references on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in four key ways. While (1) strong MIAs can succeed on pre-trained LLMs, (2) their effectiveness, remains limited (e.g., AUC<0.7) in practical settings. (3) Even when strong MIAs achieve better-than-random AUC, aggregate success metrics conceal per-sample prediction instability; many individual predictions are so unstable that they are statistically indistinguishable from a coin flip. Finally, (4) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.",
        "keywords": [
          "membership inference",
          "memorization"
        ],
        "primary_area": "social_and_economic_aspects_of_machine_learning",
        "forum_url": "https://openreview.net/forum?id=x0i7wvRLHK"
      },
      "similarity": 0.4063284397125244
    },
    {
      "paper": {
        "id": "e7jNIna1eP",
        "number": 1115,
        "title": "NaDRO: Leveraging Dual-Reward Strategies for LLMs Training on Noisy Data",
        "authors": [
          "Haolong Qian",
          "Xianliang Yang",
          "Ling Zhang",
          "Lei Song",
          "Jiang Bian",
          "Chun Yuan"
        ],
        "abstract": "Group Relative Policy Optimization (GRPO) fine-tuning has been empirically shown to significantly enhance the reasoning abilities of language models. However, it often relies on large-scale, high-quality labeled data, which is typically difficult to obtain. To address this challenge, we introduce the Noise-Aware Dual-Reward Optimization (NaDRO) , which effectively enhances LLMs training in environments where data is noisy or imperfect. NaDRO operates through two key components: \\textbf{(1) Preference-based Outcome Reward (POR)}, which extracts reliable preference signals from noisy data, guiding LLMs towards more effective decisions instead of relying on specific noisy scores; and \\textbf{(2) a Context Perception Reward (CPR) mechanism}, which ensures that LLMs conduct necessary qualitative assessment of the current problem state, rewarding accurate judgments to foster better cognitive understanding before decision-making. In the context of combinatorial optimization problems, where dynamically selecting heuristic algorithms is challenging due to large problem scales and the difficulty of obtaining accurate decision data, we designed experiments to test our approach. Our results indicate that the fine-tuned Qwen 7B and Llama 3-8B models outperform mainstream large language models (LLMs) training in this task. Code is released at \\url{https://anonymous.4open.science/r/NaDRO-D34D}",
        "keywords": [
          "LLM",
          "GRPO",
          "MCTS",
          "Finetune"
        ],
        "primary_area": "optimization",
        "forum_url": "https://openreview.net/forum?id=e7jNIna1eP"
      },
      "similarity": 0.4062649607658386
    },
    {
      "paper": {
        "id": "gm5mkiTGOy",
        "number": 15835,
        "title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
        "authors": [
          "Zheng-An Chen",
          "Tao Luo"
        ],
        "abstract": "Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in \\cite{zhou2022towards} to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results.",
        "keywords": [
          "Two-stage analysis",
          "Training dynamics of transformers",
          "Condensation",
          "Small initialization"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=gm5mkiTGOy"
      },
      "similarity": 0.4060211777687073
    },
    {
      "paper": {
        "id": "oUghNi5XWc",
        "number": 4718,
        "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs",
        "authors": [
          "Jinhong Deng",
          "Wen Li",
          "Joey Tianyi Zhou",
          "Yang He"
        ],
        "abstract": "Multimodal Large Language Models (MLLMs) typically process a large number of visual tokens, leading to considerable computational overhead, even though many of these tokens are redundant. Existing visual token pruning methods primarily focus on selecting the most salient tokens based on attention scores, resulting in the semantic incompleteness of the selected tokens. In this paper, we propose a novel visual token pruning strategy, called **S**aliency-**C**overage **O**riented token **P**runing for **E**fficient MLLMs (SCOPE), to jointly model both the saliency and coverage of the selected visual tokens to better preserve semantic completeness. Specifically, we introduce a set-coverage for a given set of selected tokens, computed based on the token relationships. We then define a token-coverage gain for each unselected token, quantifying how much additional coverage would be obtained by including it. By integrating the saliency score into the token-coverage gain, we propose our SCOPE score and iteratively select the token with the highest SCOPE score. We conduct extensive experiments on multiple vision-language understanding benchmarks using the LLaVA-1.5 and LLaVA-Next models. Experimental results demonstrate that our method consistently outperforms prior approaches.",
        "keywords": [
          "Visual Token Pruning",
          "Efficient MLLM"
        ],
        "primary_area": "deep_learning",
        "forum_url": "https://openreview.net/forum?id=oUghNi5XWc"
      },
      "similarity": 0.40596702694892883
    },
    {
      "paper": {
        "id": "jQH0gdwsuT",
        "number": 23555,
        "title": "Greedy Sampling Is Provably Efficient For RLHF",
        "authors": [
          "Di Wu",
          "Chengshuai Shi",
          "Jing Yang",
          "Cong Shen"
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post‑training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL.  Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF.",
        "keywords": [
          "RLHF",
          "Greedy Sampling",
          "General Preference Model"
        ],
        "primary_area": "reinforcement_learning",
        "forum_url": "https://openreview.net/forum?id=jQH0gdwsuT"
      },
      "similarity": 0.40550684928894043
    }
  ]
}